{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with BETO in SMM4h-Spanish\n",
    "\n",
    "In this notebook a BETO model is fine-tuned for NER. We also generate the word embeddings that we'll use later in the other notebook. Finally, we will extract the word embeddings vectors associate to the entities, to be used through cosine similarity in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install matplotlib\n",
    "!pip install spacy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BETO fine-tuning to ProfNER\n",
    "\n",
    "We fine-tuned the Spanish BERT model, known as BETO and based in https://github.com/dccuchile/beto\n",
    "This model is trained on the training and validation set. Once the training process is finished, we save the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = '/home/sergio/Escritorio/ProfNER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import es_core_news_md\n",
    "\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# BIOES NOTATION #####################\n",
    "BEGIN = 'B'\n",
    "INSIDE = 'I'\n",
    "OUTSIDE = 'O'\n",
    "END = 'E'\n",
    "SINGLE = 'S'\n",
    "\n",
    "def getDictEntities(file_ann, ent_classes = ['PROFESION', 'SITUACION_LABORAL']):\n",
    "  entities = {}\n",
    "  with open(file_ann) as anns:\n",
    "    for ann in anns:\n",
    "      if ann.split('\\t')[1].split(' ')[0] in ent_classes:\n",
    "          ent = ann[:-1].split('\\t')[2]\n",
    "          #print(ent)\n",
    "          #ent = [token for token in nlp(ent) if not token.is_stop]\n",
    "          ent = nlp(ent)\n",
    "          start = int(ann[:-1].split('\\t')[1].split(' ')[1])\n",
    "          end = int(ann[:-1].split('\\t')[1].split(' ')[2])\n",
    "          if (len(ent) == 1):\n",
    "            entities[(start, end)] = SINGLE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "          else:\n",
    "            entities[(start, start + len(ent[0].text))] = BEGIN + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            entities[(end - len(ent[-1].text)), end] = END + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            for i in range(len(ent) - 2):\n",
    "              spaces = (ent[i + 1].idx) - (ent[i].idx + len(ent[i].text))\n",
    "              start = start + len(ent[i].text) + spaces\n",
    "              entities[(start, start + len(ent[i + 1].text))] = INSIDE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            \n",
    "  return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def getProccessText(sst_home):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for file in [file[:-4] for file in os.listdir(sst_home) if file.endswith('.txt') and not ' ' in file]:\n",
    "        file_text = os.path.join(sst_home, file + '.txt')\n",
    "        file_ann = os.path.join(sst_home, file + '.ann')\n",
    "        _entities = getDictEntities(file_ann)\n",
    "        with open(file_text) as f:\n",
    "          text = f.read()\n",
    "          spacy_text = nlp(text)\n",
    "          for sent in spacy_text.sents:\n",
    "            sentence = []\n",
    "            sent_labels = []\n",
    "            for token in sent:\n",
    "                if not token.like_url:\n",
    "                    sentence.append(token.text.replace('#',''))\n",
    "                    entity = _entities.get((token.idx, token.idx + len(token.text)), 'O')\n",
    "                    sent_labels.append(entity)\n",
    "            sentences.append(sentence)\n",
    "            labels.append(sent_labels)\n",
    "    \n",
    "    return sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home_train = sst_home + '/final-profner-data/subtask-2/brat/train'\n",
    "sentences, labels = getProccessText(sst_home_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = ['B_SITUACION_LABORAL', 'I_SITUACION_LABORAL', 'E_SITUACION_LABORAL', 'S_SITUACION_LABORAL' ,\n",
    "      'B_PROFESION', 'I_PROFESION', 'E_PROFESION', 'S_PROFESION',\n",
    "      'O']\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IProgress in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (0.4)\r\n",
      "Requirement already satisfied: six in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from IProgress) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install IProgress\n",
    "import ipywidgets\n",
    "import IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\\n                                                            random_state=2018, test_size=0.1)\\ntr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\\n                                             random_state=2018, test_size=0.1)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs, tr_tags, tr_masks = input_ids, tags, attention_masks\n",
    "\n",
    "'''tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "tr_masks = torch.tensor(tr_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.3.2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'dccuchile/bert-base-spanish-wwm-cased',\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 1/3 [09:46<19:32, 586.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.04524885599088429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 2/3 [19:46<09:54, 594.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.014281350695130623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [29:48<00:00, 596.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.005524212891464112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    '''\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    #print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.save_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BETO Word Embeddings\n",
    "\n",
    "Using the fine-tuned BETO model, we generate the word-level embeddings that will serve as input to the final model, described in the other notebook. Since in the final model we use the Spacy tokenization, we try to align both tokenizations using the \"tokenizations\" library. The word-level embeddings of BETO will be generated by the mean of the vectors of the subwords that compose a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = '/home/sergio/Escritorio/ProfNER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==2.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es_core_news_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.3.1/es_core_news_md-2.3.1.tar.gz (47.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 47.4 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from es_core_news_md==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.56.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.26.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import es_core_news_md\n",
    "\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sergio/Escritorio/ProfNER/final-profner-data/subtask-2/test-background-txt-files\n"
     ]
    }
   ],
   "source": [
    "sst_home_train = sst_home + '/final-profner-data/subtask-2/test-background-txt-files'\n",
    "print(sst_home_train)\n",
    "import re\n",
    "import os\n",
    "regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "texts = []\n",
    "\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_train, file + '.txt')\n",
    "    with open(file_text) as f:\n",
    "      doc = f.read()\n",
    "      #print(doc)\n",
    "      doc = doc.replace('#', '')\n",
    "      doc = re.sub(regex, '', doc)\n",
    "      #print(doc)\n",
    "    \n",
    "    spacy_text = nlp(doc)\n",
    "    text = '[CLS] '\n",
    "    for sent in spacy_text.sents:\n",
    "        text = text + sent.text + ' [SEP]'\n",
    "    \n",
    "    texts.append((text, [token.text for token in spacy_text], file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-13 15:53:41--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
      "Resolviendo users.dcc.uchile.cl (users.dcc.uchile.cl)... 200.9.99.211, 192.80.24.4\n",
      "Conectando con users.dcc.uchile.cl (users.dcc.uchile.cl)[200.9.99.211]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 409871727 (391M) [application/x-gzip]\n",
      "Guardando como: “pytorch_weights.tar.gz”\n",
      "\n",
      "pytorch_weights.tar 100%[===================>] 390,88M  4,34MB/s    en 5m 33s  \n",
      "\n",
      "2021-02-13 15:59:16 (1,17 MB/s) - “pytorch_weights.tar.gz” guardado [409871727/409871727]\n",
      "\n",
      "--2021-02-13 15:59:16--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
      "Resolviendo users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
      "Conectando con users.dcc.uchile.cl (users.dcc.uchile.cl)[192.80.24.4]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 242120 (236K) [text/plain]\n",
      "Guardando como: “vocab.txt”\n",
      "\n",
      "vocab.txt           100%[===================>] 236,45K   205KB/s    en 1,2s    \n",
      "\n",
      "2021-02-13 15:59:19 (205 KB/s) - “vocab.txt” guardado [242120/242120]\n",
      "\n",
      "--2021-02-13 15:59:19--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
      "Resolviendo users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
      "Conectando con users.dcc.uchile.cl (users.dcc.uchile.cl)[192.80.24.4]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 313 [application/json]\n",
      "Guardando como: “config.json”\n",
      "\n",
      "config.json         100%[===================>]     313  --.-KB/s    en 0s      \n",
      "\n",
      "2021-02-13 15:59:20 (22,4 MB/s) - “config.json” guardado [313/313]\n",
      "\n",
      "pytorch/\n",
      "pytorch/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
    "!tar -xzvf pytorch_weights.tar.gz\n",
    "!mv config.json pytorch/.\n",
    "!mv vocab.txt pytorch/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/ and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs', do_lower_case=False)\n",
    "model = BertModel.from_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/', output_hidden_states = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizations\n",
    "\n",
    "def getVectors(tokens, tokens_vectors, spacy_tokens, BETO_DIM = 1536):\n",
    "    special_tokens = ['[CLS]', '[SEP]', '[UNK]']\n",
    "    tweet_data = []\n",
    "    \n",
    "    aligment, _ = tokenizations.get_alignments(spacy_tokens, tokens)\n",
    "    for token_aligment in aligment:\n",
    "        vector = [tokens_vectors[i] for i in token_aligment if not tokens[i] in special_tokens]\n",
    "        if vector == []:\n",
    "            vector = np.zeros(BETO_DIM)\n",
    "        else:\n",
    "            mean = torch.mean(torch.stack(vector), dim = 0)\n",
    "            vector = mean.cpu().detach().numpy()\n",
    "        tweet_data.append(vector)\n",
    "          \n",
    "    return np.array(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "import numpy as np\n",
    "# Now test it\n",
    "for elements in texts:\n",
    "    text = elements[0]\n",
    "    spacy_tokens = elements[1]\n",
    "    file_name = elements[2]\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    segments_ids = [1] * len(tokens)\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        #print(outputs)\n",
    "\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec_1 = torch.mean(token[-2:], dim=0)\n",
    "        sum_vec_2 = torch.mean(token[-4:-2], dim=0)\n",
    "        sum_vec = torch.cat((sum_vec_1, sum_vec_2))\n",
    "        \n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    #print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "    data[file_name] = getVectors(tokens, token_vecs_sum, spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle5\n",
      "  Using cached pickle5-0.0.11.tar.gz (132 kB)\n",
      "Building wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp36-cp36m-linux_x86_64.whl size=247167 sha256=64ce560b25bdf24715a461cb168319b43f52cdfcfacb4d43d5c3714b97ea74b0\n",
      "  Stored in directory: /home/sergio/.cache/pip/wheels/f9/b7/be/bf9768ab0daa28fa4b386f3ad1bac5dd4d9c349c60e83b24e3\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "with open('/home/sergio/Escritorio/ProfNER/final-saved_data/bert_test.pickle', 'wb') as file:\n",
    "    pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BETO Getting the Entity Vectors\n",
    "\n",
    "In this section, we store the vectors that correspond with entities found in the training and validation subset to be used in the other notebook along with the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es_core_news_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.3.1/es_core_news_md-2.3.1.tar.gz (47.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 47.4 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from es_core_news_md==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: setuptools in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.56.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.26.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import es_core_news_md\n",
    "\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# BIOES NOTATION #####################\n",
    "BEGIN = 'B'\n",
    "INSIDE = 'I'\n",
    "OUTSIDE = 'O'\n",
    "END = 'E'\n",
    "SINGLE = 'S'\n",
    "\n",
    "def getDictEntities(file_ann, ent_classes = ['PROFESION', 'SITUACION_LABORAL']):\n",
    "  entities = {}\n",
    "  with open(file_ann) as anns:\n",
    "    for ann in anns:\n",
    "      if ann.split('\\t')[1].split(' ')[0] in ent_classes:\n",
    "          ent = ann[:-1].split('\\t')[2]\n",
    "          #print(ent)\n",
    "          #ent = [token for token in nlp(ent) if not token.is_stop]\n",
    "          ent = nlp(ent)\n",
    "          start = int(ann[:-1].split('\\t')[1].split(' ')[1])\n",
    "          end = int(ann[:-1].split('\\t')[1].split(' ')[2])\n",
    "          if (len(ent) == 1):\n",
    "            entities[(start, end)] = SINGLE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "          else:\n",
    "            entities[(start, start + len(ent[0].text))] = BEGIN + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            entities[(end - len(ent[-1].text)), end] = END + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            for i in range(len(ent) - 2):\n",
    "              spaces = (ent[i + 1].idx) - (ent[i].idx + len(ent[i].text))\n",
    "              start = start + len(ent[i].text) + spaces\n",
    "              entities[(start, start + len(ent[i + 1].text))] = INSIDE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            \n",
    "  return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "#from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def getElements(sst_home, max_len_seq, getTags = True):\n",
    "  _words = dict()\n",
    "  _doc_tags = {}\n",
    "  _entities = {}\n",
    "  _docs = {}\n",
    "  _docs_offset = {}\n",
    "  #mlb = MultiLabelBinarizer(classes = classes)\n",
    "    \n",
    "  for file in [file[:-4] for file in os.listdir(sst_home) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home, file + '.txt')\n",
    "    if getTags:\n",
    "      file_ann = os.path.join(sst_home, file + '.ann')\n",
    "      _entities = getDictEntities(file_ann)\n",
    "    with open(file_text) as f:\n",
    "      text = f.read()\n",
    "      spacy_text = nlp(text)\n",
    "      #spacy_text = [token for token in spacy_text if not token.is_stop]\n",
    "    \n",
    "      _tweet = []\n",
    "      _tweet_tags = []\n",
    "    \n",
    "      for token in spacy_text[0:max_len_seq]:\n",
    "          if not token.like_url:\n",
    "              _tweet.append(token.text)\n",
    "              _entity = _entities.get((token.idx, token.idx + len(token.text)), 'O')\n",
    "              _tweet_tags.append(_entity)\n",
    "\n",
    "    _docs[file] = _tweet\n",
    "    _doc_tags[file] = _tweet_tags\n",
    "\n",
    "  #_words = list(_words)\n",
    "\n",
    "  return _docs, _doc_tags\n",
    "\n",
    "sst_home_train = sst_home + '/final-profner-data/subtask-2/brat/train'\n",
    "tweets, tags = getElements(sst_home_train, 75, getTags = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home_train = sst_home + '/final-profner-data/subtask-2/brat/train'\n",
    "import re\n",
    "import os\n",
    "regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "texts = {}\n",
    "\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_train, file + '.txt')\n",
    "    with open(file_text) as f:\n",
    "      doc = f.read()\n",
    "      doc = doc.replace('#', '')\n",
    "      doc = re.sub(regex, '', doc)\n",
    "    \n",
    "    spacy_text = nlp(doc)\n",
    "    text = '[CLS] '\n",
    "    for sent in spacy_text.sents:\n",
    "        text = text + sent.text + ' [SEP]'\n",
    "    \n",
    "    texts[file] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs', do_lower_case=False)\n",
    "model = BertModel.from_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/', output_hidden_states = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizations\n",
    "\n",
    "def getEntitiesVectors(tokens, tokens_vectors, spacy_tokens, tags, BETO_DIM = 1536):\n",
    "    special_tokens = ['[CLS]', '[SEP]', '[UNK]']\n",
    "    entity_vectors = []\n",
    "    aligment, _ = tokenizations.get_alignments(spacy_tokens, tokens)\n",
    "    \n",
    "    for token_aligment, tag in zip(aligment, tags):\n",
    "        if tag != 'O':\n",
    "            txtken = [tokens[i] for i in token_aligment if not tokens[i] in special_tokens]\n",
    "            print(txtken)\n",
    "            vector = [tokens_vectors[i] for i in token_aligment if not tokens[i] in special_tokens]\n",
    "            if vector == []:\n",
    "                vector = np.zeros(BETO_DIM)\n",
    "            else:\n",
    "                mean = torch.mean(torch.stack(vector), dim = 0)\n",
    "                vector = mean.cpu().detach().numpy()\n",
    "            entity_vectors.append(vector)\n",
    "          \n",
    "    return entity_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['profesionales']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['epidemi', '##ólogo']\n",
      "['rastre', '##adores']\n",
      "['sanitarios']\n",
      "['Diri', '##gentes']\n",
      "['Rey']\n",
      "['autoridades']\n",
      "['sanitarias']\n",
      "['médicos']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['políticos']\n",
      "['rey']\n",
      "['em', '##éri', '##to']\n",
      "['rastre', '##adores']\n",
      "['Ministro']\n",
      "['jugadores']\n",
      "['políticos']\n",
      "['periodistas']\n",
      "['Sep', '##ult', '##urero']\n",
      "['director']\n",
      "['médico']\n",
      "['trabajadores']\n",
      "['obreros']\n",
      "['fotógrafo']\n",
      "['juez']\n",
      "['santa', '##rios']\n",
      "['colegiados']\n",
      "['divul', '##gador']\n",
      "['ex', '##minist', '##ros']\n",
      "['juez']\n",
      "['del']\n",
      "['Tribunal']\n",
      "['Supremo']\n",
      "['neur', '##ólogo']\n",
      "['Presidente']\n",
      "['dirigentes']\n",
      "['de']\n",
      "['la']\n",
      "['Comunidad']\n",
      "['Del']\n",
      "['.']\n",
      "['de']\n",
      "['Gobierno']\n",
      "['profesionales']\n",
      "['de']\n",
      "['los']\n",
      "['servicios']\n",
      "['sociales']\n",
      "['médico']\n",
      "['profesionales']\n",
      "['de']\n",
      "['la']\n",
      "['información']\n",
      "['políticos']\n",
      "['policia']\n",
      "['Port', '##av', '##oz']\n",
      "['del']\n",
      "['Grupo']\n",
      "['Municipal']\n",
      "['edi', '##les']\n",
      "['enfermera']\n",
      "['ER', '##TE', '##s']\n",
      "['autónomos']\n",
      "['parados']\n",
      "['personas']\n",
      "['trabajadoras']\n",
      "['personas']\n",
      "['de']\n",
      "['la']\n",
      "['sanidad']\n",
      "['sanitarios']\n",
      "['experto']\n",
      "['en']\n",
      "['Co', '##vid', '-', '19']\n",
      "['ministros']\n",
      "['alcalde']\n",
      "['epi', '##dem', '##ólogos']\n",
      "['doctor']\n",
      "['forense']\n",
      "['sanitarias']\n",
      "['policía']\n",
      "['trabajadores']\n",
      "['de']\n",
      "['Metro']\n",
      "['de']\n",
      "['baja']\n",
      "['Rey']\n",
      "['policía']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['sanitarios']\n",
      "['policías']\n",
      "['guardia', '##ci', '##vil']\n",
      "['policia']\n",
      "['periodistas']\n",
      "['gobernante']\n",
      "['bomberos']\n",
      "['personal']\n",
      "['de']\n",
      "['limpieza']\n",
      "[]\n",
      "['Médico', '##s']\n",
      "['Magist', '##rada']\n",
      "['policías']\n",
      "['Dr']\n",
      "['policía']\n",
      "['médico']\n",
      "['dele', '##gada']\n",
      "['presidente']\n",
      "['SAN', '##ITAR', '##IOS']\n",
      "['EN', '##FER', '##ME', '##RA', '##S']\n",
      "['M', '##É', '##DI', '##CO', '##S']\n",
      "['MU', '##J', '##ER', '##ES']\n",
      "['DE']\n",
      "['LA']\n",
      "['L', '##IM', '##PI', '##E', '##Z', '##A']\n",
      "['CA', '##J', '##ERA', '##S']\n",
      "['TRANS', '##PO', '##R', '##TI', '##ST', '##AS']\n",
      "[]\n",
      "['tele', '##traba', '##jo']\n",
      "['fundador']\n",
      "['Presidente']\n",
      "['econom', '##ista']\n",
      "['obrero']\n",
      "['sanitarios']\n",
      "['presidente']\n",
      "['sanitarios']\n",
      "['autor']\n",
      "['infec', '##tó', '##log', '##o']\n",
      "['fiscal']\n",
      "['asesores']\n",
      "['presidenta']\n",
      "['detective']\n",
      "['militares']\n",
      "['efectivos']\n",
      "['de']\n",
      "['asistencia']\n",
      "['sanitaria']\n",
      "['investigadores']\n",
      "['personas']\n",
      "['trabajadoras']\n",
      "['you', '##tu', '##ber']\n",
      "['diputado']\n",
      "['bio', '##química']\n",
      "['Personal']\n",
      "['sanitario']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['profesionales']\n",
      "['autónomos']\n",
      "['sanitarios']\n",
      "['técnicos']\n",
      "['del']\n",
      "['ministerio']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['epidemi', '##ólogo']\n",
      "['científico']\n",
      "['médico']\n",
      "['Polí', '##cia']\n",
      "['ra', '##bino']\n",
      "['arquitecto']\n",
      "['dobla', '##dores']\n",
      "['periodista']\n",
      "['policía']\n",
      "['diputados']\n",
      "['personal']\n",
      "['de']\n",
      "['salud']\n",
      "['sanitarios']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['socios', '##ani', '##tario']\n",
      "['policías']\n",
      "['guardias']\n",
      "['civiles']\n",
      "['bomberos']\n",
      "['técnicos']\n",
      "['socio', '-', 'sanitarios']\n",
      "['sanitarios']\n",
      "['médicos']\n",
      "['enferme', '##ros']\n",
      "['auxiliar']\n",
      "['TE', '##S']\n",
      "['farmacéuticos']\n",
      "['rastre', '##adores']\n",
      "['representante']\n",
      "['trabajadores']\n",
      "['empleados']\n",
      "['trabajador']\n",
      "[]\n",
      "['tele', '##traba', '##jo']\n",
      "['dele', '##gada']\n",
      "['enfermera']\n",
      "['personal']\n",
      "['trabajadores']\n",
      "['Rey']\n",
      "['guía']\n",
      "['can', '##ino']\n",
      "['científicos']\n",
      "['sanitarios']\n",
      "['doctor']\n",
      "['Médico', '##s']\n",
      "['ejecutivos']\n",
      "['políticos']\n",
      "['Coronel']\n",
      "['J', '##uez']\n",
      "['juez', '##a']\n",
      "['general']\n",
      "['jefe']\n",
      "['de']\n",
      "['la']\n",
      "['U', '##ME']\n",
      "['responsables']\n",
      "['técnicos']\n",
      "['sanitarios']\n",
      "['sanitaria']\n",
      "['Ministro']\n",
      "['de']\n",
      "['Salud']\n",
      "['diputado']\n",
      "['portavoz']\n",
      "['científicos']\n",
      "['investigadores']\n",
      "['enfermera']\n",
      "['Enferme', '##ra']\n",
      "['de']\n",
      "['En', '##lace']\n",
      "['profesionales']\n",
      "['del']\n",
      "['sector']\n",
      "['sanitario']\n",
      "['alumnado']\n",
      "['conse', '##ller', '##a']\n",
      "['de']\n",
      "['Salud']\n",
      "['rastre', '##adores']\n",
      "['V', '##P']\n",
      "['Directora']\n",
      "['de']\n",
      "['Personas']\n",
      "['y']\n",
      "['Comunicación']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['pastores']\n",
      "['evan', '##gé', '##licos']\n",
      "['doctor']\n",
      "['doctor']\n",
      "['enfermera']\n",
      "['EN', '##FER', '##ME', '##RA']\n",
      "['Educa', '##dores']\n",
      "['físicos']\n",
      "['Gobernador']\n",
      "['jueces']\n",
      "['comisaría']\n",
      "['profesora']\n",
      "['en']\n",
      "['Psico', '##logía']\n",
      "['especialista']\n",
      "['en']\n",
      "['estrés']\n",
      "['y']\n",
      "['trauma']\n",
      "['gobernador']\n",
      "['Ministro']\n",
      "['Doctora']\n",
      "['dipu', '##tada']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['policia']\n",
      "['científicos']\n",
      "['arren', '##da', '##tarios']\n",
      "['de']\n",
      "['local']\n",
      "['de']\n",
      "['negocio']\n",
      "['vir', '##óloga']\n",
      "['farmacéutica']\n",
      "['comandante']\n",
      "['vicepresidente']\n",
      "['médicos']\n",
      "['personal']\n",
      "['de']\n",
      "['la']\n",
      "['salud']\n",
      "['ER', '##TES']\n",
      "['empleado']\n",
      "['funcionarios']\n",
      "['jubi', '##lados']\n",
      "['Policía']\n",
      "['miembros']\n",
      "['del']\n",
      "['gobierno']\n",
      "['dirigentes']\n",
      "['presidente']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['ministro']\n",
      "['médico']\n",
      "['profesionales']\n",
      "['del']\n",
      "['SER', '##MAS']\n",
      "[]\n",
      "['Nacional']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['científico']\n",
      "['de']\n",
      "['datos']\n",
      "['médico']\n",
      "['trabajadores']\n",
      "['sanitarios']\n",
      "['cura', '##s']\n",
      "['rastre', '##adores']\n",
      "['políticos']\n",
      "['dipu', '##tada']\n",
      "['Director']\n",
      "['Vasco']\n",
      "['de']\n",
      "['Emergencia', '##s']\n",
      "['empleado']\n",
      "['POL', '##ICI', '##A']\n",
      "['jefa']\n",
      "['científica']\n",
      "['profesionales']\n",
      "['fisio', '##tera', '##peuta']\n",
      "['Fis', '##io', '##tera', '##peuta', '##s']\n",
      "['investigador']\n",
      "['profesionales']\n",
      "['periodistas']\n",
      "['escritor']\n",
      "['diputado']\n",
      "['sanitarios']\n",
      "['servidores']\n",
      "['públicos']\n",
      "['Presidenta']\n",
      "['periodistas']\n",
      "['autónomos']\n",
      "['Vicepresidente']\n",
      "['Bomb', '##ers']\n",
      "['G', '.']\n",
      "['Urban', '##a']\n",
      "['J', '##efe']\n",
      "['del']\n",
      "['Servicio']\n",
      "['de']\n",
      "['Car', '##dio', '##logía']\n",
      "['autora']\n",
      "['so', '##cor', '##rista']\n",
      "['Presidente']\n",
      "['Gobernador']\n",
      "['dipu', '##tada']\n",
      "['doctora']\n",
      "['en']\n",
      "['Medicina']\n",
      "['funcionarios']\n",
      "['de']\n",
      "['la']\n",
      "['salud']\n",
      "['trabajadores']\n",
      "['presidentes']\n",
      "['periodista']\n",
      "['Ministra']\n",
      "['Ministra']\n",
      "['ministro']\n",
      "['de']\n",
      "['Universidades']\n",
      "['profesionales']\n",
      "['de']\n",
      "['la']\n",
      "['ta', '##uro', '##ma', '##quia']\n",
      "['sanitarios']\n",
      "['trabajadores']\n",
      "['alcalde']\n",
      "['Miembros']\n",
      "['del']\n",
      "['Parlamento']\n",
      "['Europeo']\n",
      "['doctora']\n",
      "['J', '##ue', '##ces']\n",
      "['Instru', '##ctores']\n",
      "['Fiscal']\n",
      "['General']\n",
      "['personal']\n",
      "['de']\n",
      "['comercio']\n",
      "['sanitarios']\n",
      "['empresario']\n",
      "['obrero']\n",
      "['sanitarios']\n",
      "['jueces']\n",
      "['Rey']\n",
      "['Ministro']\n",
      "['presidente']\n",
      "['sanitarios']\n",
      "['alumnado']\n",
      "['parados']\n",
      "['presidenta']\n",
      "['de']\n",
      "['la']\n",
      "['región']\n",
      "['expertos']\n",
      "['médicos']\n",
      "['presidente']\n",
      "['de']\n",
      "['la']\n",
      "['OMS']\n",
      "['primer']\n",
      "['ministro']\n",
      "['Policía']\n",
      "['Local']\n",
      "['limpia', '##dora']\n",
      "['de']\n",
      "['un']\n",
      "['hospital']\n",
      "['altos']\n",
      "['cargos']\n",
      "['limpia', '##dora']\n",
      "['líderes']\n",
      "['mundiales']\n",
      "['presidente']\n",
      "['cardenal']\n",
      "['ca', '##jeras']\n",
      "['juez']\n",
      "['coronel']\n",
      "['de']\n",
      "['la']\n",
      "['GC']\n",
      "['sanitarios']\n",
      "['cura', '##s']\n",
      "['rastre', '##adores']\n",
      "['CE', '##O']\n",
      "['desempleo']\n",
      "['er', '##tes']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['catedrá', '##tico']\n",
      "['de']\n",
      "['epidemi', '##ología']\n",
      "['miembro']\n",
      "['del']\n",
      "['comité']\n",
      "['de']\n",
      "['expertos']\n",
      "['mos', '##sos']\n",
      "['sanitarios']\n",
      "['fuerzas']\n",
      "['de']\n",
      "['seguridad']\n",
      "['trabajadores']\n",
      "['autoridades']\n",
      "['sanitarias']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['desempleados']\n",
      "['Er', '##tes']\n",
      "['presidente']\n",
      "['mad', '##eros']\n",
      "['diputados']\n",
      "['diputados']\n",
      "[]\n",
      "['trabajadores']\n",
      "['fiscales']\n",
      "['trabajadores']\n",
      "['director']\n",
      "['social']\n",
      "['alcalde']\n",
      "['autoridades']\n",
      "['sanitarias']\n",
      "['empleados']\n",
      "['enfermera']\n",
      "['científicos']\n",
      "['policías']\n",
      "['trabajadoras']\n",
      "['del']\n",
      "['hogar']\n",
      "['autor']\n",
      "['escritor']\n",
      "['diputado']\n",
      "['científicos']\n",
      "['Científico', '##s']\n",
      "['trabajadores']\n",
      "['vicepresidente']\n",
      "['d']\n",
      "['asuntos']\n",
      "['sociales']\n",
      "['vicepresidente']\n",
      "['presidente']\n",
      "['Diri', '##gentes']\n",
      "['presidenta']\n",
      "['personal']\n",
      "['director']\n",
      "['general']\n",
      "['Modelo', '##s']\n",
      "['M', '##É', '##DI', '##CO', '##S']\n",
      "['Hem', '##ató', '##log', '##o']\n",
      "['tatu', '##adora']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['diputado']\n",
      "['sanitarios']\n",
      "['alcalde']\n",
      "['enfermera']\n",
      "['sanitarios']\n",
      "['periodistas']\n",
      "['mercenarios']\n",
      "['portavoz']\n",
      "['médico']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['Cardenal']\n",
      "['tor', '##eros']\n",
      "['miembros']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['médico']\n",
      "['vicepresidente']\n",
      "['sanitarios']\n",
      "['autónoma']\n",
      "['secu', '##ra', '##tas']\n",
      "['Ministro']\n",
      "['de']\n",
      "['Salud']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['entrenador']\n",
      "['de']\n",
      "['fútbol']\n",
      "['Entren', '##adores']\n",
      "['compañeros']\n",
      "['tor', '##ero']\n",
      "['epidemi', '##ólogos']\n",
      "['fundador']\n",
      "['sanitaria']\n",
      "['empresarios']\n",
      "['dirigentes']\n",
      "['bomberos']\n",
      "['vir', '##ólogo']\n",
      "['periodista']\n",
      "['Vicepresidente']\n",
      "['de']\n",
      "['la']\n",
      "['J', '##unta']\n",
      "['Equipo']\n",
      "['de']\n",
      "[]\n",
      "['Re', '##habilitación', '##y', '##F', '##is', '##io', '##terapia']\n",
      "[]\n",
      "['ER', '##TE']\n",
      "['empleados']\n",
      "['##0']\n",
      "['Sol', '##dados']\n",
      "['médicos']\n",
      "['políticos']\n",
      "['subsec', '##re', '##tario']\n",
      "['alcalde', '##s']\n",
      "['alcalde', '##sas']\n",
      "['sanitario']\n",
      "['empleadas']\n",
      "['de']\n",
      "['hogar']\n",
      "['Enferme', '##ro']\n",
      "['autónomos']\n",
      "[]\n",
      "['auton', '##omos']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', '.', 'C', '.']\n",
      "['ex']\n",
      "['vicepresidente']\n",
      "['médicos']\n",
      "['residentes']\n",
      "['adjunto', '##s']\n",
      "['senadores']\n",
      "['Presidente']\n",
      "['de']\n",
      "['la']\n",
      "['República']\n",
      "['alcalde']\n",
      "['sanitarios']\n",
      "['científicos']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['trabajadores']\n",
      "['Policía']\n",
      "['Bomb', '##eros']\n",
      "['Médico', '##s']\n",
      "['Enferme', '##ros']\n",
      "['Auxiliar', '##es']\n",
      "['taxi', '##stas']\n",
      "['militares']\n",
      "['Personal']\n",
      "['de']\n",
      "['supermercado', '##s']\n",
      "['farmacéuticos']\n",
      "['administrativos']\n",
      "['Ministros']\n",
      "['investigadores']\n",
      "['investigadora', '##s']\n",
      "['filósofo']\n",
      "['primera']\n",
      "['dama']\n",
      "['escritor']\n",
      "['trabajadores']\n",
      "['ER', '##TE']\n",
      "['Ministra']\n",
      "['personal']\n",
      "['presidente']\n",
      "['de']\n",
      "['la']\n",
      "['J', '##unta']\n",
      "['especul', '##adores']\n",
      "['sanitarios']\n",
      "['juez', '##a']\n",
      "['presidente']\n",
      "['de']\n",
      "['la']\n",
      "['Generalitat']\n",
      "['medi', '##co']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['autores']\n",
      "['de']\n",
      "['este']\n",
      "['blog']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['ministros']\n",
      "['presidente']\n",
      "['jefe']\n",
      "['de']\n",
      "['Opin', '##ión']\n",
      "['Ter', '##tul', '##iano']\n",
      "['Guardia', '##C', '##iv', '##il']\n",
      "['Embajador']\n",
      "['minist', '##ra']\n",
      "['de']\n",
      "['Igualdad']\n",
      "['vicepresidente']\n",
      "['2', '##o']\n",
      "['trabajadores']\n",
      "['secretaria']\n",
      "['general']\n",
      "['emperador']\n",
      "['PA', '##Y', '##AS', '##OS']\n",
      "['científico']\n",
      "['Ministro']\n",
      "['abogados']\n",
      "['mos', '##sos']\n",
      "['apoyo']\n",
      "['enferme', '##ro']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['Rey']\n",
      "['em', '##éri', '##to']\n",
      "['secretario']\n",
      "['general']\n",
      "['Policía']\n",
      "['trabajadores']\n",
      "[]\n",
      "['as']\n",
      "['manuales']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['Policía']\n",
      "['Local']\n",
      "['mercen', '##ario']\n",
      "['Mé', '##dic', 's']\n",
      "['Enferme', '##r']\n",
      "['TC', '##A', '##E']\n",
      "['TE', '##S']\n",
      "['Cel', '##adores']\n",
      "['Bomb', '##er', 's']\n",
      "['Dic', '##tador']\n",
      "['cura', '##s']\n",
      "['profesores']\n",
      "['parados']\n",
      "['sanitarios']\n",
      "['médico']\n",
      "['Conduc', '##tor']\n",
      "['médico']\n",
      "['médico']\n",
      "['Médico', '##s']\n",
      "['Residen', '##tes']\n",
      "['especialistas']\n",
      "['autoridades']\n",
      "['sanitarias']\n",
      "['delegado']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['sanitarios']\n",
      "['trabajadores']\n",
      "['personal']\n",
      "['director']\n",
      "['del']\n",
      "['periódico']\n",
      "[]\n",
      "['tempor', '##eros']\n",
      "['rastre', '##adores']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['cantante']\n",
      "['epidemi', '##ólogo']\n",
      "['tempor', '##er', 's']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['profesionales']\n",
      "['autoridades']\n",
      "['sanitarias']\n",
      "['alcalde']\n",
      "['médicos']\n",
      "['son']\n",
      "['maestros']\n",
      "['empleados']\n",
      "['ER', '##TES']\n",
      "['Directora']\n",
      "['General']\n",
      "['LA', '##DR', '##ONES']\n",
      "['Presidente']\n",
      "['Presidenta']\n",
      "[]\n",
      "['Agentes', '##F', '##ores', '##tales', '##C', '##M']\n",
      "['Protección']\n",
      "['Civil']\n",
      "['portavoz']\n",
      "['pens', '##ion', '##istas']\n",
      "['Diputados']\n",
      "['arquitec', '##ta']\n",
      "['policías']\n",
      "['locales']\n",
      "['empleado']\n",
      "['sanitarios']\n",
      "['consejera']\n",
      "['gestor', '##es']\n",
      "['sanitarios']\n",
      "['enferme', '##ro']\n",
      "['militares']\n",
      "['Agentes']\n",
      "['Medio', '##ambi', '##entales']\n",
      "[]\n",
      "['AA', '##M']\n",
      "['líderes']\n",
      "['políticos']\n",
      "['enfermeras']\n",
      "['enferme', '##ros']\n",
      "['presidente']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['presidente']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['divul', '##gadores']\n",
      "['reportero']\n",
      "['estudiante']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['alumnos']\n",
      "['alumnos']\n",
      "['profesor']\n",
      "['jefe']\n",
      "['de']\n",
      "['epidemi', '##plo', '##gía']\n",
      "['compositor']\n",
      "['ER', '##TE']\n",
      "['empresario']\n",
      "['trabajadores']\n",
      "['anal', '##ista']\n",
      "['político']\n",
      "['tor', '##ero']\n",
      "['ministros']\n",
      "['de']\n",
      "['turismo']\n",
      "['primer']\n",
      "['ministro']\n",
      "['empresario']\n",
      "['persona']\n",
      "['de']\n",
      "['ciencia']\n",
      "['sacerdote']\n",
      "['médico']\n",
      "['car', '##dió', '##log', '##o']\n",
      "['MI', '##R']\n",
      "['médico']\n",
      "['científico']\n",
      "['lac', '##ayo']\n",
      "['exal', '##cal', '##de']\n",
      "['Dr', '.']\n",
      "['Pres', '##i']\n",
      "['jefe']\n",
      "['de']\n",
      "['Crí', '##ticos']\n",
      "['Qui', '##rúr', '##gicos']\n",
      "['médicos']\n",
      "['científicos']\n",
      "['jugadores']\n",
      "['sanitarios']\n",
      "['médicos']\n",
      "['enfermeras']\n",
      "['auxiliares']\n",
      "['policías']\n",
      "['guardias']\n",
      "['militares']\n",
      "['bomberos']\n",
      "['asistentes']\n",
      "['sociales']\n",
      "['sanitarios']\n",
      "['equipo']\n",
      "['de']\n",
      "['expertos']\n",
      "['director']\n",
      "['W', '##ed', '##ding']\n",
      "['plan', '##ner']\n",
      "[]\n",
      "['Polic', '##ia', '##N', '##acional']\n",
      "[]\n",
      "['Guardia', '##C', '##iv', '##il']\n",
      "['profesionales']\n",
      "['en']\n",
      "['régimen']\n",
      "['de']\n",
      "['exp', '##atri', '##ación']\n",
      "['médicos']\n",
      "['presidenta']\n",
      "['presidente']\n",
      "['despedido', '##s']\n",
      "['Policía']\n",
      "['Local']\n",
      "['Bomb', '##eros']\n",
      "['científico']\n",
      "['portavoz', '##a']\n",
      "['del']\n",
      "['gobierno']\n",
      "['ministro']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['trabajadores']\n",
      "['minist', '##ra']\n",
      "['minist', '##ra']\n",
      "['trabajadores']\n",
      "['canciller']\n",
      "['alcalde']\n",
      "['actor']\n",
      "[]\n",
      "['auton', '##omos']\n",
      "['médicos']\n",
      "['dipu', '##tada']\n",
      "['juez']\n",
      "['policia']\n",
      "['responsable']\n",
      "['de']\n",
      "['control']\n",
      "['jugadores']\n",
      "['militares']\n",
      "['directora']\n",
      "['de']\n",
      "[]\n",
      "['Data', '##S', '##cien', '##ce']\n",
      "['Dr', '.']\n",
      "['Dr', '.']\n",
      "['parados']\n",
      "['ER', '##TES']\n",
      "['Autón', '##omos']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['presidente']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['integrantes']\n",
      "['del']\n",
      "['Ejército']\n",
      "['de']\n",
      "['Tierra']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['parados']\n",
      "['chofer']\n",
      "['Secretario']\n",
      "['de']\n",
      "['Salud']\n",
      "['médico']\n",
      "['científico']\n",
      "['minist', '##ra']\n",
      "['de']\n",
      "['Trabajo']\n",
      "['médicos']\n",
      "['médico']\n",
      "['tripul', '##aciones']\n",
      "['profesionales']\n",
      "['de']\n",
      "['canal', '##su', '##r']\n",
      "['ministro']\n",
      "['Ingeniero', '##s']\n",
      "['científicos']\n",
      "['colaboradores']\n",
      "['conductores']\n",
      "['Técnicos']\n",
      "['de']\n",
      "['Emergencia', '##s']\n",
      "['San', '##itarias']\n",
      "[]\n",
      "['TE', '##S']\n",
      "['policía']\n",
      "['secretario']\n",
      "['de']\n",
      "['Salud']\n",
      "['Pública']\n",
      "['funcionario']\n",
      "['ladrón']\n",
      "['empresario']\n",
      "['presidenta']\n",
      "['de']\n",
      "['la']\n",
      "['Comisión']\n",
      "['Europea']\n",
      "['sanitarios']\n",
      "['dependientes']\n",
      "['##S']\n",
      "['Depen', '##dientes']\n",
      "['consejero']\n",
      "['de']\n",
      "['Salud']\n",
      "['Vicepres', '##identa']\n",
      "['Ejecutiva']\n",
      "['alcalde']\n",
      "['Secretaria']\n",
      "['de']\n",
      "['Salud']\n",
      "['matr', '##ona']\n",
      "['trabajadores']\n",
      "['Rey']\n",
      "['alcalde', '##sa']\n",
      "['médico']\n",
      "['Prim', '##e']\n",
      "['Minister']\n",
      "['policía']\n",
      "['presidente']\n",
      "['trabajadores']\n",
      "['camarera']\n",
      "['sanitarios']\n",
      "['consejero']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['policia']\n",
      "['Policía']\n",
      "['Local']\n",
      "['compañeros']\n",
      "['sanitarios']\n",
      "['policías']\n",
      "['cami', '##oner', '##os']\n",
      "['Trabaja', '##dora']\n",
      "['de']\n",
      "['hos', '##tele', '##ría']\n",
      "['médicos']\n",
      "['alumno']\n",
      "['alumno']\n",
      "['alumnos']\n",
      "['matemático', '##s']\n",
      "['Científico', '##s']\n",
      "['estudiantes']\n",
      "['compañeros']\n",
      "['portavoz']\n",
      "['tatu', '##adora']\n",
      "['doctor']\n",
      "['PER', '##SON', '##AL']\n",
      "['DE']\n",
      "['QU', '##IR', '##Ó', '##FA', '##NO']\n",
      "['Capitán']\n",
      "[]\n",
      "['Medic', '##os']\n",
      "[]\n",
      "['médico']\n",
      "['rastre', '##adores']\n",
      "['trabajadores']\n",
      "['autónomos']\n",
      "['juez', '##a']\n",
      "['experto']\n",
      "['Er', '##tz', '##ain', '##tz', '##a']\n",
      "['Investi', '##gadores']\n",
      "['médico']\n",
      "['sanitarios']\n",
      "['Enfer', '##mos']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['deportista']\n",
      "['deportistas']\n",
      "['actores']\n",
      "['tele', '##traba', '##jo']\n",
      "['rey']\n",
      "['doctora']\n",
      "['futbolista', '##s']\n",
      "['futbolista']\n",
      "['ministro']\n",
      "['del']\n",
      "['Interior']\n",
      "['coronel']\n",
      "['de']\n",
      "['la']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['gin', '##ec', '##óloga']\n",
      "['empleado']\n",
      "['portavoz']\n",
      "['nacional']\n",
      "['autoridades']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['Profesional', '##es']\n",
      "['de']\n",
      "['la']\n",
      "['Salud']\n",
      "['Ministra']\n",
      "['de']\n",
      "['Igualdad']\n",
      "['Polic', '##ia']\n",
      "['Barre', '##n', '##deros']\n",
      "['Seguridad']\n",
      "['servidores']\n",
      "['públicos']\n",
      "['periodista']\n",
      "['científica']\n",
      "['profesionales']\n",
      "['del']\n",
      "['Hospital']\n",
      "['Tempor', '##eros']\n",
      "['bra', '##ceros']\n",
      "['jorn', '##ale', '##ros']\n",
      "['ca', '##jera']\n",
      "['en']\n",
      "['un']\n",
      "['supermercado']\n",
      "['presos']\n",
      "['médicos']\n",
      "['efectivos']\n",
      "['de']\n",
      "['seguridad']\n",
      "['trabajadores']\n",
      "['ficti', '##cios']\n",
      "['ER', '##TE', '##s']\n",
      "['Dr', '.']\n",
      "['autónomos']\n",
      "['presidente']\n",
      "['Médico', '##s']\n",
      "['trabajadores']\n",
      "['vicepres', '##identa']\n",
      "['ladrones']\n",
      "['presidenta']\n",
      "['portavoz']\n",
      "['presidente']\n",
      "['Mos', '##sos']\n",
      "['alcalde']\n",
      "['trabajadores']\n",
      "['ER', '##TE']\n",
      "['arque', '##óloga']\n",
      "['bí', '##blica']\n",
      "['sanitarios']\n",
      "['dictador']\n",
      "['médico']\n",
      "['doctor']\n",
      "['cargos']\n",
      "['del']\n",
      "['PP']\n",
      "['presidente']\n",
      "['de']\n",
      "['Nuevas']\n",
      "['Gener', '##aciones']\n",
      "['conductor']\n",
      "['de']\n",
      "['autobús']\n",
      "['cham', '##anes']\n",
      "['científicos']\n",
      "['gu', '##r', '##ús']\n",
      "['presidente']\n",
      "['pedi', '##at', '##ra']\n",
      "['políticos']\n",
      "['políticos']\n",
      "['arzobispo']\n",
      "['trabajadores']\n",
      "[]\n",
      "['ru', '##n', '##ners']\n",
      "['exal', '##cal', '##de']\n",
      "['universitarios']\n",
      "['estudiantes']\n",
      "['de']\n",
      "['f', '##p']\n",
      "['cura']\n",
      "['cantante']\n",
      "['arzobispo']\n",
      "['bomberos']\n",
      "['presidente']\n",
      "['ministros']\n",
      "['sanitarios']\n",
      "['diputados']\n",
      "['parlamentarios']\n",
      "['Rey']\n",
      "['Enferme', '##ras']\n",
      "['Doctora', '##s']\n",
      "['Personal']\n",
      "['de']\n",
      "['limpieza']\n",
      "['Perio', '##distas']\n",
      "['Conduc', '##toras']\n",
      "['Trabaja', '##doras']\n",
      "[]\n",
      "['es']\n",
      "['de']\n",
      "['supermercado', '##s']\n",
      "['So', '##cor', '##ristas']\n",
      "['empleadas']\n",
      "['empleados']\n",
      "['sanitarios']\n",
      "['ER', '##TES']\n",
      "['enferme', '##ro']\n",
      "['sin']\n",
      "['empleo']\n",
      "['Vicepresidente']\n",
      "['Policía']\n",
      "['Nacional']\n",
      "['GU', '##ARD', '##IAS']\n",
      "['CI', '##VI', '##LE', '##S']\n",
      "['##a']\n",
      "['chef']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['taxi', '##stas']\n",
      "['sanitarios']\n",
      "['taxi', '##stas']\n",
      "['empresario']\n",
      "['sanitarios']\n",
      "['gerente']\n",
      "['jubi', '##lada']\n",
      "['árbitro', '##s']\n",
      "['guardia']\n",
      "['civil']\n",
      "['guardia']\n",
      "['civil']\n",
      "['Trabajadores']\n",
      "['compañera']\n",
      "['traba', '##ya', '##dores']\n",
      "['públicos']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['cantante']\n",
      "['policia']\n",
      "['mos', '##sos']\n",
      "['rastre', '##ador']\n",
      "['alumnado']\n",
      "['alcalde', '##sa']\n",
      "['conductor']\n",
      "['del']\n",
      "['bus']\n",
      "['personas']\n",
      "['trabajadoras']\n",
      "['por']\n",
      "['cuenta']\n",
      "['propia']\n",
      "[]\n",
      "['autónomas']\n",
      "['pres', '##bí', '##tero']\n",
      "['dictador']\n",
      "['FUN', '##CI', '##ONA', '##RI', '##OS']\n",
      "['DE']\n",
      "['PR', '##ISI', '##ONES']\n",
      "['desempleo']\n",
      "['director']\n",
      "['en']\n",
      "['paro']\n",
      "['sanitarios']\n",
      "['alumno']\n",
      "['médicos']\n",
      "['doctora']\n",
      "['periodistas']\n",
      "['autónomos']\n",
      "['Er', '##tes']\n",
      "['autónomos']\n",
      "['trabajadores']\n",
      "['ER', '##TE']\n",
      "['diputados']\n",
      "['diputados']\n",
      "['operar', '##ios']\n",
      "['técnicos']\n",
      "['Papa']\n",
      "['asesores']\n",
      "['agentes']\n",
      "['sociales']\n",
      "['miembros']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['Policía']\n",
      "['Nacional']\n",
      "['ministro']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['director']\n",
      "['bailar', '##ines']\n",
      "['gobernador']\n",
      "['rastre', '##adores']\n",
      "['Presidente']\n",
      "['del']\n",
      "['gobierno']\n",
      "['Policía']\n",
      "['enferme', '##ro']\n",
      "['médico']\n",
      "['médico']\n",
      "['enfermera']\n",
      "['rey']\n",
      "['agente']\n",
      "['cív', '##ico']\n",
      "['policia']\n",
      "['influen', '##cer', '##s']\n",
      "['expertos']\n",
      "['económicos']\n",
      "['Director']\n",
      "['General']\n",
      "['de']\n",
      "['la']\n",
      "['policia']\n",
      "['conse', '##ller', '##a']\n",
      "['conse', '##ller', '##a']\n",
      "['juez']\n",
      "['Presidente']\n",
      "['Médico', '##s']\n",
      "['estudiantes']\n",
      "['manda', '##tario']\n",
      "['##d']\n",
      "['ta', '##tto', '##ist']\n",
      "['policía']\n",
      "['policías']\n",
      "['trabajador', 's']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['sanitarios']\n",
      "['policía']\n",
      "['guardia']\n",
      "['civil']\n",
      "['um', '##e']\n",
      "['cel', '##adores']\n",
      "['limpia', '##doras']\n",
      "['Directora']\n",
      "['del']\n",
      "['Instituto']\n",
      "['Salud']\n",
      "['portavoz']\n",
      "['de']\n",
      "['la']\n",
      "['Embajada']\n",
      "['Investi', '##gadores']\n",
      "['sanitarios']\n",
      "['juez']\n",
      "['guardias']\n",
      "['civiles']\n",
      "['directora']\n",
      "['enfermeras']\n",
      "['enferme', '##ros']\n",
      "['fisio', '##tera', '##peuta', '##s']\n",
      "['sanitarios']\n",
      "['diputados']\n",
      "['periodistas']\n",
      "[]\n",
      "['trabajadores']\n",
      "['Policía']\n",
      "['Presidente']\n",
      "['gobernantes']\n",
      "['jugadores']\n",
      "['empleados']\n",
      "['Público', '##s']\n",
      "['personal']\n",
      "['políticos']\n",
      "['consejera']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['##3']\n",
      "['tele', '##traba', '##jo']\n",
      "['delegados']\n",
      "['de']\n",
      "[]\n",
      "['CC', '##O', '##O']\n",
      "['ger', '##oc', '##ult', '##ora']\n",
      "['sanitarios']\n",
      "['anestes', '##ista']\n",
      "['estudiantes']\n",
      "['médico']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['minist', '##ra']\n",
      "['filósofo']\n",
      "['ministro']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['presidente']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['médicos']\n",
      "['alcalde']\n",
      "['animado', '##ra']\n",
      "['cirujano']\n",
      "['cardíaco']\n",
      "['policia']\n",
      "['policía']\n",
      "['policia']\n",
      "['sanitarios']\n",
      "['minist', '##ra']\n",
      "['diputados']\n",
      "['Ministra']\n",
      "['policía']\n",
      "['reyes']\n",
      "['director']\n",
      "['del']\n",
      "['Laboratorio']\n",
      "['de']\n",
      "['Micro', '##bio', '##logía']\n",
      "['compañera']\n",
      "['directora']\n",
      "['del']\n",
      "['hospital']\n",
      "['asesor']\n",
      "['conductores']\n",
      "['políticos']\n",
      "['SS', '##M', '##M']\n",
      "['Los']\n",
      "['Reyes']\n",
      "['periodista']\n",
      "['personal']\n",
      "['dependiente']\n",
      "['del']\n",
      "['Ministerio']\n",
      "['presidente']\n",
      "['primera']\n",
      "['dama']\n",
      "['agricultores']\n",
      "['gana', '##deros']\n",
      "['guardia']\n",
      "['de']\n",
      "['seguridad']\n",
      "['ex', '##com', '##isa', '##rio']\n",
      "['de']\n",
      "['policía']\n",
      "['personal']\n",
      "['alcalde']\n",
      "['Dra', '.']\n",
      "['presidente']\n",
      "['de']\n",
      "['la']\n",
      "['Fe', '##d']\n",
      "['Reyes']\n",
      "['autoridades']\n",
      "['religiosas']\n",
      "['personal']\n",
      "[]\n",
      "['trabajadores']\n",
      "['de']\n",
      "['la']\n",
      "['Sanidad']\n",
      "['enfermera']\n",
      "['enfermera']\n",
      "['auxiliar']\n",
      "['autónomos']\n",
      "[]\n",
      "['auton', '##omos']\n",
      "['trabajadores']\n",
      "['ER', '##TES']\n",
      "['trabajadores']\n",
      "['ER', '##TES']\n",
      "['Dr', '.']\n",
      "['secretario']\n",
      "['general']\n",
      "['vicepresidente']\n",
      "['de']\n",
      "['la']\n",
      "['Comisión']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['portavoz']\n",
      "['parlamentario']\n",
      "['Reyes']\n",
      "['minist', '##ra']\n",
      "['de']\n",
      "['Defensa', '##go', '##b']\n",
      "['farmacéuticos']\n",
      "['pedi', '##at', '##ras']\n",
      "['trabajadores']\n",
      "['autónomos']\n",
      "['temporales']\n",
      "['domésticos']\n",
      "['desempleados']\n",
      "['voluntarios']\n",
      "[]\n",
      "['ingenieros', '##ind', '##ust', '##ria', '##les']\n",
      "['general']\n",
      "['inmun', '##ólogo']\n",
      "['militares']\n",
      "['técnicos']\n",
      "['del']\n",
      "['Depar', '##tamen', '##t']\n",
      "['de']\n",
      "['Sal', '##ut']\n",
      "['Ministro']\n",
      "['de']\n",
      "['Cultura']\n",
      "['papa', '##ra', '##zzi', '##s']\n",
      "['periodistas']\n",
      "['autónomos']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['juez', '##a']\n",
      "['epidemi', '##ólogo']\n",
      "['profesor']\n",
      "['de']\n",
      "['la']\n",
      "['Universidad']\n",
      "['Presidente']\n",
      "['del']\n",
      "['Principado']\n",
      "['de']\n",
      "['Asturias']\n",
      "['Pit', '##oni', '##sa']\n",
      "['jefe']\n",
      "['ER', '##TE']\n",
      "['médicos']\n",
      "['vicepres', '##identa']\n",
      "['Alcalde']\n",
      "['cor', '##sar', '##ios']\n",
      "['reyes']\n",
      "['presidenta']\n",
      "['ex', '##con', '##ce', '##jal']\n",
      "['sanitarios']\n",
      "['SAN', '##ITAR', '##IOS']\n",
      "['juez']\n",
      "['juez', '##a']\n",
      "['presidente']\n",
      "[]\n",
      "['sanitarios']\n",
      "['modelo']\n",
      "['rid', '##ers']\n",
      "[]\n",
      "['dentista']\n",
      "['alcalde']\n",
      "['guardia', '##ci', '##vil']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['presidente']\n",
      "['gobernantes']\n",
      "['gestor', '##es']\n",
      "['policía']\n",
      "['nacional']\n",
      "['Polic', '##ia', '##N', '##acional']\n",
      "['policia']\n",
      "['co', '##pres', '##idente']\n",
      "['Rey']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['presidente']\n",
      "['periodistas']\n",
      "['profesor']\n",
      "['vir', '##ólogo']\n",
      "['empren', '##dedores']\n",
      "['Presidente']\n",
      "['presidente']\n",
      "['médico']\n",
      "['neuro', '##cir', '##uj', '##ano']\n",
      "['Dr']\n",
      "[]\n",
      "['Enferme', '##ra']\n",
      "['enfermeras']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['policías']\n",
      "['Reyes']\n",
      "['autónomo']\n",
      "['empren', '##dedor']\n",
      "['presidente']\n",
      "['Agentes']\n",
      "['director']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['conductores']\n",
      "['de']\n",
      "['ambulancia', '##s']\n",
      "['MI', '##LI', '##TA', '##R']\n",
      "['forense']\n",
      "['juez']\n",
      "['docentes']\n",
      "['alumnos']\n",
      "['sanitarios']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['rastre', '##adores']\n",
      "['médicos']\n",
      "['de']\n",
      "['Atención']\n",
      "['Prima', '##ria']\n",
      "['Em', '##éri', '##to']\n",
      "['tele', '##traba', '##jo']\n",
      "['limpia', '##dora']\n",
      "['directivos']\n",
      "['del']\n",
      "['hospital']\n",
      "['presidenta']\n",
      "['Médico', '##s']\n",
      "['escolta', '##s']\n",
      "['fotógrafo', '##s']\n",
      "['investigadores']\n",
      "['ER', '##TE', '##s']\n",
      "['autónomos']\n",
      "['ER', '##TE']\n",
      "['ministro']\n",
      "['alcalde']\n",
      "['rastre', '##adores']\n",
      "['econom', '##ista']\n",
      "['autónomos']\n",
      "['preca', '##rios']\n",
      "['sanitarios']\n",
      "[]\n",
      "['Guardia', '##C', '##iv', '##il']\n",
      "['profesionales']\n",
      "['presidentes']\n",
      "['de']\n",
      "['CC', '##A', '##A']\n",
      "['presidente']\n",
      "['del']\n",
      "['gobierno']\n",
      "['jugadores']\n",
      "['Inf', '##ov', '##log', '##ger']\n",
      "['primer']\n",
      "['ministro']\n",
      "['Repar', '##tidor', '##es']\n",
      "['J', '##efa']\n",
      "['de']\n",
      "['ventas']\n",
      "['agente']\n",
      "['inmobili', '##aria']\n",
      "['tele', '##op', '##era', '##dora']\n",
      "['ministro']\n",
      "['Papa']\n",
      "['cura']\n",
      "['Presidenta']\n",
      "['de']\n",
      "['las']\n",
      "['Residen', '##cias']\n",
      "['senador']\n",
      "['ministro']\n",
      "['GC']\n",
      "['policía']\n",
      "['GC']\n",
      "['t', '##ca', '##e']\n",
      "['cel', '##ador']\n",
      "['agricultores']\n",
      "['escolta']\n",
      "['líderes']\n",
      "['mundiales']\n",
      "['médicos']\n",
      "['sanitarios']\n",
      "['Vice', '##minist', '##ro']\n",
      "['de']\n",
      "['Salud']\n",
      "['médico']\n",
      "['astrona', '##utas']\n",
      "['médico']\n",
      "['políticos']\n",
      "['ER', '##TE']\n",
      "['ER', '##TE']\n",
      "['funcionario']\n",
      "['policía']\n",
      "['trabajadores']\n",
      "['sanitarios']\n",
      "['políticos']\n",
      "['miembros']\n",
      "['de']\n",
      "['la']\n",
      "['prensa']\n",
      "['tempor', '##eros']\n",
      "['médicos']\n",
      "['Rey']\n",
      "['em', '##éri', '##to']\n",
      "['minist', '##ra']\n",
      "['trabajadores']\n",
      "['empleados']\n",
      "['empleados']\n",
      "['cron', '##ista']\n",
      "['alcalde']\n",
      "['ER', '##TE']\n",
      "['médico']\n",
      "['de']\n",
      "['familia']\n",
      "['políticos']\n",
      "['conductores']\n",
      "['trabajadores']\n",
      "['policía']\n",
      "['Dr', '.']\n",
      "['jefe']\n",
      "['de']\n",
      "['Ries', '##gos']\n",
      "['Laboral', '##es']\n",
      "['Policía']\n",
      "['policía']\n",
      "['trabajadores']\n",
      "['directivo']\n",
      "['de']\n",
      "['un']\n",
      "['hospital']\n",
      "['MEDIC', '##O']\n",
      "['político']\n",
      "['compañeros']\n",
      "['conductores']\n",
      "['conductores']\n",
      "['expertos']\n",
      "[]\n",
      "['CO', '##VI', '##D', '##19']\n",
      "['subsec', '##re', '##tario']\n",
      "['conductor']\n",
      "['de']\n",
      "['U', '##ber']\n",
      "['taxi', '##sta']\n",
      "['guardia', '##ci', '##vil']\n",
      "['Presidente']\n",
      "['médicos']\n",
      "['compañeros']\n",
      "['rastre', '##adores']\n",
      "['rastre', '##adores']\n",
      "['médico']\n",
      "['Presidenta']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['residentes']\n",
      "['médicos']\n",
      "['de']\n",
      "['centros']\n",
      "['de']\n",
      "['salud']\n",
      "['sanitarios']\n",
      "['trabajadores']\n",
      "['Doctor']\n",
      "['inversores']\n",
      "['empleados']\n",
      "['personal']\n",
      "['de']\n",
      "['la']\n",
      "['Fiscalía']\n",
      "['General']\n",
      "['presidentes']\n",
      "['deportistas']\n",
      "['P', '##te']\n",
      "['.']\n",
      "['dueño']\n",
      "['de']\n",
      "['una']\n",
      "['funer', '##aria']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['presidente']\n",
      "['periodista']\n",
      "['alumnos']\n",
      "['profesores']\n",
      "['limpia', '##doras']\n",
      "['alumnos']\n",
      "['profesores']\n",
      "['limpia', '##doras']\n",
      "['ca', '##jera']\n",
      "['trabajadoras']\n",
      "['Dr', '.']\n",
      "['médicos']\n",
      "['auxiliar']\n",
      "['de']\n",
      "['enfermería']\n",
      "['trabajadores']\n",
      "[]\n",
      "['ru', '##n', '##ners']\n",
      "['Presidente']\n",
      "['presidente']\n",
      "['ministros']\n",
      "['guardia', '##ci', '##vil']\n",
      "['Policía']\n",
      "['Nacional']\n",
      "['compañeros']\n",
      "['portavoz']\n",
      "['del']\n",
      "['gobierno']\n",
      "['Diri', '##gentes']\n",
      "['corresponsal']\n",
      "['médico']\n",
      "['autónomos']\n",
      "[]\n",
      "['magistrado']\n",
      "['director']\n",
      "['monarca']\n",
      "['Rey']\n",
      "['mato', '##nes']\n",
      "['a']\n",
      "['sueldo']\n",
      "['policia']\n",
      "['guardia', '##ci', '##vil']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['econom', '##ista']\n",
      "['escritor']\n",
      "['arzobispo']\n",
      "['Policía']\n",
      "['arzobispo']\n",
      "['policia']\n",
      "['sin']\n",
      "['trabajo']\n",
      "['trabajador']\n",
      "['Rey']\n",
      "['Rey']\n",
      "['guardia', '##ci', '##vil']\n",
      "['operar', '##io']\n",
      "['de']\n",
      "['Protección']\n",
      "['Civil']\n",
      "['vir', '##ólogo']\n",
      "['experto']\n",
      "['en']\n",
      "['corona', '##vir', '##us']\n",
      "['policías']\n",
      "['guardias']\n",
      "['civiles']\n",
      "['personal']\n",
      "['de']\n",
      "['riesgo']\n",
      "['ministro']\n",
      "['policía']\n",
      "['alcalde']\n",
      "['limpia', '##doras']\n",
      "['de']\n",
      "['hospitales']\n",
      "['personal']\n",
      "['rastre', '##adores']\n",
      "['minist', '##ra']\n",
      "['portavoz']\n",
      "['miembros']\n",
      "['del']\n",
      "['gobierno']\n",
      "['ministro']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['Presidente']\n",
      "['Policía']\n",
      "['D', '##J']\n",
      "['trabajadores']\n",
      "['investigadores']\n",
      "['profesionales']\n",
      "['del']\n",
      "['sector']\n",
      "['presentador', '##a']\n",
      "['Policía', '##s']\n",
      "['agentes']\n",
      "['investigador']\n",
      "['director']\n",
      "['ladrón']\n",
      "['policía']\n",
      "['nin', '##ja']\n",
      "['agentes']\n",
      "['de']\n",
      "['la']\n",
      "['policía']\n",
      "['guardia']\n",
      "['civil']\n",
      "['médicos']\n",
      "['compañeros']\n",
      "['sanitarios']\n",
      "['jubi', '##lados']\n",
      "['presidente']\n",
      "['portavoz']\n",
      "['de']\n",
      "['la']\n",
      "['Generalitat']\n",
      "['periodistas']\n",
      "['cient', '##ific', 's']\n",
      "['investigadores']\n",
      "['er', '##tes']\n",
      "['presidente']\n",
      "['empleados']\n",
      "['coordinador']\n",
      "['del']\n",
      "['equipo']\n",
      "['DC', '##I']\n",
      "['mono', '##log', '##uis', '##ta']\n",
      "['actriz']\n",
      "['ER', '##TE']\n",
      "['sanitarios']\n",
      "['vicepresidente']\n",
      "['ministro']\n",
      "['de']\n",
      "['consumo']\n",
      "['Rey']\n",
      "['médico']\n",
      "['directora']\n",
      "['del']\n",
      "['Centro']\n",
      "['Nacional']\n",
      "['sanitarios']\n",
      "['Cardenal']\n",
      "['bomberos']\n",
      "['guardias']\n",
      "['de']\n",
      "['seguridad']\n",
      "['empleados']\n",
      "['presi']\n",
      "['empleados']\n",
      "['tele', '##traba', '##jando']\n",
      "['doctor']\n",
      "['profe']\n",
      "['policia']\n",
      "['agentes']\n",
      "['historiadores']\n",
      "['restaur', '##adores']\n",
      "['subsec', '##re', '##tario']\n",
      "['de']\n",
      "['salud']\n",
      "['mad', '##ero']\n",
      "['agente']\n",
      "['ER', '##TE']\n",
      "['ER', '##TE']\n",
      "['rey']\n",
      "['Ministros']\n",
      "['presidente']\n",
      "['manda', '##tario']\n",
      "['ingenieros']\n",
      "['senador']\n",
      "[]\n",
      "['foto', '##grafo']\n",
      "['guardia', '##ci', '##vil']\n",
      "['policia']\n",
      "['COMISI', '##ON', '##ISTA']\n",
      "['guardia', '##ci', '##vil']\n",
      "['estudiantes']\n",
      "[]\n",
      "['Estudiantes']\n",
      "['presidente']\n",
      "['legi', '##onarios']\n",
      "['tric', '##orn', '##ios']\n",
      "['##M']\n",
      "['Ac', '##tores']\n",
      "[]\n",
      "['ac', '##trices']\n",
      "[]\n",
      "['Director', '##es']\n",
      "['enfermera']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['diputados']\n",
      "['diputados']\n",
      "['director']\n",
      "['personas']\n",
      "['trabajadoras']\n",
      "['autora', '##s']\n",
      "['profesores']\n",
      "['presidenta']\n",
      "['Dr']\n",
      "['doctor']\n",
      "['cantante']\n",
      "['bailar', '##ín']\n",
      "['médica']\n",
      "['J', '##efa']\n",
      "['del']\n",
      "['Servicio']\n",
      "['de']\n",
      "['Dig', '##estiv', '##o']\n",
      "['Ministra']\n",
      "['de']\n",
      "['Igualdad']\n",
      "['ER', '##TE']\n",
      "['trabajador']\n",
      "['ER', '##TES']\n",
      "['autónomos']\n",
      "['pequeño']\n",
      "['empresarios']\n",
      "['sanitarios']\n",
      "['estudiantes']\n",
      "['portavoz']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['miembros']\n",
      "['de']\n",
      "['las']\n",
      "['FA', '##S']\n",
      "['ER', '##TE']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['Presidenta']\n",
      "['mos', '##sos']\n",
      "['presidentes']\n",
      "['auton', '##ómico', '##s']\n",
      "['sanitarios']\n",
      "['Policía']\n",
      "['Local']\n",
      "['Conse', '##jera']\n",
      "['Medio']\n",
      "['Ambiente']\n",
      "['juez', '##a']\n",
      "['propietarios']\n",
      "['de']\n",
      "['vivienda']\n",
      "['presidente']\n",
      "['personal']\n",
      "['esencial']\n",
      "['policía']\n",
      "['segura', '##tas']\n",
      "['policías']\n",
      "['alcalde']\n",
      "['Policía']\n",
      "['Municipal']\n",
      "['alcalde']\n",
      "['Ministra']\n",
      "['de']\n",
      "['Asuntos']\n",
      "['Exteriores']\n",
      "['parados']\n",
      "['políticos']\n",
      "['abogado']\n",
      "['diputado']\n",
      "['políticos']\n",
      "['periodistas']\n",
      "['dele', '##gada']\n",
      "['de']\n",
      "['U', '##GT']\n",
      "['sanitarios']\n",
      "['taxi', '##stas']\n",
      "['taxi', '##stas']\n",
      "['Dra', '.']\n",
      "['vicepresidente']\n",
      "['del']\n",
      "['Congreso']\n",
      "['presidentes']\n",
      "['Vicepresidente']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['ex']\n",
      "['ministro']\n",
      "[]\n",
      "['Presidenta']\n",
      "['de']\n",
      "['una']\n",
      "['Comunidad']\n",
      "['Autónoma']\n",
      "['intermediarios']\n",
      "['médicos']\n",
      "['ministros']\n",
      "['de']\n",
      "['la']\n",
      "['UE']\n",
      "['ministros']\n",
      "['compañera', '##s']\n",
      "['de']\n",
      "['enfermería']\n",
      "['ayudante']\n",
      "['Bomb', '##eros']\n",
      "['porta', '##vo', '##ces']\n",
      "['auton', '##ómico', '##s']\n",
      "['sanitarios']\n",
      "['Dra', '.']\n",
      "['Profesor', '##a']\n",
      "['de']\n",
      "['la']\n",
      "['Universidad']\n",
      "[]\n",
      "['médico']\n",
      "['coronel']\n",
      "['de']\n",
      "['la']\n",
      "['guardia']\n",
      "['civil']\n",
      "[]\n",
      "['abogados']\n",
      "['personal']\n",
      "['de']\n",
      "['residencias']\n",
      "['gestor']\n",
      "['policía']\n",
      "['municipal']\n",
      "['Médico', '##s']\n",
      "['concejal', '##es']\n",
      "['diputado']\n",
      "['ministro']\n",
      "['científico']\n",
      "['presidente']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['presidentes']\n",
      "['de']\n",
      "['comunidades']\n",
      "['minist', '##ra']\n",
      "['portavoz']\n",
      "['ministro']\n",
      "['de']\n",
      "['sanidad', '##go', '##b']\n",
      "['alum', '##n', '##x', '##s']\n",
      "['periodistas']\n",
      "['militares']\n",
      "['futbolista']\n",
      "['trabajadoras']\n",
      "['de']\n",
      "['la']\n",
      "['limpieza']\n",
      "['ministro']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['alcalde']\n",
      "['Policía']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['investigadores']\n",
      "['juez']\n",
      "['policía']\n",
      "['empresario']\n",
      "['ER', '##E']\n",
      "['trabajadores']\n",
      "['SAN', '##ITAR', '##IOS']\n",
      "['P', '##Ú', '##BL', '##ICOS']\n",
      "['tempor', '##eros']\n",
      "['sanitarios']\n",
      "['coronel']\n",
      "['de']\n",
      "['la']\n",
      "['guardia']\n",
      "['civil']\n",
      "['juez']\n",
      "['Sub', '##dele', '##gado']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['Polic', '##ia']\n",
      "['mando']\n",
      "['de']\n",
      "['la']\n",
      "['Er', '##tz', '##ain', '##tz', '##a']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['compañeros']\n",
      "['expertos']\n",
      "['que']\n",
      "['le']\n",
      "['asesor', '##an']\n",
      "['concejal']\n",
      "['trabajadores']\n",
      "['er', '##te']\n",
      "['productores']\n",
      "['presidente']\n",
      "['minist', '##r', 's']\n",
      "['enfermera']\n",
      "['Rey']\n",
      "['EMP', '##RE', '##N', '##DE', '##DO', '##RES']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['empresario']\n",
      "['autónomo']\n",
      "['trabajadores']\n",
      "['trabajadores']\n",
      "['director']\n",
      "['sanitarios']\n",
      "['profesor']\n",
      "['de']\n",
      "['química']\n",
      "['reportero']\n",
      "['sanitarios']\n",
      "[]\n",
      "['sanitarios']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['Rector']\n",
      "['mineros']\n",
      "['trabajador', '##x', '##s']\n",
      "['presidente']\n",
      "['de']\n",
      "['la']\n",
      "['J', '##unta']\n",
      "[]\n",
      "['Polic', '##ia', '##N', '##acional']\n",
      "[]\n",
      "['Polic', '##ia', '##N', '##acional']\n",
      "[]\n",
      "['Polic', '##ia', '##N', '##acional']\n",
      "['personal']\n",
      "['de']\n",
      "['sanidad']\n",
      "['ministro']\n",
      "['del']\n",
      "['Interior']\n",
      "['ER', '##TE']\n",
      "[]\n",
      "['ER', '##TE']\n",
      "['alumno']\n",
      "['Sac', '##er', '##do', '##tes']\n",
      "['alumno']\n",
      "['reporte', '##ra']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['enferme', '##ros']\n",
      "['policia']\n",
      "['guardia', '##ci', '##vil']\n",
      "['taxi', '##stas']\n",
      "['secretario']\n",
      "['general']\n",
      "['presidente']\n",
      "['minist', '##ra']\n",
      "['de']\n",
      "['Trabajo']\n",
      "['Policía']\n",
      "['Nacional']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['juez', '##a']\n",
      "['guardia', '##ci', '##vil']\n",
      "['trabajador']\n",
      "['gobernadores']\n",
      "['especialista']\n",
      "['de']\n",
      "['medicina']\n",
      "['preventiva']\n",
      "['sanitarios']\n",
      "['profesionales']\n",
      "['guardia']\n",
      "['civil']\n",
      "['guardia']\n",
      "['civil']\n",
      "['J', '##uez']\n",
      "['ocul', '##ista']\n",
      "['ca', '##jera']\n",
      "['en']\n",
      "['un']\n",
      "['supermercado']\n",
      "['profesionales']\n",
      "['necesarios']\n",
      "['en']\n",
      "['unidades']\n",
      "[]\n",
      "['CO', '##VI', '##D', '##19']\n",
      "['Reyes']\n",
      "['parados']\n",
      "['presidente']\n",
      "['presidentes']\n",
      "['Ministro']\n",
      "['patrul', '##ler', '##o']\n",
      "['compañeros']\n",
      "['presidente']\n",
      "['de']\n",
      "['la']\n",
      "['Cámara']\n",
      "['autob', '##use', '##ro']\n",
      "['ministro']\n",
      "['periodistas']\n",
      "['sanitarias']\n",
      "['vir', '##óloga']\n",
      "['inmun', '##óloga']\n",
      "['investigadora']\n",
      "['desempleo']\n",
      "['despedido']\n",
      "['juez', '##a']\n",
      "['directora']\n",
      "['de']\n",
      "['la']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['arque', '##ólogos']\n",
      "['sin']\n",
      "['trabajo']\n",
      "['Profesional', '##es']\n",
      "['de']\n",
      "['la']\n",
      "[]\n",
      "['Arque', '##ología']\n",
      "[]\n",
      "['sanitarios']\n",
      "['conse', '##ller', '##a']\n",
      "['de']\n",
      "['g', '##vas', '##ani', '##tat']\n",
      "['empleado']\n",
      "['del']\n",
      "['famoso']\n",
      "['laboratorio']\n",
      "['altos']\n",
      "['cargos']\n",
      "['médico']\n",
      "['enfermera']\n",
      "['sanitario']\n",
      "['compañeros']\n",
      "['científicos']\n",
      "['cantante']\n",
      "['vir', '##óloga']\n",
      "['catedrá', '##tica']\n",
      "['de']\n",
      "[]\n",
      "['Micro', '##bio', '##logía']\n",
      "['senador']\n",
      "['reina']\n",
      "['enferme', '##ro']\n",
      "['sanitarios']\n",
      "['ladrones']\n",
      "['investigadores']\n",
      "['agentes']\n",
      "['sanitarios']\n",
      "['policías']\n",
      "['enfermera']\n",
      "['farmacéuticos']\n",
      "['comunitarios']\n",
      "['Ministra']\n",
      "['vicepres', '##identa']\n",
      "['autónomos']\n",
      "['M', '##H', '##P']\n",
      "['director']\n",
      "['Director']\n",
      "['General']\n",
      "['de']\n",
      "['la']\n",
      "['OMS']\n",
      "['profesor']\n",
      "['de']\n",
      "['Psico', '##logía']\n",
      "['portavoz']\n",
      "['de']\n",
      "[]\n",
      "['Forma', '##cion', '##P', '##ro', '##fes', '##ional']\n",
      "['sanitarios']\n",
      "['polis']\n",
      "['Autón', '##omo']\n",
      "['autónomos']\n",
      "['Investi', '##gadores']\n",
      "['Ministro']\n",
      "['de']\n",
      "['Transporte']\n",
      "['Ministra']\n",
      "['de']\n",
      "['educación']\n",
      "['Tele', '##traba', '##jo']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['ER', '##TES']\n",
      "['dirigentes']\n",
      "['Report', '##era']\n",
      "['ex', '##dir', '##ige', '##n', '##te']\n",
      "['Presidente']\n",
      "['periodista']\n",
      "['monjas']\n",
      "['rastre', '##adores']\n",
      "['médicos']\n",
      "['enfermeras']\n",
      "['Expres', '##identes']\n",
      "['de']\n",
      "['la']\n",
      "['Generalitat']\n",
      "['enfermeras']\n",
      "['Trabajadores']\n",
      "['fijos']\n",
      "['discontin', '##uo', '##s']\n",
      "['er', '##tes']\n",
      "['trabajadores']\n",
      "['de']\n",
      "['Centros']\n",
      "['de']\n",
      "['Mayor', '##es']\n",
      "['Ges', '##tor']\n",
      "['Deportivo']\n",
      "['ministro']\n",
      "['del']\n",
      "['Interior']\n",
      "['Cap', '##ella', '##nes']\n",
      "[]\n",
      "['rastre', '##adores']\n",
      "['presidente']\n",
      "['ER', '##TE']\n",
      "['jueces']\n",
      "['presidente']\n",
      "['forense']\n",
      "['delegado']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['Leg', '##ionario', '##s']\n",
      "['Leg', '##ionario', '##s']\n",
      "['sanitarios']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['presidente']\n",
      "['productor']\n",
      "['de']\n",
      "['narco']\n",
      "['series']\n",
      "['persona']\n",
      "['del']\n",
      "['ámbito']\n",
      "['sanitario']\n",
      "['portavoz']\n",
      "['ER', '##TE']\n",
      "['GC']\n",
      "['diputado']\n",
      "['policía']\n",
      "[]\n",
      "['Estudi', '##ante']\n",
      "['poli', '##ticos']\n",
      "['ladrón']\n",
      "['ministro']\n",
      "['políticas']\n",
      "['políticos']\n",
      "['presidente']\n",
      "['jefe']\n",
      "['de']\n",
      "['gabinete']\n",
      "['alum', '##na']\n",
      "['doctora']\n",
      "['empresario']\n",
      "['empresario']\n",
      "['legi', '##onarios']\n",
      "['directivos']\n",
      "['de']\n",
      "['la']\n",
      "['industria']\n",
      "['farmacéutica']\n",
      "['representantes']\n",
      "['del']\n",
      "['arco']\n",
      "['político']\n",
      "['Director']\n",
      "['general']\n",
      "['de']\n",
      "['economía']\n",
      "['Doctor']\n",
      "['Secretario']\n",
      "['de']\n",
      "['Salud']\n",
      "['del']\n",
      "['Estado']\n",
      "['minist', '##ra']\n",
      "['de']\n",
      "['educación']\n",
      "['trabajadores']\n",
      "['residentes']\n",
      "['residentes']\n",
      "['médicos']\n",
      "['abogado']\n",
      "['escolta', '##s']\n",
      "['personal']\n",
      "['escolta', '##s']\n",
      "['presidente']\n",
      "['policia']\n",
      "['secreta']\n",
      "['alcalde']\n",
      "['alcalde']\n",
      "['alcalde', '##ee']\n",
      "['policía']\n",
      "['Far', '##mac', '##é', '##uti', '##cos']\n",
      "['funcionar', '##ia']\n",
      "['de']\n",
      "['prisiones']\n",
      "['Go', '##b', '.']\n",
      "['alcalde']\n",
      "['presidente']\n",
      "['Médico', '##s']\n",
      "['residentes']\n",
      "[]\n",
      "['profesionales']\n",
      "['periodistas']\n",
      "['Dr']\n",
      "['Dr']\n",
      "['Diputados']\n",
      "['parlamentario']\n",
      "['investigador']\n",
      "['presidente']\n",
      "['de']\n",
      "['la']\n",
      "['Comunidad']\n",
      "['presidente']\n",
      "['policia']\n",
      "['guardia', '##ci', '##vil']\n",
      "['en']\n",
      "['paro']\n",
      "['trazado', '##res']\n",
      "['de']\n",
      "['contagi', '##os']\n",
      "['médicos']\n",
      "['cuidado', '##ras']\n",
      "['profesionales']\n",
      "['san', '##itar', '##i', 's']\n",
      "['er', '##te']\n",
      "['er', '##te']\n",
      "['fundador']\n",
      "['trabajadores']\n",
      "['autónomos']\n",
      "['vicepres', '##identa']\n",
      "['minist', '##ra']\n",
      "['jefa']\n",
      "['del']\n",
      "['Servicio']\n",
      "['Federal']\n",
      "['sanitarios']\n",
      "['autoridades']\n",
      "['jefa']\n",
      "['médica']\n",
      "['gobernantes']\n",
      "['crítico']\n",
      "['profesionales']\n",
      "['del']\n",
      "['Hospital']\n",
      "['líderes']\n",
      "['de']\n",
      "['la']\n",
      "['UE']\n",
      "['letra', '##do']\n",
      "['de']\n",
      "['las']\n",
      "['Cortes']\n",
      "['sanitarios']\n",
      "['Presidente']\n",
      "['neuro', '##cien', '##tí', '##ficos']\n",
      "['cogni', '##tivos']\n",
      "['econom', '##istas']\n",
      "['conduc', '##tuales']\n",
      "['mante', '##ros']\n",
      "['doctor']\n",
      "[]\n",
      "['transport', '##istas']\n",
      "['investigadores']\n",
      "['medi', '##cos']\n",
      "['enfermera']\n",
      "['del']\n",
      "['hospital']\n",
      "['EMP', '##LE', '##ADO']\n",
      "['investigadores']\n",
      "['sanitarios']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['entre', '##pren', '##e', '##urs']\n",
      "['sanitarios']\n",
      "['profesionales']\n",
      "['presidenta']\n",
      "['ministros']\n",
      "['Ministra']\n",
      "['presidente']\n",
      "['Abogados']\n",
      "['presidente']\n",
      "['manda', '##tario']\n",
      "[]\n",
      "['autónomos']\n",
      "['ra', '##bino']\n",
      "['policía']\n",
      "['arzobispo']\n",
      "['alcalde']\n",
      "['personal']\n",
      "['de']\n",
      "['salud']\n",
      "['médicos']\n",
      "['médicos']\n",
      "['político']\n",
      "['doctor']\n",
      "['Científico', '##s']\n",
      "['investigador', 's']\n",
      "['jefe']\n",
      "['del']\n",
      "['Estado']\n",
      "['Mayor']\n",
      "['profesionales']\n",
      "['rastre', '##ador']\n",
      "['estudiantes']\n",
      "['ministro']\n",
      "['de']\n",
      "['Interior']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['taxi', '##stas']\n",
      "['rap', '##eros']\n",
      "['personal']\n",
      "['médico']\n",
      "['trabajadores']\n",
      "['de']\n",
      "['supermercado', '##s']\n",
      "['Dr', '.']\n",
      "['jefe']\n",
      "['de']\n",
      "['En', '##f']\n",
      "['.']\n",
      "['Inf', '##ec', '##ciosas']\n",
      "['reportero', '##s']\n",
      "['sanitarios']\n",
      "['Enferme', '##ros']\n",
      "['sanitarios']\n",
      "['rid', '##ers']\n",
      "['ca', '##jera']\n",
      "['del']\n",
      "['súper']\n",
      "['portero']\n",
      "['tele', '##traba', '##jo']\n",
      "['policía']\n",
      "['municipal']\n",
      "['trabajadores']\n",
      "['Min']\n",
      "['transport', '##istas']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['presidente']\n",
      "['Presidente']\n",
      "['secretario']\n",
      "['de']\n",
      "['Estado']\n",
      "['guardia', '##ci', '##vil']\n",
      "['presidenta']\n",
      "['empresarios']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['alumnos']\n",
      "['médico']\n",
      "['guardia']\n",
      "['civil']\n",
      "['san', '##itar', '##i', 's']\n",
      "['ladrones']\n",
      "['asesores']\n",
      "['pro', '##x', '##ene', '##tas']\n",
      "['empresarios']\n",
      "['director']\n",
      "['jugador', '##a']\n",
      "['presidente']\n",
      "['compañera', '##s']\n",
      "['de']\n",
      "['equipo']\n",
      "['cuerpo']\n",
      "['técnico']\n",
      "['técnicos']\n",
      "['de']\n",
      "[]\n",
      "['protocolo']\n",
      "['gestor', '##es']\n",
      "['de']\n",
      "['af', '##oro', '##s']\n",
      "['consejera']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['periodista']\n",
      "['Obispo', '##s']\n",
      "['presidente']\n",
      "['Ministro']\n",
      "['responsables']\n",
      "['de']\n",
      "['las']\n",
      "['Fuerzas']\n",
      "['Armadas']\n",
      "['Policía']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['científica']\n",
      "['trabajadores']\n",
      "['sanitarios']\n",
      "['ter', '##tul', '##ianos']\n",
      "['presidente']\n",
      "['presidente']\n",
      "['Alcalde', '##sa']\n",
      "['futbolista', '##s']\n",
      "['Dra', '.']\n",
      "[]\n",
      "['intern', '##ista']\n",
      "['policia']\n",
      "['guardia', '##ci', '##vil']\n",
      "['trabajador']\n",
      "['tele', '##traba', '##jo']\n",
      "['científicos']\n",
      "['empleados']\n",
      "[]\n",
      "['em', '##éri', '##to']\n",
      "['delantero']\n",
      "['centro']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['abogados']\n",
      "['trabajadores']\n",
      "['ER', '##TES']\n",
      "['Directora']\n",
      "['guardias']\n",
      "['civiles']\n",
      "['médicos']\n",
      "['##D']\n",
      "['auton', '##omos']\n",
      "['sanitarios']\n",
      "['micro', '##bio', '##log', '##a']\n",
      "['médico']\n",
      "['médico']\n",
      "['de']\n",
      "['familia']\n",
      "[]\n",
      "['ru', '##n', '##ners']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['alcalde']\n",
      "['director']\n",
      "['del']\n",
      "['Centro']\n",
      "['de']\n",
      "['Coordinación']\n",
      "['director']\n",
      "['de']\n",
      "['cine']\n",
      "['jugadores']\n",
      "['sanitarios']\n",
      "['vir', '##ólogo']\n",
      "['secretario']\n",
      "[]\n",
      "['políticos']\n",
      "['Im', '##án']\n",
      "['alcalde']\n",
      "['inversores']\n",
      "['entrenador']\n",
      "['políticos']\n",
      "['artistas']\n",
      "['músicos']\n",
      "['parlamentarios']\n",
      "['autónomos']\n",
      "['asalari', '##ada']\n",
      "['Presidente']\n",
      "['del']\n",
      "[]\n",
      "['Gobierno']\n",
      "['Ministro']\n",
      "['doctor']\n",
      "['técnicos']\n",
      "['de']\n",
      "['la']\n",
      "['sanidad']\n",
      "['policía']\n",
      "['J', '##ue', '##ces']\n",
      "['Policía', '##s']\n",
      "['presidente']\n",
      "['portavoz']\n",
      "['auxiliares']\n",
      "['de']\n",
      "['enfermería']\n",
      "['enfermeras']\n",
      "['facultativo', '##s']\n",
      "['periodista']\n",
      "['vir', '##óloga']\n",
      "['médico']\n",
      "['compañeros']\n",
      "['de']\n",
      "['trabajo']\n",
      "['ama']\n",
      "['de']\n",
      "['llaves']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['científicos']\n",
      "['epidemi', '##olo', '##go']\n",
      "['Agentes']\n",
      "['investigador']\n",
      "['policia']\n",
      "['agentes']\n",
      "['médico']\n",
      "['médicos']\n",
      "['médico']\n",
      "['sanitarios']\n",
      "['medi', '##ca']\n",
      "['anuncia', '##n', '##tes']\n",
      "['juez']\n",
      "['ER', '##TES']\n",
      "['político']\n",
      "['médico']\n",
      "['reyes']\n",
      "['ministro']\n",
      "['del']\n",
      "['interior']\n",
      "['policías']\n",
      "['políticos']\n",
      "['políticos']\n",
      "['jugadores']\n",
      "['rey']\n",
      "['rey']\n",
      "['diputado']\n",
      "['empleados']\n",
      "['ministro']\n",
      "['tempor', '##eros']\n",
      "['conductor']\n",
      "['de']\n",
      "['autobús']\n",
      "['rastre', '##adores']\n",
      "['rastre', '##adores']\n",
      "['vicepresidente']\n",
      "['directora']\n",
      "['de']\n",
      "['salud']\n",
      "['pública']\n",
      "['enfermeras']\n",
      "['presidente']\n",
      "['del']\n",
      "['gobierno']\n",
      "['médico']\n",
      "['portavoz']\n",
      "['de']\n",
      "['sanidad']\n",
      "['rey']\n",
      "['profesores']\n",
      "['políticos']\n",
      "['consejera']\n",
      "['presidente']\n",
      "['de']\n",
      "['la']\n",
      "['CC', '##A', '##A']\n",
      "['Le', '##hen', '##da', '##k', '##ari']\n",
      "['número']\n",
      "['2']\n",
      "['en']\n",
      "['Interior']\n",
      "['directora']\n",
      "['general']\n",
      "['de']\n",
      "['la']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['primer']\n",
      "['ministro']\n",
      "['bas', '##urero']\n",
      "['policia']\n",
      "['guardia', '##ci', '##vil']\n",
      "['policía']\n",
      "['guardia']\n",
      "['civil']\n",
      "['presidenta']\n",
      "['de']\n",
      "['la']\n",
      "['CAM']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "[]\n",
      "['sanitarios']\n",
      "['médicos']\n",
      "['VER', '##D', '##UG', '##O']\n",
      "['autor']\n",
      "[]\n",
      "['ver', '##du', '##go']\n",
      "['ministro']\n",
      "['de']\n",
      "['sanidad', '##go', '##b']\n",
      "['director']\n",
      "['del']\n",
      "['Centro']\n",
      "['de']\n",
      "['Coordinación']\n",
      "['investigadores']\n",
      "['Médico', '##s']\n",
      "['de']\n",
      "['Cabe', '##cera']\n",
      "['Mona', '##r', '##ca']\n",
      "['abogado']\n",
      "['ministro']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['estudiantes']\n",
      "['Políticos']\n",
      "['intérpretes']\n",
      "['conse', '##ller', '##a']\n",
      "['ministro']\n",
      "['trabajadoras']\n",
      "['del']\n",
      "['hogar']\n",
      "['Presidente']\n",
      "['militares']\n",
      "['ex', '##di', '##pu', '##tada']\n",
      "['polí', '##tic', '##x', '##s']\n",
      "['trabajador', '##x', '##s']\n",
      "['policías']\n",
      "['Abogados']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['compañeros']\n",
      "['limpia', '##dora']\n",
      "['Au', '##x']\n",
      "['.']\n",
      "['De']\n",
      "['enfermería']\n",
      "['ER', '##TE']\n",
      "['trabajadores']\n",
      "['gestor']\n",
      "['empleados']\n",
      "['POL', '##Í', '##TI', '##CO', '##S']\n",
      "['médico']\n",
      "['presidente']\n",
      "['empresarios']\n",
      "['fiscales']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['trabajadoras']\n",
      "['de']\n",
      "['la']\n",
      "['limpieza']\n",
      "['líderes']\n",
      "['sociales']\n",
      "['ministro']\n",
      "['de']\n",
      "['Cultura']\n",
      "[',']\n",
      "['Educación']\n",
      "['Secretario']\n",
      "['General']\n",
      "['de']\n",
      "['la']\n",
      "['OTAN']\n",
      "['Alto']\n",
      "['Representante']\n",
      "['de']\n",
      "['UE']\n",
      "['alcalde', '##s']\n",
      "['dictador']\n",
      "['tor', '##ero']\n",
      "['científica']\n",
      "['de']\n",
      "['aero', '##sol', '##es']\n",
      "['Presidente']\n",
      "['médico']\n",
      "['diputado']\n",
      "['médicos']\n",
      "['médicos']\n",
      "['Reyes']\n",
      "['Papa', '##s']\n",
      "['Reina']\n",
      "['consejera', '##s']\n",
      "['abogado']\n",
      "['médicos']\n",
      "['tempor', '##eras']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['profesionales']\n",
      "[]\n",
      "['Primer']\n",
      "['Ministro']\n",
      "['Sec', '##r']\n",
      "['.']\n",
      "['de']\n",
      "['Ex', '##t']\n",
      "['.']\n",
      "['canciller']\n",
      "['Fis', '##io', '##tera', '##peuta']\n",
      "['en']\n",
      "['Ger', '##iat', '##ría']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['médico']\n",
      "['de']\n",
      "['Familia']\n",
      "['personal']\n",
      "['San', '##itario']\n",
      "['reportero']\n",
      "['entrenador', '##es']\n",
      "['Alcalde']\n",
      "['expres', '##identes']\n",
      "['ex', '##cons', '##e', '##jeros']\n",
      "['de']\n",
      "['la']\n",
      "['Generalitat']\n",
      "['desempleados']\n",
      "['policia']\n",
      "['guardias']\n",
      "['civiles']\n",
      "['policía']\n",
      "['Nacional']\n",
      "['portavoz']\n",
      "['del']\n",
      "['Go', '##vern']\n",
      "['Ministros']\n",
      "['militares']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['trabajadores']\n",
      "['de']\n",
      "['salud']\n",
      "[]\n",
      "['reportero']\n",
      "['cardenal']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monjas']\n",
      "['de']\n",
      "['clausura']\n",
      "['rastre', '##adores']\n",
      "['profesionales']\n",
      "['colaborador', '##a']\n",
      "['policía']\n",
      "['minist', '##ras']\n",
      "['policia']\n",
      "['doctora']\n",
      "['médicos']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['trabajadores']\n",
      "['ficti', '##cios']\n",
      "['ER', '##TE', '##s']\n",
      "['consejera']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['sanitarios']\n",
      "['Capitán']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['pere', '##odi', '##sta']\n",
      "['Ex']\n",
      "['director']\n",
      "['ex', '-', 'asalariados']\n",
      "['rastre', '##adores']\n",
      "['estudiantes']\n",
      "['ministro']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['médicos']\n",
      "['agentes']\n",
      "['personal']\n",
      "['de']\n",
      "['rastre', '##o']\n",
      "['sanitarios']\n",
      "['juez']\n",
      "['consejero']\n",
      "['Presidente']\n",
      "['sanitarios']\n",
      "['Diri', '##gentes']\n",
      "['médico']\n",
      "['Neu', '##ró', '##log', '##os']\n",
      "['expertos']\n",
      "['en']\n",
      "['ce', '##fal', '##eas']\n",
      "['Reyes']\n",
      "['gobernantes']\n",
      "['directora']\n",
      "['M', '##É', '##DI', '##CA']\n",
      "['DE']\n",
      "['AT', '##ENCI', '##Ó', '##N']\n",
      "['PRIM', '##ARIA']\n",
      "['J', '##ugador', '##es']\n",
      "['sanitarios']\n",
      "['policía']\n",
      "['porta', '##vo', '##ces']\n",
      "['Alcalde', '##sa']\n",
      "['fotógrafo']\n",
      "['presidente']\n",
      "['del']\n",
      "['gobierno']\n",
      "['Dr']\n",
      "['inmun', '##ólogo']\n",
      "['contable']\n",
      "['J', '##efe']\n",
      "['de']\n",
      "['la']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['juez']\n",
      "['líder']\n",
      "['abogados']\n",
      "['Vice', '##p', '##dt', '##a']\n",
      "['.']\n",
      "['policía']\n",
      "['trabajadores']\n",
      "['clo', '##w', '##n']\n",
      "['minist', '##ra']\n",
      "['pod', '##emi', '##ta']\n",
      "['de']\n",
      "['Transición']\n",
      "['Eco', '##lógica']\n",
      "['desempleados']\n",
      "['Er', '##tes']\n",
      "['trabajadores']\n",
      "['alumnado']\n",
      "['agricultores']\n",
      "['gana', '##deros']\n",
      "['Mon', '##s']\n",
      "['.']\n",
      "['políticos']\n",
      "['Ministros']\n",
      "['alcalde', '##s']\n",
      "['científica']\n",
      "['inmun', '##óloga', '##s']\n",
      "['Ven', '##dedores']\n",
      "['Am', '##bula', '##n', '##tes']\n",
      "['narco']\n",
      "['guardias']\n",
      "['civiles']\n",
      "['Commun', '##ity']\n",
      "['Mana', '##ger']\n",
      "['San', '##itarios']\n",
      "['técnico']\n",
      "['policía']\n",
      "['San', '##itarios']\n",
      "['primera']\n",
      "['minist', '##ra']\n",
      "['Médico', '##s']\n",
      "['médicos']\n",
      "['responsables']\n",
      "['políticos']\n",
      "['sanitarios']\n",
      "['cad', '##etes']\n",
      "['sargento', '##s']\n",
      "['comerciantes']\n",
      "['epidemi', '##ólogos']\n",
      "['del']\n",
      "['comité']\n",
      "['de']\n",
      "['expertos']\n",
      "['consejero']\n",
      "['ER', '##TE']\n",
      "['militares']\n",
      "['Militar', '##es']\n",
      "['dipu', '##tada']\n",
      "['ingenieros']\n",
      "['informáticos']\n",
      "['futbolista', '##s']\n",
      "['experto']\n",
      "['español']\n",
      "['en']\n",
      "['corona', '##vir', '##us']\n",
      "['escritor']\n",
      "['periodista']\n",
      "['Rey']\n",
      "[]\n",
      "['tele', '##traba', '##jo']\n",
      "['auxiliar']\n",
      "['de']\n",
      "['enfermería']\n",
      "['rector']\n",
      "[]\n",
      "['soldados']\n",
      "['dueño']\n",
      "['vicepresidente']\n",
      "['segundo']\n",
      "['médico']\n",
      "['auxiliar']\n",
      "[]\n",
      "['sanitarios']\n",
      "['policías']\n",
      "['ministro']\n",
      "['del']\n",
      "['Interior']\n",
      "['agentes']\n",
      "['agricultores']\n",
      "['gana', '##deros']\n",
      "['médico']\n",
      "['Director']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['micro', '##bió', '##log', '##os']\n",
      "['enfermera']\n",
      "['enfermera']\n",
      "['enfermera']\n",
      "['entrenador', '##es']\n",
      "['concejal']\n",
      "['del']\n",
      "['Ayuntamiento']\n",
      "['internos']\n",
      "['rey']\n",
      "['trabajadores']\n",
      "['pseudo', '##per', '##io', '##dista']\n",
      "['minist', '##ra']\n",
      "['fijos']\n",
      "['preca', '##rios']\n",
      "['despedido', '##s']\n",
      "['sanitarios']\n",
      "['Policía']\n",
      "['juez']\n",
      "['director']\n",
      "['autor']\n",
      "['especialistas']\n",
      "['chinos']\n",
      "['de']\n",
      "['la']\n",
      "['salud']\n",
      "['presidente']\n",
      "['médico']\n",
      "['directivo']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['alcalde', '##sa']\n",
      "['especialistas']\n",
      "['científicos']\n",
      "['político']\n",
      "['epidemi', '##ólogo']\n",
      "['jefe']\n",
      "['del']\n",
      "['Servicio']\n",
      "['de']\n",
      "['Dig', '##estiv', '##o']\n",
      "['arque', '##ro']\n",
      "['militares']\n",
      "['efectivos']\n",
      "['de']\n",
      "['asistencia']\n",
      "['sanitaria']\n",
      "['Compañ', '##ero']\n",
      "['guardia', '##ci', '##vil']\n",
      "['trabajadores']\n",
      "['san', '##itar', '##i', 's']\n",
      "['g', '.']\n",
      "['civiles']\n",
      "['agentes']\n",
      "['escolta']\n",
      "['presidente']\n",
      "['periodistas']\n",
      "['periodista']\n",
      "['fotógrafo']\n",
      "['empren', '##dedor']\n",
      "['docentes']\n",
      "['sanitarios']\n",
      "['guarda']\n",
      "['de']\n",
      "['metro']\n",
      "['mos', '##sos']\n",
      "['vir', '##óloga']\n",
      "['líderes']\n",
      "['sociales']\n",
      "['San', '##itarios']\n",
      "['Compañ', '##eros']\n",
      "['trabajadoras']\n",
      "['policía']\n",
      "['empleadas']\n",
      "['empleados']\n",
      "['públicos']\n",
      "['profesionales']\n",
      "['sanitarios']\n",
      "['Presidente']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['sanitarios']\n",
      "['Er', '##tes']\n",
      "['presidente']\n",
      "['cura']\n",
      "['you', '##tu', '##ber']\n",
      "['secretario']\n",
      "['general']\n",
      "['trabajadores']\n",
      "['San', '##itarios']\n",
      "['trabajadores']\n",
      "['concejal']\n",
      "['sanitarios']\n",
      "['poli', '##tó', '##log', '##o']\n",
      "['vicepresidente']\n",
      "['jugadores']\n",
      "['científicos']\n",
      "['trabajadores']\n",
      "['trabajadores']\n",
      "['bi', '##ólogo']\n",
      "['secretario']\n",
      "['general']\n",
      "['de']\n",
      "['la']\n",
      "['ONU']\n",
      "['trabajador']\n",
      "['del']\n",
      "['gobierno']\n",
      "['Presidenta']\n",
      "['de']\n",
      "['la']\n",
      "['Comunidad']\n",
      "['trabajadoras']\n",
      "['trabajadores']\n",
      "['médico']\n",
      "['de']\n",
      "['cabecera']\n",
      "['trabajadoras']\n",
      "['universitarios']\n",
      "['P', '##te']\n",
      "['presidente']\n",
      "['ER', '##TE']\n",
      "['autónomos']\n",
      "['ministro']\n",
      "['de']\n",
      "[]\n",
      "['J', '##usticia']\n",
      "['F', '.']\n",
      "['de']\n",
      "['Seguridad']\n",
      "['árbitro', '##s']\n",
      "['ER', '##TE']\n",
      "['tele', '##traba', '##jo']\n",
      "['##ide', '##mi', '##ólogo']\n",
      "['minist', '##ra']\n",
      "['avi', '##adores']\n",
      "['TC', '##P', '##s']\n",
      "['PI', '##lot', '##os']\n",
      "[]\n",
      "['t', '##c']\n",
      "['##p']\n",
      "['tripul', '##ante', '##dec', '##abi', '##na']\n",
      "[]\n",
      "['auxiliar', '##de', '##v', '##uelo']\n",
      "['##a']\n",
      "['##a', 'az', '##af', '##ata']\n",
      "['Dr', '.']\n",
      "['presidente']\n",
      "['trabajadores']\n",
      "[]\n",
      "['de']\n",
      "[]\n",
      "['los']\n",
      "[]\n",
      "['servicios']\n",
      "['esenciales']\n",
      "['Guardia', '##s']\n",
      "['Civiles']\n",
      "['presidente']\n",
      "['Guardia']\n",
      "['GU', '##ARD', '##IA']\n",
      "['CI', '##VI', '##L']\n",
      "['RE', '##Y']\n",
      "['guardia', '##ci', '##vil']\n",
      "[]\n",
      "['medi', '##cos']\n",
      "[]\n",
      "['medi', '##cas']\n",
      "['transport', '##istas']\n",
      "['sanitarios']\n",
      "['conse', '##ller', '##a']\n",
      "['alumnado']\n",
      "['investigadora']\n",
      "['subsec', '##re', '##tario']\n",
      "['de']\n",
      "['Prevención']\n",
      "['y']\n",
      "['Promoción']\n",
      "['científico']\n",
      "['catedrá', '##tico']\n",
      "['de']\n",
      "['Bio', '##química']\n",
      "['y']\n",
      "['Bio', '##logía']\n",
      "['guardia', '##ci', '##vil']\n",
      "['enfermera']\n",
      "['Rey']\n",
      "['ex']\n",
      "['ministro']\n",
      "['primer']\n",
      "['Ministro']\n",
      "['taxi', '##sta']\n",
      "['profesionales']\n",
      "['de']\n",
      "['la']\n",
      "['seguridad']\n",
      "['profesionales']\n",
      "['de']\n",
      "['la']\n",
      "['seguridad']\n",
      "['pública']\n",
      "['profe']\n",
      "['médicos']\n",
      "['investigador']\n",
      "['director']\n",
      "['general']\n",
      "['profesores']\n",
      "['estudiantes']\n",
      "['expe', '##n', '##dedor']\n",
      "['de']\n",
      "['alimentos']\n",
      "['en']\n",
      "['plazas']\n",
      "['minist', '##ra']\n",
      "['Gobernador']\n",
      "['Reyes']\n",
      "['directores']\n",
      "['del']\n",
      "['Hospital']\n",
      "['consejera']\n",
      "['socialista']\n",
      "['de']\n",
      "['Turismo']\n",
      "[',']\n",
      "['Comercio']\n",
      "['Policía']\n",
      "['presidente']\n",
      "['políticos']\n",
      "['médicos']\n",
      "['enfermeras']\n",
      "['pul', '##is', '##ia']\n",
      "['Médico']\n",
      "['residente']\n",
      "['sanitarios']\n",
      "['inversores']\n",
      "['Reyes']\n",
      "['Reyes']\n",
      "['responsable']\n",
      "['de']\n",
      "['una']\n",
      "['empresa']\n",
      "['Médico', '##s']\n",
      "['vir', '##ólogo']\n",
      "['Papa']\n",
      "['desempleados']\n",
      "['policia']\n",
      "['Ministro']\n",
      "['medi', '##co']\n",
      "['de']\n",
      "['UCI']\n",
      "['médicos']\n",
      "['investigadores']\n",
      "['científicos']\n",
      "['académicos']\n",
      "['Ministro']\n",
      "['de']\n",
      "['Sanidad']\n",
      "['mayordomo']\n",
      "['presidentes']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['políticos']\n",
      "['empleados']\n",
      "['sanitarios']\n",
      "['portavoz']\n",
      "['director']\n",
      "['entrenador']\n",
      "['políticos']\n",
      "['presidentes']\n",
      "['de']\n",
      "['multinacionales']\n",
      "['entrenador']\n",
      "['médicos']\n",
      "['empleados']\n",
      "['públicos']\n",
      "['personal']\n",
      "['Científico', '##s']\n",
      "['guardia', '##ci', '##vil']\n",
      "['científicos']\n",
      "['presidente']\n",
      "['Médico']\n",
      "['de']\n",
      "['familia']\n",
      "['alcalde']\n",
      "['trabajadores']\n",
      "[]\n",
      "['autónomos']\n",
      "['vir', '##óloga']\n",
      "['organizadores']\n",
      "['policía']\n",
      "[]\n",
      "['tele', '##traba', '##jo']\n",
      "['responsable']\n",
      "['de']\n",
      "['gestión']\n",
      "['de']\n",
      "['personas']\n",
      "['papas']\n",
      "['presidentes']\n",
      "['Rey']\n",
      "['militares']\n",
      "['rector']\n",
      "['miembro']\n",
      "['del']\n",
      "['Comité']\n",
      "['Olímpico']\n",
      "['Español']\n",
      "['rastre', '##adores']\n",
      "['despedido']\n",
      "['primer']\n",
      "['ministro']\n",
      "['ministros']\n",
      "['presidente']\n",
      "['guardia', '##ci', '##vil']\n",
      "['Dr', '.']\n",
      "['director']\n",
      "['presidentes']\n",
      "['auton', '##ómico', '##s']\n",
      "['alcalde', '##s']\n",
      "['sanitarios']\n",
      "['profesores']\n",
      "['de']\n",
      "['Medicina']\n",
      "['deportistas']\n",
      "['de']\n",
      "['alto']\n",
      "['rendimiento']\n",
      "['Abogados']\n",
      "['alumnado']\n",
      "['parados']\n",
      "['alcalde']\n",
      "['personal']\n",
      "['de']\n",
      "['salud']\n",
      "['consejera']\n",
      "['de']\n",
      "['Salud']\n",
      "['juez']\n",
      "['sanitarios']\n",
      "['trabajadores']\n",
      "['Dr', '.']\n",
      "['alumnos']\n",
      "['sanitarios']\n",
      "['enfermeras']\n",
      "['enfermeras']\n",
      "['matr', '##onas']\n",
      "['ER', '##TES']\n",
      "[]\n",
      "['artesanos']\n",
      "['trabajador']\n",
      "['San', '##itarios']\n",
      "['rey']\n",
      "['sanitarios']\n",
      "['sanitarios']\n",
      "['agentes']\n",
      "['policías']\n",
      "['econom', '##istas']\n",
      "['tele', '##traba', '##jo']\n",
      "['funcionarios']\n",
      "['públicos']\n",
      "['autor']\n",
      "['personal']\n",
      "['de']\n",
      "['seguridad']\n",
      "['Ministro']\n",
      "['dipu', '##tada']\n",
      "['conse', '##ller']\n",
      "['de']\n",
      "['Hacienda']\n",
      "['médicos']\n",
      "['jubi', '##lados']\n",
      "['gradu', '##ados']\n",
      "['estudiantes']\n",
      "['marinos']\n",
      "['trabajador', '##x', '##s']\n",
      "['esenciales']\n",
      "['investigadores']\n",
      "['Funciona', '##rios']\n",
      "['trabajadores']\n",
      "['médico']\n",
      "['Policía']\n",
      "['Local']\n",
      "['Guardia']\n",
      "['Civil']\n",
      "['futbolista', '##s']\n",
      "['M', '##É', '##DI', '##CO', '##S']\n",
      "['San', '##itarios']\n",
      "['consejero']\n",
      "['Subs', '##ecre', '##taria']\n",
      "['de']\n",
      "['la']\n",
      "['ONU']\n",
      "['pedi', '##at', '##ra']\n",
      "['Guardia']\n",
      "['Civil']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sanitarios']\n",
      "['Policía']\n",
      "['sanitarios']\n",
      "['dipu', '##tadas']\n",
      "['presidente']\n",
      "['políticos']\n",
      "['Comisario']\n",
      "['J', '##efe']\n",
      "['Opera', '##tivo']\n",
      "['de']\n",
      "['la']\n",
      "['Comis', '##aría']\n",
      "['policia']\n",
      "['Comisario']\n",
      "['Inspector']\n",
      "['J', '##efe']\n",
      "['policías']\n",
      "['fijos']\n",
      "['preca', '##rios']\n",
      "['despedido', '##s']\n",
      "['presidente']\n",
      "['sanitarios']\n",
      "['putas']\n",
      "['líderes']\n",
      "['europeos']\n",
      "['Agentes']\n",
      "['sociales']\n",
      "['alcalde', '##s']\n",
      "['porta', '##vo', '##ces']\n",
      "['vicepres', '##identa']\n",
      "['investigadores']\n",
      "['investigadores']\n",
      "['vir', '##óloga']\n",
      "['personal']\n",
      "['del']\n",
      "['avión']\n",
      "['Médico', '##s']\n",
      "['periodistas']\n",
      "['Policía']\n",
      "['Nacional']\n",
      "['Policía']\n",
      "['Nacional']\n",
      "['comisario', '##s']\n",
      "['políticos']\n",
      "['porta', '##vo', '##ces']\n",
      "['de']\n",
      "['la']\n",
      "['Asamblea']\n",
      "['dem', '##óg', '##ra', '##fa']\n",
      "['empleados']\n",
      "['sanitarios']\n",
      "['Rey']\n",
      "['ministro']\n",
      "['de']\n",
      "['Agricultura']\n",
      "[',']\n",
      "['Pesca']\n",
      "['investigador']\n",
      "['gene', '##tista']\n",
      "['desempleo']\n",
      "['sanitaria']\n",
      "['reportero']\n",
      "['Presidente']\n",
      "['de']\n",
      "['la']\n",
      "['República']\n",
      "['experta']\n",
      "['policia']\n",
      "['hac', '##k', '##ers']\n",
      "['presidente']\n",
      "['policía']\n",
      "['trabajadores']\n",
      "['sanitarios']\n",
      "['estudiantes']\n",
      "['trabajadoras']\n",
      "['gal', '##enos']\n",
      "['policías']\n",
      "['parados']\n",
      "['obreros']\n",
      "['policía']\n",
      "['Guardia', '##C', '##iv', '##il']\n",
      "['parados']\n",
      "['diputado']\n",
      "['presidente']\n",
      "['monitor']\n",
      "['sacerdotes']\n",
      "['Médico', '##s']\n",
      "['profesionales']\n",
      "['ch', '##ó', '##fer']\n",
      "['de']\n",
      "['bus']\n",
      "['médica']\n",
      "['preso']\n",
      "['vigilancia']\n",
      "['cent', '##ine', '##la']\n",
      "['Presidente']\n",
      "['militar']\n",
      "['agentes']\n",
      "['armados']\n",
      "['cura', '##s']\n",
      "['rastre', '##adores']\n",
      "['profesores']\n",
      "['cura', '##s']\n",
      "['Guardia', '##s']\n",
      "['.']\n",
      "['Civiles']\n",
      "['Policía', '##s']\n",
      "['Nacionales']\n",
      "['sanitarios']\n",
      "['primer']\n",
      "['ministro']\n",
      "['parados']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['autónomos']\n",
      "['segura', '##tas']\n",
      "['presidenta']\n",
      "['de']\n",
      "['la']\n",
      "['CAM']\n",
      "['presidentes']\n",
      "['auton', '##ómico', '##s']\n",
      "['pens', '##ion', '##istas']\n",
      "['sanitarios']\n",
      "['autor']\n",
      "['Presidenta']\n",
      "['guardia']\n",
      "['civil']\n",
      "['directores']\n",
      "['de']\n",
      "['la']\n",
      "['OMS']\n",
      "['sanitarios']\n",
      "['altos']\n",
      "['representantes']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['Profesional', '##es']\n",
      "['de']\n",
      "['los']\n",
      "['servicios']\n",
      "['funer', '##arios']\n",
      "['vir', '##ólogos']\n",
      "['asesores']\n",
      "['expertos']\n",
      "['Cir', '##uj', '##anos']\n",
      "['compañeros']\n",
      "['investigadores']\n",
      "['Comisionada']\n",
      "['de']\n",
      "['la']\n",
      "['Generalitat']\n",
      "['empresarios']\n",
      "['científica']\n",
      "['epidemi', '##ólogo']\n",
      "['secretario']\n",
      "['general']\n",
      "['rastre', '##adores']\n",
      "['pens', '##ion', '##istas']\n",
      "['Perio', '##dista']\n",
      "['ministro']\n",
      "['ministro']\n",
      "['diputados']\n",
      "['asesor']\n",
      "['sanitarios']\n",
      "['policías']\n",
      "['Sep', '##ult', '##urero']\n",
      "['investigadora']\n",
      "['Relator']\n",
      "['ONU']\n",
      "['vir', '##olo', '##gos']\n",
      "['portavoz']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['sanitarios']\n",
      "['inmun', '##ólogo']\n",
      "['guardias']\n",
      "['civiles']\n",
      "['actor']\n",
      "['compañeros']\n",
      "['alcalde']\n",
      "['chef']\n",
      "['ministro']\n",
      "['de']\n",
      "['Consumo']\n",
      "['ministro']\n",
      "['ministro']\n",
      "['M', '##H', '##P']\n",
      "['trabajadores']\n",
      "['Científico', '##s']\n",
      "['maestro']\n",
      "['rei', '##k', '##i']\n",
      "['Presidente']\n",
      "['especialista']\n",
      "['of', '##tal', '##mó', '##log', '##o']\n",
      "['jugadores']\n",
      "['artistas']\n",
      "['autónomos']\n",
      "['consejero']\n",
      "['Presidente']\n",
      "['TRABA', '##J', '##ADOR', '##ES']\n",
      "[]\n",
      "[]\n",
      "['tele', '##traba', '##jo']\n",
      "['investigadores']\n",
      "['policías']\n",
      "['médico']\n",
      "['diputado']\n",
      "['sanitarios']\n",
      "['profesionales']\n",
      "['Policía']\n",
      "['policia']\n",
      "['jefe']\n",
      "['de']\n",
      "['voluntaria', '##do']\n",
      "['ex']\n",
      "['arzobispo']\n",
      "['Presidente']\n",
      "['ER', '##TE']\n",
      "['políticos']\n",
      "['políticos']\n",
      "['AL', '##CA', '##L', '##DE']\n",
      "['notici', '##ero']\n",
      "['co', '##fra', '##de']\n",
      "['médicos']\n",
      "['enferme', '##ros']\n",
      "['policía']\n",
      "['guardia']\n",
      "['civil']\n",
      "['limpia', '##dores']\n",
      "['Bomb', '##eros']\n",
      "['médico']\n",
      "['juez']\n",
      "['ten', '##deros']\n",
      "['del']\n",
      "['mercado']\n",
      "['enfermera']\n",
      "['asesores']\n",
      "['diputado']\n",
      "['espías']\n",
      "['ministro']\n",
      "['de']\n",
      "['sanidad', '##go', '##b']\n",
      "['director']\n",
      "['periodistas']\n",
      "['ciru', '##jan', '##a']\n",
      "['policia']\n",
      "['tor', '##ero']\n",
      "['ministro']\n",
      "['de']\n",
      "['sanidad']\n",
      "['cuerpos']\n",
      "['de']\n",
      "['seguridad']\n",
      "['personal']\n",
      "['sanitario']\n",
      "['periodista']\n",
      "['Magist', '##rado']\n",
      "['de']\n",
      "['la']\n",
      "['Audiencia']\n",
      "['Provincial']\n",
      "['expres', '##idente']\n",
      "['capitán']\n",
      "['miembros']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['Presidente']\n",
      "['del']\n",
      "['Gobierno']\n",
      "['médico']\n",
      "['Técnico']\n",
      "['en']\n",
      "['Integración']\n",
      "['Social']\n",
      "[]\n",
      "['Empresa', '##rios']\n",
      "['doctora']\n",
      "['asesor']\n",
      "['científico']\n",
      "['GC']\n",
      "['Guardia']\n",
      "['Civil']\n"
     ]
    }
   ],
   "source": [
    "entity_vectors = []\n",
    "import numpy as np\n",
    "for file_name, text in texts.items():\n",
    "    spacy_tokens = tweets[file_name]\n",
    "    tweet_tags = tags[file_name]\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    segments_ids = [1] * len(tokens)\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        #print(outputs)\n",
    "\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec_1 = torch.mean(token[-2:], dim=0)\n",
    "        sum_vec_2 = torch.mean(token[-4:-2], dim=0)\n",
    "        sum_vec = torch.cat((sum_vec_1, sum_vec_2))\n",
    "        \n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    #print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "    entity_vectors = entity_vectors + getEntitiesVectors(tokens, token_vecs_sum, spacy_tokens, tweet_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "with open('/home/sergio/Escritorio/ProfNER/final-saved_data/bert_entities.pickle', 'wb') as file:\n",
    "    pickle.dump(entity_vectors, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
