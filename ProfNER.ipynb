{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troy&AbedInTheMorning Team Participation in SMM4H-Spanish\n",
    "\n",
    "Team Participants:\n",
    "* Sergio Santamaria Carrasco\n",
    "* Roberto Cuervo Rosillo\n",
    "\n",
    "In these notebook we describe our proposed system based on an encoder-decoder architecture with an attention mechanism powered by a combination of word embeddings that include pre-trained fine-tuned spanish BERT embeddings.\n",
    "Our system serves as participation for ProfNER-ST focuses on the recognition of professions and occupations from Twitter using Spanish data.\n",
    "\n",
    "<a href=\"https://temu.bsc.es/smm4h-spanish/\">SMM4H-Spanish ProfNER-ST</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "1. Setting the parent dirrectory\n",
    "2. Checking GPU is avaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = '/home/sergio/Escritorio/ProfNER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "    Tensorflow 1.14.0\n",
    "    Keras 2.2.4\n",
    "    Fasttext 0.2.0\n",
    "    MeaningCloud-python\n",
    "    Spacy\n",
    "    Sklearn_crfsuite\n",
    "    Keras-contrib\n",
    "    Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Format\n",
    "\n",
    "The data of the corpus was obtained from a Twitter crawl that used keywords like â€œCovid-19â€, â€œepidemiaâ€ (epidemic) or â€œconfinamientoâ€ (lockdown), as well as hashtags such as â€œ#yomequedoencasaâ€ (#istayathome), to retrieve relevant tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerramos nuestra querida Radio ðŸ˜¢ Nuestros colaboradores y conductores Â¡Se quedan en casa! Desde maÃ±ana todos los programas de Radio Hoy se harÃ¡n vÃ­a remoto X Skype. Seguimos al aire con el compromiso de siempre, nosotros apoyamos el #QuedateEnCasa #covÄ±d19 #Coronavirus #RadioHoy https://t.co/Q81BBYpbdM\n"
     ]
    }
   ],
   "source": [
    "file_text = sst_home + '/profner-data/subtask-2/brat/train/1242399976644325376.txt' \n",
    "\n",
    "with open(file_text, 'r') as file:\n",
    "  print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations Format\n",
    "\n",
    "The corpus was annotated by linguist experts in an iterative process that included the creation of annotation guidelines specifically for this task. We could find the data annotated by BRAT and BIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1\tPROFESION 42 55\tcolaboradores\n",
      "T2\tPROFESION 58 69\tconductores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_ann = sst_home + '/profner-data/subtask-2/brat/train/1242399976644325376.ann' \n",
    "\n",
    "with open(file_ann) as file:\n",
    "  print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "\n",
    "    Tensorflow 1.14.0\n",
    "    Keras 2.2.4\n",
    "    Fasttext 0.2.0\n",
    "    MeaningCloud-python\n",
    "    Spacy\n",
    "    Sklearn_crfsuite\n",
    "    Keras-contrib\n",
    "    Pandas\n",
    "    Pickle5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syllabizer\n",
    "\n",
    "Using the rules of Spanish orthography, the syllabizer allows a word to be broken into its component syllables. The necessary code has been extracted from https://github.com/mabodo/sibilizador/blob/master/Silabizator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class char():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "class char_line():\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.char_line = [(char, self.char_type(char)) for char in word]\n",
    "        self.type_line = ''.join(chartype for char, chartype in self.char_line)\n",
    "        \n",
    "    def char_type(self, char):\n",
    "        if char in set(['a', 'Ã¡', 'e', 'Ã©','o', 'Ã³', 'Ã­', 'Ãº']):\n",
    "            return 'V' #strong vowel\n",
    "        if char in set(['i', 'u', 'Ã¼']):\n",
    "            return 'v' #week vowel\n",
    "        if char=='x':\n",
    "            return 'x'\n",
    "        if char=='s':\n",
    "            return 's'\n",
    "        else:\n",
    "            return 'c'\n",
    "            \n",
    "    def find(self, finder):\n",
    "        return self.type_line.find(finder)\n",
    "        \n",
    "    def split(self, pos, where):\n",
    "        return char_line(self.word[0:pos+where]), char_line(self.word[pos+where:])\n",
    "    \n",
    "    def split_by(self, finder, where):\n",
    "        split_point = self.find(finder)\n",
    "        if split_point!=-1:\n",
    "            chl1, chl2 = self.split(split_point, where)\n",
    "            return chl1, chl2\n",
    "        return self, False\n",
    "     \n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.word)\n",
    "\n",
    "class silabizer():\n",
    "    def __init__(self):\n",
    "        self.grammar = []\n",
    "        \n",
    "    def split(self, chars):\n",
    "        rules  = [('VV',1), ('cccc',2), ('xcc',1), ('ccx',2), ('csc',2), ('xc',1), ('cc',1), ('vcc',2), ('Vcc',2), ('sc',1), ('cs',1),('Vc',1), ('vc',1), ('Vs',1), ('vs',1)]\n",
    "        for split_rule, where in rules:\n",
    "            first, second = chars.split_by(split_rule,where)\n",
    "            if second:\n",
    "                if first.type_line in set(['c','s','x','cs']) or second.type_line in set(['c','s','x','cs']):\n",
    "                    #print 'skip1', first.word, second.word, split_rule, chars.type_line\n",
    "                    continue\n",
    "                if first.type_line[-1]=='c' and second.word[0] in set(['l','r']):\n",
    "                    continue\n",
    "                if first.word[-1]=='l' and second.word[-1]=='l':\n",
    "                    continue\n",
    "                if first.word[-1]=='r' and second.word[-1]=='r':\n",
    "                    continue\n",
    "                if first.word[-1]=='c' and second.word[-1]=='h':\n",
    "                    continue\n",
    "                return self.split(first)+self.split(second)\n",
    "        return [chars]\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        return self.split(char_line(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Model\n",
    "\n",
    "The Spacy model selected is 'es_core_news_md'. Cause this model doesn't tokenize hashtags, a new component is added at the end of the pipeline (HashtagMerger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import es_core_news_md\n",
    "\n",
    "nlp = es_core_news_md.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# We're using a class because the component needs to be initialised with\n",
    "# the shared vocab via the nlp object\n",
    "class HashtagMerger(object):\n",
    "    def __init__(self, nlp):\n",
    "        # Register a new token extension to flag bad HTML\n",
    "        Token.set_extension(\"is_hashtag\", default=False)\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        # Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "        self.matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"TEXT\": {\"REGEX\": \"[A-Za-zÃ¡Ã©Ã­Ã³ÃºÃÃ‰ÃÃ“Ãš]+\"}}])\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # This method is invoked when the component is called on a Doc\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # Collect the matched spans here\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for span in spans:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token._.is_hashtag = True  # Mark token as bad HTML\n",
    "        return doc\n",
    "    \n",
    "hashtag_merger = HashtagMerger(nlp)\n",
    "nlp.add_pipe(hashtag_merger, last = True)  # Add component to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRAT to BIOES annotation\n",
    "\n",
    "The next code transform the BRAT annotations into BIOES format. The code return a dict with the token offset and the BIOES schema. In this schema,tokens are annotated using the following tags:\n",
    "\n",
    "\n",
    "    B: represents a token that conform the begining of an entity.\n",
    "    I: indicate that the token belongs to an entity.\n",
    "    O: represents that the token does not belong to an entity.\n",
    "    E: marks a token as the end of a given entity.\n",
    "    S: indicates that an entity is comprised of a single token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# BIOES NOTATION #####################\n",
    "BEGIN = 'B'\n",
    "INSIDE = 'I'\n",
    "OUTSIDE = 'O'\n",
    "END = 'E'\n",
    "SINGLE = 'S'\n",
    "\n",
    "def getDictEntities(file_ann, ent_classes = ['PROFESION', 'SITUACION_LABORAL']):\n",
    "  entities = {}\n",
    "  with open(file_ann) as anns:\n",
    "    for ann in anns:\n",
    "      if ann.split('\\t')[1].split(' ')[0] in ent_classes:\n",
    "          ent = ann[:-1].split('\\t')[2]\n",
    "          #print(ent)\n",
    "          #ent = [token for token in nlp(ent) if not token.is_stop]\n",
    "          ent = nlp(ent)\n",
    "          start = int(ann[:-1].split('\\t')[1].split(' ')[1])\n",
    "          end = int(ann[:-1].split('\\t')[1].split(' ')[2])\n",
    "          if (len(ent) == 1):\n",
    "            entities[(start, end)] = SINGLE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "          else:\n",
    "            entities[(start, start + len(ent[0].text))] = BEGIN + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            entities[(end - len(ent[-1].text)), end] = END + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            for i in range(len(ent) - 2):\n",
    "              spaces = (ent[i + 1].idx) - (ent[i].idx + len(ent[i].text))\n",
    "              start = start + len(ent[i].text) + spaces\n",
    "              entities[(start, start + len(ent[i + 1].text))] = INSIDE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            \n",
    "  return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def getElements(sst_home, max_len_seq, getTags = True):\n",
    "  _words = dict()\n",
    "  _doc_tags = {}\n",
    "  _entities = {}\n",
    "  _docs = {}\n",
    "  _docs_offset = {}\n",
    "    \n",
    "  for file in [file[:-4] for file in os.listdir(sst_home) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home, file + '.txt')\n",
    "    if getTags:\n",
    "      file_ann = os.path.join(sst_home, file + '.ann')\n",
    "      _entities = getDictEntities(file_ann)\n",
    "    with open(file_text) as f:\n",
    "      text = f.read()\n",
    "      spacy_text = nlp(text)\n",
    "    \n",
    "      _tweet = []\n",
    "      _tweet_tags = []\n",
    "      _tweet_pos = []\n",
    "      _tweet_gtags = []\n",
    "      _tweet_ent = []\n",
    "      _tweet_offset = []\n",
    "    \n",
    "      for token in spacy_text[0:max_len_seq]:\n",
    "          if not token.like_url:\n",
    "              _tweet.append(token.text)\n",
    "              _entity = _entities.get((token.idx, token.idx + len(token.text)), 'O')\n",
    "              _tweet_tags.append(_entity)\n",
    "              _words[token.text] = _words.get(token.text, 0) + 1\n",
    "              _tweet_pos.append(token.pos_)\n",
    "              _tweet_gtags.append(token.tag_)\n",
    "              _tweet_offset.append((token.text, token.idx, token.idx + len(token.text)))\n",
    "\n",
    "    _docs[file] = (_tweet, _tweet_pos, _tweet_gtags, _tweet_ent)\n",
    "    _doc_tags[file] = _tweet_tags\n",
    "    _docs_offset[file] = _tweet_offset\n",
    "\n",
    "  return _words, _docs, _doc_tags, _docs_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the 2idx\n",
    "\n",
    "Our model is based in a Bag of Words/Charachters/Syllables/PoS-Tags, so we need to create a dictionary to assign each of one see it during training to a cardinal id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get2idx(words, chars, pos, gramm_tags, sylls, ents):\n",
    "    \n",
    "  ET = ['B_SITUACION_LABORAL', 'B_PROFESION', \n",
    "        'I_SITUACION_LABORAL', 'I_PROFESION', \n",
    "        'E_SITUACION_LABORAL', 'E_PROFESION', \n",
    "        'S_SITUACION_LABORAL', 'S_PROFESION',\n",
    "        'O']\n",
    "\n",
    "  word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "  word2idx[\"#ENDPAD\"] = 0\n",
    "  word2idx[\"UNK\"] = 1\n",
    "\n",
    "  char2idx = {char:i + 2 for i,char in enumerate(chars)}\n",
    "  char2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  char2idx[\"UNK\"] = 1\n",
    "\n",
    "  pos2idx = {pos:i + 2 for i,pos in enumerate(pos)}\n",
    "  pos2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  pos2idx[\"UNK\"] = 1\n",
    "\n",
    "  grammtags2idx = {tag:i + 2 for i,tag in enumerate(gramm_tags)}\n",
    "  grammtags2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  grammtags2idx[\"UNK\"] = 1\n",
    "\n",
    "  sylls2idx = {syll:i + 2 for i,syll in enumerate(sylls)}\n",
    "  sylls2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  sylls2idx[\"UNK\"] = 1\n",
    "\n",
    "  tag2idx = {tag:i + 1 for i,tag in enumerate(ET)}\n",
    "  tag2idx[\"#ENDPAD\"] = 0 \n",
    "\n",
    "  ent2idx = {ent:i + 2 for i,ent in enumerate(ents)}\n",
    "  ent2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  ent2idx[\"UNK\"] = 1\n",
    "\n",
    "  return word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx, ent2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the input Features\n",
    "\n",
    "The proposed system are feed with the next features:\n",
    "\n",
    "        Words: Two different 300 dimensional representations based on pre-trained word embeddings has been used with FastText. Both have been selected for their contribution of domain-specific knowledge since the former have been generated from Spanish medical corpora and the latter have been trained with Spanish Twitter data related to COVID-19. Contextual embeddings generated with a fine-tuned BETO model are also included, as these word representations are dynamically informed by the surrounding words improving performance.\n",
    "        \n",
    "        Part-of-speech: This feature has been considered due to the significant amount of information it offers about the word and its neighbors. It can also help in word sense disambiguation. The PoS-Tagging model used was the one provided by the Spacy. An embedding representaton of this feature is learned during training, resulting in a 40-dimensional vector.\n",
    "        \n",
    "        Characters: We also add character-level embeddings of the words, learned during training and resulting in a 30-dimensional vector. These have proven to be useful for specific-domain tasks and morphologically-rich languages.\n",
    "    \n",
    "       Syllables: Syllable-level embeddings of the words, learned during training and resulting in a 75-dimensional vector is also added. Like character-level embeddings, they help to deal with words outside the vocabulary and contribute to capturing common prefixes and suffixes in the domain and correctly classifying words.\n",
    "    \n",
    "       Cosine Similarity: The BETO embeddings of the entities found in the training and validation set are used to calculate the cosine similarity between the BETO representation of the word to be analyzed, since previous work has shown that could help to improve the results on data extracted from Twitter. This information is encoded as a 3717-dimensional vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHARACTERS ###\n",
    "\n",
    "def getCharacterInput(sentences, max_seq_len, max_chars_len, char2idx):\n",
    "  X_char = []\n",
    "  for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    for i in range(max_seq_len):\n",
    "      word_seq = []\n",
    "      # char sequence for words\n",
    "      for j in range(max_chars_len):\n",
    "        try:\n",
    "          # chars of specific sentence of i\n",
    "          word_seq.append(char2idx.get(sentence[i][j], 1)) \n",
    "        except:  # if char-sequence is out of range , pad it with \"PAD\" tag\n",
    "          word_seq.append(char2idx.get(\"#ENDPAD\"))\n",
    "\n",
    "      sent_seq.append(word_seq)\n",
    "    X_char.append(np.array(sent_seq))\n",
    "\n",
    "  return np.array(X_char)\n",
    "\n",
    "### SYLLABLES ###\n",
    "\n",
    "def getSyllsInput(sentences, max_seq_len, max_sylls_len, sylls2idx):\n",
    "  X_syll = []\n",
    "  for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    for i in range(max_seq_len):\n",
    "      word_seq = []\n",
    "      syllables = []\n",
    "      try:\n",
    "        syllables = silabizer(sentence[i])\n",
    "      except:\n",
    "        pass\n",
    "      for j in range(max_sylls_len):\n",
    "        try:\n",
    "          word_seq.append(sylls2idx.get(str(syllables[j]).lower(), 1)) \n",
    "        except: \n",
    "          word_seq.append(sylls2idx.get(\"#ENDPAD\"))\n",
    "\n",
    "      sent_seq.append(word_seq)\n",
    "    X_syll.append(np.array(sent_seq))\n",
    "\n",
    "  return np.array(X_syll)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "### WORDS ###\n",
    "\n",
    "def getWordInput(sentences, max_seq_len, word2idx):\n",
    "  X = [[word2idx.get(w,1) for w in s[:max_seq_len]] for s in sentences]\n",
    "  X = pad_sequences(maxlen = max_seq_len, sequences = X, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X)\n",
    "\n",
    "### PoS-Tags ###\n",
    "\n",
    "def getPosInput(sentences, max_seq_len, pos2idx):\n",
    "  X_pos = [[pos2idx.get(w,1) for w in s] for s in sentences]\n",
    "  X_pos = pad_sequences(maxlen = max_seq_len, sequences = X_pos, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X_pos)\n",
    "\n",
    "def getGTagInput(sentences, max_seq_len, grammtags2idx):\n",
    "  X_gramm_tags = [[grammtags2idx.get(w,1) for w in s] for s in sentences]\n",
    "  X_gramm_tags = pad_sequences(maxlen = max_seq_len, sequences = X_gramm_tags, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X_gramm_tags)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "### NER TAG ###\n",
    "def getY(tags, max_seq_len, tag2idx):\n",
    "  y = [[tag2idx[t] for t in sent_tags] for sent_tags in tags]\n",
    "  y = pad_sequences(maxlen = max_seq_len, sequences = y, padding = \"post\", truncating = \"post\", value = tag2idx[\"#ENDPAD\"])\n",
    "  \n",
    "  y = [to_categorical(i, num_classes = len(tag2idx)) for i in y]\n",
    "  return np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the total words\n",
    "\n",
    "Cause we are using pre-trained embeddings, we use as our vocabulary the words contained in both the training, development and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_total_words = set()\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/train'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_train, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            _total_words.add(token.text)\n",
    "           \n",
    "sst_home_test = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_test) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_test, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            _total_words.add(token.text)\n",
    "\n",
    "\n",
    "sst_home_test = sst_home + '/profner-data/subtask-2/test-background-txt-files'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_test) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_test, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                _total_words.add(token.text)\n",
    "\n",
    "_words = list(_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generating Training Data\n",
    "\n",
    "The next code generate the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/train'\n",
    "\n",
    "MAX_CHARS_LEN = 25\n",
    "MAX_SEQ_LEN = 75\n",
    "MAX_SYLLS_LEN = 10\n",
    "\n",
    "silabizer = silabizer()\n",
    "\n",
    "words_tr, docs, docs_tags, _ = getElements(sst_home_train, MAX_SEQ_LEN)\n",
    "words_tr = list(words_tr.keys())\n",
    "chars = list(set(''.join(words_tr)))\n",
    "sylls = list(set(str(syll).lower() for word in words_tr for syll in silabizer(word)))\n",
    "\n",
    "tweets_train = [tweet[0] for tweet in docs.values()]\n",
    "tweets_pos = [tweet[1] for tweet in docs.values()]\n",
    "tweets_gtags = [tweet[2] for tweet in docs.values()]\n",
    "\n",
    "all_pos = set(pos for sent in tweets_pos for pos in sent)\n",
    "all_gtags = set(gtag for sent in tweets_gtags for gtag in sent)\n",
    "\n",
    "tags = [tags for tags in docs_tags.values()]\n",
    "\n",
    "word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx = get2idx(_words, chars, all_pos, all_gtags, sylls)\n",
    "\n",
    "X_char_train = getCharacterInput(tweets_train, MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "X_words_train = getWordInput(tweets_train, MAX_SEQ_LEN, word2idx)\n",
    "X_pos_train = getPosInput(tweets_pos, MAX_SEQ_LEN, pos2idx)\n",
    "X_tag_train = getGTagInput(tweets_gtags, MAX_SEQ_LEN, grammtags2idx)\n",
    "X_syll_train = getSyllsInput(tweets_train, MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "\n",
    "y_train = getY(tags, MAX_SEQ_LEN, tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings\n",
    "\n",
    "We load the bert embeddings generated from the other notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "\n",
    "BERT_DIM = 1536\n",
    "\n",
    "train_bert = {}\n",
    "with open(sst_home + '/saved_data/bert_train.pickle', 'rb') as handle:\n",
    "    train_bert = pickle.load(handle)\n",
    "\n",
    "X_train_bert = []\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/train'\n",
    "\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    bert_vector = train_bert[file]\n",
    "    X_train_bert.append(bert_vector)\n",
    "X_train_bert = pad_sequences(maxlen = 75, sequences = X_train_bert, truncating= 'post', padding ='post', value=np.zeros(BERT_DIM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Validation Data\n",
    "\n",
    "The next code generate the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "MAX_CHARS_LEN = 25\n",
    "MAX_SEQ_LEN = 75\n",
    "MAX_SYLLS_LEN = 10\n",
    "\n",
    "sst_home_test = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "\n",
    "_, docs, docs_tags, docs_offset = getElements(sst_home_test, MAX_SEQ_LEN, get)\n",
    "\n",
    "tweets_test = [tweet[0] for tweet in docs.values()]\n",
    "tweets_pos = [tweet[1] for tweet in docs.values()]\n",
    "tweets_gtags = [tweet[2] for tweet in docs.values()]\n",
    "\n",
    "\n",
    "tags_test = [tags for tags in docs_tags.values()]\n",
    "\n",
    "X_char_test = getCharacterInput(tweets_test, MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "X_words_test = getWordInput(tweets_test, MAX_SEQ_LEN, word2idx)\n",
    "X_pos_test = getPosInput(tweets_pos, MAX_SEQ_LEN, pos2idx)\n",
    "X_tag_test = getGTagInput(tweets_gtags, MAX_SEQ_LEN, grammtags2idx)\n",
    "X_syll_test = getSyllsInput(tweets_test, MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "\n",
    "y_test = getY(tags_test, MAX_SEQ_LEN, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bert = {}\n",
    "with open(sst_home + '/saved_data/test_bert.pickle', 'rb') as handle:\n",
    "    test_bert = pickle.load(handle)\n",
    "    \n",
    "def getBertInput(file_name):\n",
    "    bert_vector = test_bert[file_name]\n",
    "    X_test_bert = [bert_vector]\n",
    "    X_test_bert = pad_sequences(maxlen = 75, sequences = X_test_bert, truncating= 'post', padding ='post', value=np.zeros(1536))\n",
    "    return X_test_bert\n",
    "\n",
    "X_test_bert = []\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    bert_vector = test_bert[file]\n",
    "    X_test_bert.append(bert_vector)\n",
    "X_test_bert = pad_sequences(maxlen = 75, sequences = X_test_bert, truncating= 'post', padding ='post', value=np.zeros(BERT_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings from Spanish Medical Corpora & Twitter\n",
    "\n",
    "In this model we'll use two different pre-trained word embeddings:\n",
    "\n",
    "1. The Word Embeddings from Spanish Medical Corpora can be found in https://www.aclweb.org/anthology/W19-1916/\n",
    "2. The Word Embedding from Spanish Twitter (Covid-19) can be found in https://zenodo.org/record/4449930#.YBbYOtaCE5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "def getEmbeddingMatrix(words2idx, emb_dim, model):\n",
    "  embedding_matrix = np.zeros((len(words2idx), emb_dim))\n",
    "  for word, i in words2idx.items():\n",
    "    embedding_matrix[i] = model[word]\n",
    "\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sst_home_embeddings = sst_home + '/fast-text-model/'\n",
    "\n",
    "### SPANISH MEDICAL CORPORA ###\n",
    "ft = fasttext.load_model(sst_home_embeddings + 'cantemist-resource.bin')\n",
    "medical_embedding_matrix = getEmbeddingMatrix(word2idx, ft.get_dimension(), ft)\n",
    "del ft\n",
    "\n",
    "### SPANISH TWITTER COVID-19 ###\n",
    "ft = fasttext.load_model(sst_home_embeddings + 'covid_19_es_twitter_cbow_cased.bin')\n",
    "twitter_embedding_matrix = getEmbeddingMatrix(word2idx, ft.get_dimension(), ft)\n",
    "del ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "\n",
    "We save the generated data so that it is not necessary to generate it again next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "train_data = [X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_train_bert, y_train]\n",
    "with open(sst_home + '/saved_data/train.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "test_data = [X_char_test, X_words_test, X_pos_test, X_tag_test, X_syll_test, X_test_bert, y_test]\n",
    "with open(sst_home + '/saved_data/test.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "embedding_matrix = [medical_embedding_matrix, twitter_embedding_matrix, cosine_matrix, spacy_embedding_matrix]   \n",
    "with open(sst_home + '/saved_data/we.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "two2idx = [word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx, ent2idx]\n",
    "with open(sst_home + '/saved_data/2idx.pickle', 'wb') as handle:\n",
    "    pickle.dump(two2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "In case the data is previously saved, we load these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "X_train = []\n",
    "with open(sst_home + '/saved_data/train.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "\n",
    "X_test = []\n",
    "with open(sst_home + '/saved_data/test.pickle', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "\n",
    "embedding_matrixes = []\n",
    "with open(sst_home + '/saved_data/we.pickle', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)\n",
    "\n",
    "two2idx = []\n",
    "with open(sst_home + '/saved_data/2idx.pickle', 'rb') as handle:\n",
    "    two2idx = pickle.load(handle)\n",
    "    \n",
    "\n",
    "\n",
    "X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_train_bert, y_train = X_train\n",
    "X_char_test, X_words_test, X_pos_test, X_tag_test, X_syll_test, X_test_bert, y_test = X_test\n",
    "word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx, ent2idx = two2idx\n",
    "medical_embedding_matrix, twitter_embedding_matrix, _, spacy_embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings Entities\n",
    "\n",
    "The BERT embeddings of the entities founded in train corpus are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "bert_entities = []\n",
    "with open(sst_home + '/saved_data/bert_entities.pickle', 'rb') as handle:\n",
    "    bert_entities = pickle.load(handle)\n",
    "    \n",
    "bert_entities_ = np.array(bert_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Architecture\n",
    "\n",
    "In the proposed system, the character and syllable information is previously proccesed by a convolutional and global max pooling block, to be concatenated with the rest of the input features to serve as input to an encoder-decoder architecture with attention mechanism. The context vector as well as decoder outputs feeds a fully connected dense layer with $tanh$ activation function. The last layer (CRF optimization layer) consists of a conditional random fields layer selected due to the ability of the layer to take into account the dependencies between the different labels.  The output of this layer provides the most probable sequence of labels.\n",
    "\n",
    "![Architecture of the proposed model for pro-fession and occupations recognition.](./imgs/model2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Layer\n",
    "\n",
    "A custom Keras layer is created to calculate Cosine Similarity between BERT Word Embeddings and Entities found in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.backend import constant\n",
    "\n",
    "class CosineSimilarity(Layer):\n",
    "    def __init__(self):\n",
    "        super(CosineSimilarity, self).__init__()\n",
    "        self.result = None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        entities = constant(bert_entities_)\n",
    "        bert_input = inputs\n",
    "        norm_entities_bert = tf.norm(entities, axis = 1)\n",
    "        norm_bert_input = tf.norm(bert_input, axis =  2)\n",
    "\n",
    "        cosine = tf.einsum('nd,bmd->bmn', entities, bert_input)\n",
    "        norm = tf.einsum('bm,n->bmn',norm_bert_input, norm_entities_bert)\n",
    "\n",
    "        self.result = tf.math.divide_no_nan(cosine, norm)\n",
    "        \n",
    "        return self.result\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(None, 75, 3717)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Concatenate, Input, SpatialDropout1D\n",
    "from keras.layers import Conv1D, MaxPooling1D,Flatten,GlobalMaxPooling1D, Reshape, RepeatVector,  Dot, GRU, Activation\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.backend import constant, variable\n",
    "\n",
    "CHAR_EMBEDDINGS_SIZE = 30    # Characters Embeddings Size\n",
    "SYLL_EMBEDDINGS_SIZE = 75    # Syllable Embeddings Size\n",
    "WORD_EMBEDDINGS_SIZE = 300   # Word Embeddings Size\n",
    "POS_EMBEDDING_SIZE = 20      # PoS Embedding Size\n",
    "GTAGS_EMBEDDING_SIZE = 20    # Tag Embedding Size\n",
    "\n",
    "MAX_CHARS_LEN = 25           # Max sequence char length\n",
    "MAX_SEQ_LEN = 75             # Max sequence word length Level II\n",
    "MAX_SYLLS_LEN = 10           # Max sequence sylls length\n",
    "\n",
    "CONV_FILTERS = 50            # Convolutional Filters in Character and Syllable Convolutional Layer\n",
    "LSTM_UNITS = 300             # LSTM Units in both LSTM layers\n",
    "DENSE_UNITS = 200            # Number of units in Dense layer  \n",
    "\n",
    "BERT_DIM = 1536\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), name = 'word_input')\n",
    "pos_input = Input(shape=(MAX_SEQ_LEN,), name = 'pos_input')\n",
    "gtag_input = Input(shape=(MAX_SEQ_LEN,), name = 'tag_input')\n",
    "char_input = Input(shape=(MAX_SEQ_LEN, MAX_CHARS_LEN), name = 'char_input')\n",
    "sylls_input = Input(shape=(MAX_SEQ_LEN, MAX_SYLLS_LEN), name = 'sylls_input')\n",
    "bert_input = Input(shape=(MAX_SEQ_LEN, BERT_DIM,), name = 'bert_input')\n",
    "\n",
    "###################### COSINE SIMILARITY ##################################################################\n",
    "cosine_bert = CosineSimilarity()(bert_input)\n",
    "\n",
    "###################### CHARACTER SEQUENCE PROCCESSED BY CONVOLUTIONAL LAYER ################################\n",
    "char_embedding = TimeDistributed(Embedding(input_dim = len(char2idx), output_dim = CHAR_EMBEDDINGS_SIZE, input_length=MAX_CHARS_LEN, name = 'char_embeddings', trainable = True))(char_input)\n",
    "conv_1d = TimeDistributed(Conv1D(filters = CONV_FILTERS, kernel_size = 3,padding = \"valid\", activation = \"relu\", name=\"Conv1D_char\"))(char_embedding)\n",
    "conv_1d = TimeDistributed(Dropout(0.4))(conv_1d)\n",
    "maxpool1d = TimeDistributed(GlobalMaxPooling1D(), name = 'max_pooling')(conv_1d)\n",
    "char_enc = TimeDistributed(Flatten(), name = 'char_enc')(maxpool1d)\n",
    "\n",
    "###################### SYLLABLE SEQUENCE PROCCESSED BY CONVOLUTIONAL LAYER ################################\n",
    "syll_embedding = TimeDistributed(Embedding(input_dim = len(sylls2idx), output_dim = SYLL_EMBEDDINGS_SIZE, input_length=MAX_SYLLS_LEN, name = 'sylls_embeddings', trainable = True))(sylls_input)\n",
    "conv_1d_syll = TimeDistributed(Conv1D(filters = CONV_FILTERS, kernel_size = 3,padding=\"valid\", activation=\"relu\", name=\"Conv1D_syll\"))(syll_embedding)\n",
    "conv_1d_syll = TimeDistributed(Dropout(0.4))(conv_1d_syll)\n",
    "maxpool1d_syll = TimeDistributed(GlobalMaxPooling1D())(conv_1d_syll)\n",
    "syll_enc = TimeDistributed(Flatten())(maxpool1d_syll)\n",
    "\n",
    "##################### PoS + TAG EMBEDDINGS ################################################################\n",
    "pos_embedding = Embedding(input_dim = len(pos2idx), output_dim = POS_EMBEDDING_SIZE, input_length = MAX_SEQ_LEN, name = 'pos_embeddings', trainable = True)(pos_input)\n",
    "pos_embedding = Dropout(0.4)(pos_embedding)\n",
    "gtags_embedding = Embedding(input_dim = len(grammtags2idx), output_dim = GTAGS_EMBEDDING_SIZE, input_length = MAX_SEQ_LEN, name = 'gtags_embeddings', trainable = True)(gtag_input)\n",
    "gtags_embedding = Dropout(0.4)(gtags_embedding)\n",
    "\n",
    "##################### WORD EMBEDDING LAYER ################################################################\n",
    "medical_embedding_layer = Embedding(input_dim = len(word2idx), output_dim = WORD_EMBEDDINGS_SIZE, weights=[medical_embedding_matrix], trainable=True, name = 'medical_word_embeddings')\n",
    "medical_word_embedding = medical_embedding_layer(word_input)\n",
    "\n",
    "twitter_embedding_layer = Embedding(input_dim = len(word2idx), output_dim = WORD_EMBEDDINGS_SIZE, weights=[twitter_embedding_matrix], trainable=True, name = 'twitter_word_embeddings')\n",
    "twitter_word_embedding = twitter_embedding_layer(word_input)\n",
    "\n",
    "bert_out = Dense(BERT_DIM, activation='relu')(bert_input)\n",
    "bert_out = Dropout(0.4)(bert_out)\n",
    "\n",
    "################### CONCATENATE INPUT FEATURES ###########################################\n",
    "x = Concatenate(axis = -1)([medical_word_embedding, twitter_word_embedding, bert_out, pos_embedding, gtags_embedding, char_enc, syll_enc, cosine_bert])\n",
    "\n",
    "################## BiLSTM MAX SEQUENCE WORD LENGTH 50 ######################################################\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(units=LSTM_UNITS//2, return_sequences=True,recurrent_dropout=0, return_state = True, recurrent_activation = 'sigmoid'))(x)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "################## Attention Mechanism ################################################################\n",
    "decoder_outputs, _, _ = LSTM(units=LSTM_UNITS, recurrent_dropout=0, return_sequences = True, recurrent_activation = 'sigmoid', return_state = True)(x, initial_state = encoder_states)\n",
    "attention = Dot(axes = (2,2))([decoder_outputs, encoder_outputs])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = Dot(axes=(2,1))([attention, encoder_outputs])\n",
    "x = Concatenate()([context, decoder_outputs])\n",
    "################## DENSE LAYER ############################################################################\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(DENSE_UNITS, activation='tanh')(x)\n",
    "\n",
    "################## CRF LAYER ####################################################################################\n",
    "crf = CRF(len(tag2idx), sparse_target = False)\n",
    "loss = crf.loss_function\n",
    "y_output = crf(x)\n",
    "\n",
    "loss = crf.loss_function\n",
    "\n",
    "model = Model(inputs = [char_input, word_input, pos_input, gtag_input, sylls_input, bert_input], outputs = y_output)\n",
    "model.compile(optimizer = \"adam\", loss = loss, metrics = [crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "checkpoint = ModelCheckpoint(sst_home + '/model_weights/final_model_4.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "history = model.fit([X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_train_bert], y_train,\n",
    "                    batch_size = 32,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the annotations\n",
    "\n",
    "The next step is generate the annotations in the right format to be evaluated. First we loaded the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.backend import constant\n",
    "\n",
    "class CosineSimilarity(Layer):\n",
    "    def __init__(self):\n",
    "        super(CosineSimilarity, self).__init__()\n",
    "        self.result = None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        entities = constant(bert_entities_)\n",
    "        bert_input = inputs\n",
    "        norm_entities_bert = tf.norm(entities, axis = 1)\n",
    "        norm_bert_input = tf.norm(bert_input, axis =  2)\n",
    "\n",
    "        cosine = tf.einsum('nd,bmd->bmn', entities, bert_input)\n",
    "        norm = tf.einsum('bm,n->bmn',norm_bert_input, norm_entities_bert)\n",
    "\n",
    "        self.result = tf.math.divide_no_nan(cosine, norm)\n",
    "        \n",
    "        return self.result\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(None, None, 3717)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Concatenate, Input, SpatialDropout1D\n",
    "from keras.layers import Conv1D, MaxPooling1D,Flatten,GlobalMaxPooling1D, Reshape, RepeatVector,  Dot, GRU, Activation\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.backend import constant, variable\n",
    "\n",
    "CHAR_EMBEDDINGS_SIZE = 30    # Characters Embeddings Size\n",
    "SYLL_EMBEDDINGS_SIZE = 75    # Syllable Embeddings Size\n",
    "WORD_EMBEDDINGS_SIZE = 300   # Word Embeddings Size\n",
    "POS_EMBEDDING_SIZE = 20      # PoS Embedding Size\n",
    "GTAGS_EMBEDDING_SIZE = 20    # Tag Embedding Size\n",
    "SPACY_EMBEDDING_SIZE = 20\n",
    "\n",
    "MAX_CHARS_LEN = 25           # Max sequence char length\n",
    "MAX_SEQ_LEN = 75             # Max sequence word length Level II\n",
    "MAX_SYLLS_LEN = 10           # Max sequence sylls length\n",
    "\n",
    "CONV_FILTERS = 50            # Convolutional Filters in Character and Syllable Convolutional Layer\n",
    "LSTM_UNITS = 300             # LSTM Units in both LSTM layers\n",
    "DENSE_UNITS = 200            # Number of units in Dense layer  \n",
    "\n",
    "BERT_DIM = 1536\n",
    "\n",
    "word_input = Input(shape=(None,), name = 'word_input')\n",
    "pos_input = Input(shape=(None,), name = 'pos_input')\n",
    "gtag_input = Input(shape=(None,), name = 'tag_input')\n",
    "char_input = Input(shape=(None, MAX_CHARS_LEN), name = 'char_input')\n",
    "sylls_input = Input(shape=(None, MAX_SYLLS_LEN), name = 'sylls_input')\n",
    "bert_input = Input(shape=(None, BERT_DIM,), name = 'bert_input')\n",
    "\n",
    "###################### COSINE SIMILARITY ##################################################################\n",
    "cosine_bert = CosineSimilarity()(bert_input)\n",
    "\n",
    "###################### CHARACTER SEQUENCE PROCCESSED BY CONVOLUTIONAL LAYER ################################\n",
    "char_embedding = TimeDistributed(Embedding(input_dim = len(char2idx), output_dim = CHAR_EMBEDDINGS_SIZE, input_length=MAX_CHARS_LEN, name = 'char_embeddings', trainable = True))(char_input)\n",
    "conv_1d = TimeDistributed(Conv1D(filters = CONV_FILTERS, kernel_size = 3,padding = \"valid\", activation = \"relu\", name=\"Conv1D_char\"))(char_embedding)\n",
    "conv_1d = TimeDistributed(Dropout(0.4))(conv_1d)\n",
    "maxpool1d = TimeDistributed(GlobalMaxPooling1D(), name = 'max_pooling')(conv_1d)\n",
    "char_enc = TimeDistributed(Flatten(), name = 'char_enc')(maxpool1d)\n",
    "\n",
    "###################### SYLLABLE SEQUENCE PROCCESSED BY CONVOLUTIONAL LAYER ################################\n",
    "syll_embedding = TimeDistributed(Embedding(input_dim = len(sylls2idx), output_dim = SYLL_EMBEDDINGS_SIZE, input_length=MAX_SYLLS_LEN, name = 'sylls_embeddings', trainable = True))(sylls_input)\n",
    "conv_1d_syll = TimeDistributed(Conv1D(filters = CONV_FILTERS, kernel_size = 3,padding=\"valid\", activation=\"relu\", name=\"Conv1D_syll\"))(syll_embedding)\n",
    "conv_1d_syll = TimeDistributed(Dropout(0.4))(conv_1d_syll)\n",
    "maxpool1d_syll = TimeDistributed(GlobalMaxPooling1D())(conv_1d_syll)\n",
    "syll_enc = TimeDistributed(Flatten())(maxpool1d_syll)\n",
    "\n",
    "##################### PoS + TAG EMBEDDINGS ################################################################\n",
    "pos_embedding = Embedding(input_dim = len(pos2idx), output_dim = POS_EMBEDDING_SIZE, name = 'pos_embeddings', trainable = True)(pos_input)\n",
    "pos_embedding = Dropout(0.4)(pos_embedding)\n",
    "gtags_embedding = Embedding(input_dim = len(grammtags2idx), output_dim = GTAGS_EMBEDDING_SIZE, name = 'gtags_embeddings', trainable = True)(gtag_input)\n",
    "gtags_embedding = Dropout(0.4)(gtags_embedding)\n",
    "\n",
    "##################### WORD EMBEDDING LAYER ################################################################\n",
    "medical_embedding_layer = Embedding(input_dim = len(word2idx), output_dim = WORD_EMBEDDINGS_SIZE, weights=[medical_embedding_matrix], trainable=True, name = 'medical_word_embeddings')\n",
    "medical_word_embedding = medical_embedding_layer(word_input)\n",
    "\n",
    "twitter_embedding_layer = Embedding(input_dim = len(word2idx), output_dim = WORD_EMBEDDINGS_SIZE, weights=[twitter_embedding_matrix], trainable=True, name = 'twitter_word_embeddings')\n",
    "twitter_word_embedding = twitter_embedding_layer(word_input)\n",
    "\n",
    "bert_out = Dense(BERT_DIM, activation='relu')(bert_input)\n",
    "bert_out = Dropout(0.4)(bert_out)\n",
    "################### CONCATENATE INPUT FEATURES ###########################################\n",
    "x = Concatenate(axis = -1)([medical_word_embedding, twitter_word_embedding, bert_out, pos_embedding, gtags_embedding, char_enc, syll_enc, cosine_bert])\n",
    "\n",
    "################## BiLSTM MAX SEQUENCE WORD LENGTH 50 ######################################################\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(units=LSTM_UNITS//2, return_sequences=True,recurrent_dropout=0, return_state = True, recurrent_activation = 'sigmoid'))(x)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "################## Attention Mechanism ################################################################\n",
    "decoder_outputs, _, _ = LSTM(units=LSTM_UNITS, recurrent_dropout=0, return_sequences = True, recurrent_activation = 'sigmoid', return_state = True)(x, initial_state = encoder_states)\n",
    "attention = Dot(axes = (2,2))([decoder_outputs, encoder_outputs])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = Dot(axes=(2,1))([attention, encoder_outputs])\n",
    "\n",
    "x = Concatenate()([context, decoder_outputs])\n",
    "################## DENSE LAYER ############################################################################\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(DENSE_UNITS, activation='tanh')(x)\n",
    "\n",
    "################## CRF LAYER ####################################################################################\n",
    "crf = CRF(len(tag2idx), sparse_target = False)\n",
    "loss = crf.loss_function\n",
    "y_output = crf(x)\n",
    "\n",
    "loss = crf.loss_function\n",
    "\n",
    "model = Model(inputs = [char_input, word_input, pos_input, gtag_input, sylls_input, bert_input], outputs = y_output)\n",
    "model.compile(optimizer = \"adam\", loss = loss, metrics = [crf.accuracy])\n",
    "model.load_weights(sst_home + '/model_weights/final_model_4.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE = 'S'\n",
    "BEGIN = 'B'\n",
    "END = 'E'\n",
    "INSIDE = 'I'\n",
    "OUT = 'O'\n",
    "\n",
    "def annotateV2(y_pred, offset, file_name, extra = None):    \n",
    "    max_index = len(y_pred)\n",
    "    if '#ENDPAD' in y_pred:\n",
    "        max_index = y_pred.index('#ENDPAD')\n",
    "        \n",
    "    right_moves = {'B': [ 'I', 'E'],\n",
    "                   'I': ['I', 'E'],\n",
    "                   'E': ['B', 'S', 'O'],\n",
    "                   'S': ['B', 'S', 'O'],\n",
    "                   'O': ['B', 'S', 'O']}\n",
    "    text = ''\n",
    "    entity = ''\n",
    "    start = -1\n",
    "    end = -1\n",
    "    \n",
    "    has_error = False\n",
    "    for i in range(max_index - 1):\n",
    "        if not (has_error and y_pred[i][0] in ['I','E']):\n",
    "            has_error = False\n",
    "            ann = y_pred[i][0]\n",
    "            info = offset[i]\n",
    "            next_ann = y_pred[i + 1][0]\n",
    "            entity_class = y_pred[i][2:]\n",
    "            if next_ann in right_moves[ann]:\n",
    "                info = offset[i]\n",
    "                if ann == BEGIN:\n",
    "                    entity = info[0]\n",
    "                    start = info[1]\n",
    "                    end = info[2]\n",
    "                if ann == INSIDE:\n",
    "                    entity = entity + ' ' + info[0]\n",
    "                    end = info[2]\n",
    "                if ann == END:\n",
    "                    entity = entity + ' ' + info[0]\n",
    "                    end = info[2]\n",
    "                    text = text + f'{file_name}\\t{start}\\t{end}\\t{entity_class}\\t{entity}\\n'\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "                if ann == SINGLE:\n",
    "                    text = text + f'{file_name}\\t{info[1]}\\t{info[2]}\\t{entity_class}\\t{info[0]}\\n'\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "                if ann == OUT:\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "            else:\n",
    "                has_error = True\n",
    "                \n",
    "                entity = ''\n",
    "                start = -1\n",
    "                end = -1\n",
    "            \n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sst_home_test = sst_home + '/final-profner-data/subtask-2/test-background-txt-files'\n",
    "idx2tag = {idx:tag for (tag, idx) in tag2idx.items()}\n",
    "\n",
    "_, docs, _, docs_offset = getElements(sst_home_test, 1000, getTags = False, minit = 25000, maxi = 28000)\n",
    "\n",
    "result_path = sst_home + '/final-result/'\n",
    "text = ''\n",
    "\n",
    "for (file_name, doc_sents) in docs.items():\n",
    "  MAX_SEQ_LEN = len(docs[file_name][0])\n",
    "  X_char = getCharacterInput([docs[file_name][0]], MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "  X_words = getWordInput([docs[file_name][0]], MAX_SEQ_LEN, word2idx)\n",
    "  X_pos = getPosInput([docs[file_name][1]], MAX_SEQ_LEN, pos2idx)\n",
    "  X_tag = getGTagInput([docs[file_name][2]], MAX_SEQ_LEN, grammtags2idx)\n",
    "  X_syll = getSyllsInput([docs[file_name][0]], MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "  X_bert = np.array(getBertInput(file_name))\n",
    "  X_bert = pad_sequences(maxlen = MAX_SEQ_LEN, sequences = X_bert, truncating= 'post', padding ='post', value=np.zeros(1536))\n",
    "\n",
    "  y_pred = model.predict([X_char, X_words, X_pos, X_tag, X_syll, X_bert])\n",
    "  print(y_pred)\n",
    "  y_pred = [list(map(lambda x: idx2tag[np.argmax(x)], sent)) for sent in y_pred][0]\n",
    "  offset_test = docs_offset[file_name]\n",
    "  text = text + annotateV2(y_pred, offset_test, file_name, result_path)\n",
    "\n",
    "text = 'tweet_id\\tbegin\\tend\\ttype\\textraction\\n' + text\n",
    "with open(result_path + 'results.tsv', 'w') as f:\n",
    "    f.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
