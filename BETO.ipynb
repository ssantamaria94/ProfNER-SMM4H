{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "laden-biotechnology",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chronic-aerospace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: transformers in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (4.3.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: filelock in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: requests in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (4.56.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (2.0.0)\n",
      "Requirement already satisfied: dataclasses in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: joblib in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from matplotlib) (1.19.5)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 50.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 49.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.3.4 pillow-8.1.0\n",
      "Requirement already satisfied: spacy in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (2.3.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (7.4.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (4.56.2)\n",
      "Requirement already satisfied: setuptools in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.5 pytz-2021.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install matplotlib\n",
    "!pip install spacy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-sender",
   "metadata": {},
   "source": [
    "## BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surrounded-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = '/home/sergio/Escritorio/ProfNER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vulnerable-perry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==2.3.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (2.3.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (4.56.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (1.19.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (2.25.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy==2.3.5) (7.4.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==2.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dense-tampa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es_core_news_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.3.1/es_core_news_md-2.3.1.tar.gz (47.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 47.4 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from es_core_news_md==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.56.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import es_core_news_md\n",
    "\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "radical-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sergio/Escritorio/ProfNER/profner-data/subtask-2/brat/valid\n"
     ]
    }
   ],
   "source": [
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "print(sst_home_train)\n",
    "import re\n",
    "import os\n",
    "regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "texts = []\n",
    "\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_train, file + '.txt')\n",
    "    with open(file_text) as f:\n",
    "      doc = f.read()\n",
    "      #print(doc)\n",
    "      doc = doc.replace('#', '')\n",
    "      doc = re.sub(regex, '', doc)\n",
    "      #print(doc)\n",
    "    \n",
    "    spacy_text = nlp(doc)\n",
    "    text = '[CLS] '\n",
    "    for sent in spacy_text.sents:\n",
    "        text = text + sent.text + ' [SEP]'\n",
    "    \n",
    "    texts.append((text, [token.text for token in spacy_text], file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "front-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-13 15:53:41--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
      "Resolviendo users.dcc.uchile.cl (users.dcc.uchile.cl)... 200.9.99.211, 192.80.24.4\n",
      "Conectando con users.dcc.uchile.cl (users.dcc.uchile.cl)[200.9.99.211]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 409871727 (391M) [application/x-gzip]\n",
      "Guardando como: “pytorch_weights.tar.gz”\n",
      "\n",
      "pytorch_weights.tar 100%[===================>] 390,88M  4,34MB/s    en 5m 33s  \n",
      "\n",
      "2021-02-13 15:59:16 (1,17 MB/s) - “pytorch_weights.tar.gz” guardado [409871727/409871727]\n",
      "\n",
      "--2021-02-13 15:59:16--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
      "Resolviendo users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
      "Conectando con users.dcc.uchile.cl (users.dcc.uchile.cl)[192.80.24.4]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 242120 (236K) [text/plain]\n",
      "Guardando como: “vocab.txt”\n",
      "\n",
      "vocab.txt           100%[===================>] 236,45K   205KB/s    en 1,2s    \n",
      "\n",
      "2021-02-13 15:59:19 (205 KB/s) - “vocab.txt” guardado [242120/242120]\n",
      "\n",
      "--2021-02-13 15:59:19--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
      "Resolviendo users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
      "Conectando con users.dcc.uchile.cl (users.dcc.uchile.cl)[192.80.24.4]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 313 [application/json]\n",
      "Guardando como: “config.json”\n",
      "\n",
      "config.json         100%[===================>]     313  --.-KB/s    en 0s      \n",
      "\n",
      "2021-02-13 15:59:20 (22,4 MB/s) - “config.json” guardado [313/313]\n",
      "\n",
      "pytorch/\n",
      "pytorch/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
    "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
    "!tar -xzvf pytorch_weights.tar.gz\n",
    "!mv config.json pytorch/.\n",
    "!mv vocab.txt pytorch/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stretch-trader",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/ and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs', do_lower_case=False)\n",
    "model = BertModel.from_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/', output_hidden_states = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alone-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(tokens, tokens_vectors, spacy_tokens, BETO_DIM = 1536):\n",
    "    beto_2_spacy = {}\n",
    "    beto_enum = {}\n",
    "    special_tokens = ['[CLS]', '[SEP]', '[UNK]']\n",
    "\n",
    "    token2txt = ''\n",
    "    token2vect = []\n",
    "    for token, vector in zip(tokens, tokens_vectors):\n",
    "        if token in special_tokens:\n",
    "            pass\n",
    "        else:\n",
    "            if not token.startswith('#'):\n",
    "                if token2txt != '':\n",
    "                    mean = torch.mean(torch.stack(token2vect), dim = 0)\n",
    "                    mean = mean.cpu().detach().numpy()\n",
    "                    idx = beto_enum.get(token2txt, -1) + 1\n",
    "                    beto_2_spacy[(token2txt, idx)] = mean\n",
    "                token2txt = token\n",
    "                token2vect = [vector]\n",
    "            else:\n",
    "                token2txt = token2txt + token.replace('#','')\n",
    "                token2vect.append(vector)\n",
    "    if token2txt != '':\n",
    "        mean = torch.mean(torch.stack(token2vect), dim = 0)\n",
    "        mean = mean.cpu().detach().numpy()\n",
    "        idx = beto_enum.get(token2txt, -1) + 1\n",
    "        beto_2_spacy[(token2txt, idx)] = mean\n",
    "    \n",
    "    tensor_enum = {}\n",
    "    tweet_data = []\n",
    "    for token in spacy_tokens:\n",
    "        idx = tensor_enum.get(token, -1) + 1\n",
    "        if (token, idx) in beto_2_spacy:\n",
    "            tweet_data.append(beto_2_spacy[(token,idx)])\n",
    "        else:\n",
    "            tweet_data.append(np.zeros(BETO_DIM))\n",
    "        \n",
    "    return np.array(tweet_data)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "architectural-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "import numpy as np\n",
    "# Now test it\n",
    "for elements in texts:\n",
    "    text = elements[0]\n",
    "    spacy_tokens = elements[1]\n",
    "    file_name = elements[2]\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    segments_ids = [1] * len(tokens)\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        #print(outputs)\n",
    "\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec_1 = torch.mean(token[-2:], dim=0)\n",
    "        sum_vec_2 = torch.mean(token[-4:-2], dim=0)\n",
    "        sum_vec = torch.cat((sum_vec_1, sum_vec_2))\n",
    "        \n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    #print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "    data[file_name] = getVectors(tokens, token_vecs_sum, spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "other-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle5\n",
      "  Using cached pickle5-0.0.11.tar.gz (132 kB)\n",
      "Building wheels for collected packages: pickle5\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp36-cp36m-linux_x86_64.whl size=247167 sha256=64ce560b25bdf24715a461cb168319b43f52cdfcfacb4d43d5c3714b97ea74b0\n",
      "  Stored in directory: /home/sergio/.cache/pip/wheels/f9/b7/be/bf9768ab0daa28fa4b386f3ad1bac5dd4d9c349c60e83b24e3\n",
      "Successfully built pickle5\n",
      "Installing collected packages: pickle5\n",
      "Successfully installed pickle5-0.0.11\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "everyday-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "with open('/home/sergio/Escritorio/ProfNER/saved_data/bert_valid.pickle', 'wb') as file:\n",
    "    pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-villa",
   "metadata": {},
   "source": [
    "## BETO fine-tuning to ProfNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latter-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = '/home/sergio/Escritorio/ProfNER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "personalized-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es_core_news_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.3.1/es_core_news_md-2.3.1.tar.gz (47.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 47.4 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from es_core_news_md==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: setuptools in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.56.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import es_core_news_md\n",
    "\n",
    "nlp = es_core_news_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "employed-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# BIOES NOTATION #####################\n",
    "BEGIN = 'B'\n",
    "INSIDE = 'I'\n",
    "OUTSIDE = 'O'\n",
    "END = 'E'\n",
    "SINGLE = 'S'\n",
    "\n",
    "def getDictEntities(file_ann, ent_classes = ['PROFESION', 'SITUACION_LABORAL']):\n",
    "  entities = {}\n",
    "  with open(file_ann) as anns:\n",
    "    for ann in anns:\n",
    "      if ann.split('\\t')[1].split(' ')[0] in ent_classes:\n",
    "          ent = ann[:-1].split('\\t')[2]\n",
    "          #print(ent)\n",
    "          #ent = [token for token in nlp(ent) if not token.is_stop]\n",
    "          ent = nlp(ent)\n",
    "          start = int(ann[:-1].split('\\t')[1].split(' ')[1])\n",
    "          end = int(ann[:-1].split('\\t')[1].split(' ')[2])\n",
    "          if (len(ent) == 1):\n",
    "            entities[(start, end)] = SINGLE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "          else:\n",
    "            entities[(start, start + len(ent[0].text))] = BEGIN + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            entities[(end - len(ent[-1].text)), end] = END + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            for i in range(len(ent) - 2):\n",
    "              spaces = (ent[i + 1].idx) - (ent[i].idx + len(ent[i].text))\n",
    "              start = start + len(ent[i].text) + spaces\n",
    "              entities[(start, start + len(ent[i + 1].text))] = INSIDE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            \n",
    "  return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "japanese-acting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(31, 41): 'S_PROFESION'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDictEntities('/home/sergio/Escritorio/ProfNER/profner-data/subtask-2/brat/valid/1242407018465579008.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "destroyed-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def getProccessText(sst_home):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for file in [file[:-4] for file in os.listdir(sst_home) if file.endswith('.txt') and not ' ' in file]:\n",
    "        file_text = os.path.join(sst_home, file + '.txt')\n",
    "        file_ann = os.path.join(sst_home, file + '.ann')\n",
    "        _entities = getDictEntities(file_ann)\n",
    "        with open(file_text) as f:\n",
    "          text = f.read()\n",
    "          spacy_text = nlp(text)\n",
    "          for sent in spacy_text.sents:\n",
    "            sentence = []\n",
    "            sent_labels = []\n",
    "            for token in sent:\n",
    "                sentence.append(token.text.replace('#',''))\n",
    "                entity = _entities.get((token.idx, token.idx + len(token.text)), 'O')\n",
    "                sent_labels.append(entity)\n",
    "            sentences.append(sentence)\n",
    "            labels.append(sent_labels)\n",
    "    \n",
    "    return sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "knowing-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/train'\n",
    "sentences, labels = getProccessText(sst_home_train)\n",
    "\n",
    "sst_home_test = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "sentences_valid, labels_valid = getProccessText(sst_home_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exciting-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = ['B_SITUACION_LABORAL', 'I_SITUACION_LABORAL', 'E_SITUACION_LABORAL', 'S_SITUACION_LABORAL' ,\n",
    "      'B_PROFESION', 'I_PROFESION', 'E_PROFESION', 'S_PROFESION',\n",
    "      'O']\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "committed-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nuclear-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entitled-attribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "addressed-sucking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: IProgress in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (0.4)\r\n",
      "Requirement already satisfied: six in /home/sergio/anaconda3/envs/bert/lib/python3.6/site-packages (from IProgress) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install IProgress\n",
    "import ipywidgets\n",
    "import IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exact-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "usual-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "present-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]\n",
    "\n",
    "tokenized_texts_and_labels_test = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences_valid, labels_valid)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "linear-brook",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "tokenized_texts_test = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_test]\n",
    "labels_test = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "single-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids_test = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incident-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "tags_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_test],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "interracial-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "attention_masks_test = [[float(i != 0.0) for i in ii] for ii in input_ids_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "disciplinary-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\\n                                                            random_state=2018, test_size=0.1)\\ntr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\\n                                             random_state=2018, test_size=0.1)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs, tr_tags, tr_masks = input_ids, tags, attention_masks\n",
    "val_inputs, val_tags, val_masks = input_ids_test, tags_test, attention_masks_test\n",
    "\n",
    "'''tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "damaged-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ultimate-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "southwest-synthetic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.3.2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "reserved-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/sergio/Escritorio/ProfNER/pytorch/ were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at /home/sergio/Escritorio/ProfNER/pytorch/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    '/home/sergio/Escritorio/ProfNER/pytorch/',\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "related-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "medium-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "formal-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cardiac-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "composite-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.04292192439080926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 1/4 [07:55<23:46, 475.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.021197308957714517\n",
      "Validation Accuracy: 0.9939513640291322\n",
      "\n",
      "Average train loss: 0.012423428417517245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 2/4 [16:04<16:07, 483.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.020311886659316006\n",
      "Validation Accuracy: 0.9948066376285115\n",
      "\n",
      "Average train loss: 0.005480610090717899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 3/4 [24:17<08:07, 487.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.021707191045213653\n",
      "Validation Accuracy: 0.9954767489022519\n",
      "\n",
      "Average train loss: 0.002368010606933954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [32:29<00:00, 487.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.02382261357504069\n",
      "Validation Accuracy: 0.9953973936198353\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    #print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "floppy-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.save_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "equal-andorra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/tokenizer_config.json',\n",
       " '/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/special_tokens_map.json',\n",
       " '/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/vocab.txt',\n",
       " '/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs/added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('/home/sergio/Escritorio/ProfNER/fine-tuned-bert_3_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-apartment",
   "metadata": {},
   "source": [
    "## BETO Cosine Similarity Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-referral",
   "metadata": {},
   "source": [
    "### Getting the Entity Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-audit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
