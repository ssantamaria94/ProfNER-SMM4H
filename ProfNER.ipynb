{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troy&AbedInTheMorning Team Participation in SMM4H-Spanish\n",
    "\n",
    "Team Participants:\n",
    "* Sergio Santamaria Carrasco\n",
    "* Roberto Cuervo Rosillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "1. Setting the parent dirrectory\n",
    "2. Checking GPU is avaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_home = '/home/sergio/Escritorio/ProfNER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12994388458322486098\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15057625033811068942\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3547922432\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7148418811094183583\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4373140510700367521\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "    Tensorflow 1.14.0\n",
    "    Keras 2.2.4\n",
    "    Fasttext 0.2.0\n",
    "    MeaningCloud-python\n",
    "    Spacy\n",
    "    Sklearn_crfsuite\n",
    "    Keras-contrib\n",
    "    Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Format\n",
    "\n",
    "The data of the corpus was obtained from a Twitter crawl that used keywords like ‚ÄúCovid-19‚Äù, ‚Äúepidemia‚Äù (epidemic) or ‚Äúconfinamiento‚Äù (lockdown), as well as hashtags such as ‚Äú#yomequedoencasa‚Äù (#istayathome), to retrieve relevant tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerramos nuestra querida Radio üò¢ Nuestros colaboradores y conductores ¬°Se quedan en casa! Desde ma√±ana todos los programas de Radio Hoy se har√°n v√≠a remoto X Skype. Seguimos al aire con el compromiso de siempre, nosotros apoyamos el #QuedateEnCasa #covƒ±d19 #Coronavirus #RadioHoy https://t.co/Q81BBYpbdM\n"
     ]
    }
   ],
   "source": [
    "file_text = sst_home + '/profner-data/subtask-2/brat/train/1242399976644325376.txt' \n",
    "\n",
    "with open(file_text, 'r') as file:\n",
    "  print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotations Format\n",
    "\n",
    "The corpus was annotated by linguist experts in an iterative process that included the creation of annotation guidelines specifically for this task. We could find the data annotated by BRAT and BIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1\tPROFESION 42 55\tcolaboradores\n",
      "T2\tPROFESION 58 69\tconductores\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_ann = sst_home + '/profner-data/subtask-2/brat/train/1242399976644325376.ann' \n",
    "\n",
    "with open(file_ann) as file:\n",
    "  print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "TODO: Incluir los diferentes requerimientos del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syllabizer\n",
    "\n",
    "Using the rules of Spanish orthography, the syllabizer allows a word to be broken into its component syllables. The necessary code has been extracted from https://github.com/mabodo/sibilizador/blob/master/Silabizator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class char():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "class char_line():\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.char_line = [(char, self.char_type(char)) for char in word]\n",
    "        self.type_line = ''.join(chartype for char, chartype in self.char_line)\n",
    "        \n",
    "    def char_type(self, char):\n",
    "        if char in set(['a', '√°', 'e', '√©','o', '√≥', '√≠', '√∫']):\n",
    "            return 'V' #strong vowel\n",
    "        if char in set(['i', 'u', '√º']):\n",
    "            return 'v' #week vowel\n",
    "        if char=='x':\n",
    "            return 'x'\n",
    "        if char=='s':\n",
    "            return 's'\n",
    "        else:\n",
    "            return 'c'\n",
    "            \n",
    "    def find(self, finder):\n",
    "        return self.type_line.find(finder)\n",
    "        \n",
    "    def split(self, pos, where):\n",
    "        return char_line(self.word[0:pos+where]), char_line(self.word[pos+where:])\n",
    "    \n",
    "    def split_by(self, finder, where):\n",
    "        split_point = self.find(finder)\n",
    "        if split_point!=-1:\n",
    "            chl1, chl2 = self.split(split_point, where)\n",
    "            return chl1, chl2\n",
    "        return self, False\n",
    "     \n",
    "    def __str__(self):\n",
    "        return self.word\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.word)\n",
    "\n",
    "class silabizer():\n",
    "    def __init__(self):\n",
    "        self.grammar = []\n",
    "        \n",
    "    def split(self, chars):\n",
    "        rules  = [('VV',1), ('cccc',2), ('xcc',1), ('ccx',2), ('csc',2), ('xc',1), ('cc',1), ('vcc',2), ('Vcc',2), ('sc',1), ('cs',1),('Vc',1), ('vc',1), ('Vs',1), ('vs',1)]\n",
    "        for split_rule, where in rules:\n",
    "            first, second = chars.split_by(split_rule,where)\n",
    "            if second:\n",
    "                if first.type_line in set(['c','s','x','cs']) or second.type_line in set(['c','s','x','cs']):\n",
    "                    #print 'skip1', first.word, second.word, split_rule, chars.type_line\n",
    "                    continue\n",
    "                if first.type_line[-1]=='c' and second.word[0] in set(['l','r']):\n",
    "                    continue\n",
    "                if first.word[-1]=='l' and second.word[-1]=='l':\n",
    "                    continue\n",
    "                if first.word[-1]=='r' and second.word[-1]=='r':\n",
    "                    continue\n",
    "                if first.word[-1]=='c' and second.word[-1]=='h':\n",
    "                    continue\n",
    "                return self.split(first)+self.split(second)\n",
    "        return [chars]\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        return self.split(char_line(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Model\n",
    "\n",
    "The Spacy model selected is 'es_core_news_md'. Cause this model doesn't tokenize hashtags, is necessarily keep the attention during preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: es_core_news_md==2.3.1 from https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.3.1/es_core_news_md-2.3.1.tar.gz#egg=es_core_news_md==2.3.1 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from es_core_news_md==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: setuptools in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (49.1.0.post20200710)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.1.0)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import es_core_news_md\n",
    "\n",
    "nlp = es_core_news_md.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# We're using a class because the component needs to be initialised with\n",
    "# the shared vocab via the nlp object\n",
    "class HashtagMerger(object):\n",
    "    def __init__(self, nlp):\n",
    "        # Register a new token extension to flag bad HTML\n",
    "        Token.set_extension(\"is_hashtag\", default=False)\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        # Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "        self.matcher.add(\"HASHTAG\", None, [{\"ORTH\": \"#\"}, {\"TEXT\": {\"REGEX\": \"[A-Za-z√°√©√≠√≥√∫√Å√â√ç√ì√ö]+\"}}])\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # This method is invoked when the component is called on a Doc\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # Collect the matched spans here\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for span in spans:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token._.is_hashtag = True  # Mark token as bad HTML\n",
    "        return doc\n",
    "    \n",
    "hashtag_merger = HashtagMerger(nlp)\n",
    "nlp.add_pipe(hashtag_merger, last = True)  # Add component to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRAT to BIOES annotation\n",
    "\n",
    "The next code transform the BRAT annotations into BIOES format. The code return a dict with the token offset and the BIOES schema. In this schema,tokens are annotated using the following tags:\n",
    "\n",
    "\n",
    "    B: represents a token that conform the begining of an entity.\n",
    "    I: indicate that the token belongs to an entity.\n",
    "    O: represents that the token does not belong to an entity.\n",
    "    E: marks a token as the end of a given entity.\n",
    "    S: indicates that an entity is comprised of a single token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# BIOES NOTATION #####################\n",
    "BEGIN = 'B'\n",
    "INSIDE = 'I'\n",
    "OUTSIDE = 'O'\n",
    "END = 'E'\n",
    "SINGLE = 'S'\n",
    "\n",
    "def getDictEntities(file_ann, ent_classes = ['PROFESION', 'SITUACION_LABORAL']):\n",
    "  entities = {}\n",
    "  with open(file_ann) as anns:\n",
    "    for ann in anns:\n",
    "      if ann.split('\\t')[1].split(' ')[0] in ent_classes:\n",
    "          ent = ann[:-1].split('\\t')[2]\n",
    "          #print(ent)\n",
    "          #ent = [token for token in nlp(ent) if not token.is_stop]\n",
    "          ent = nlp(ent)\n",
    "          start = int(ann[:-1].split('\\t')[1].split(' ')[1])\n",
    "          end = int(ann[:-1].split('\\t')[1].split(' ')[2])\n",
    "          if (len(ent) == 1):\n",
    "            entities[(start, end)] = SINGLE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "          else:\n",
    "            entities[(start, start + len(ent[0].text))] = BEGIN + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            entities[(end - len(ent[-1].text)), end] = END + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            for i in range(len(ent) - 2):\n",
    "              spaces = (ent[i + 1].idx) - (ent[i].idx + len(ent[i].text))\n",
    "              start = start + len(ent[i].text) + spaces\n",
    "              entities[(start, start + len(ent[i + 1].text))] = INSIDE + '_' + ann.split('\\t')[1].split(' ')[0]\n",
    "            \n",
    "  return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "#from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def getElements(sst_home, max_len_seq, getTags = True):\n",
    "  _words = dict()\n",
    "  _doc_tags = {}\n",
    "  _entities = {}\n",
    "  _docs = {}\n",
    "  _docs_offset = {}\n",
    "  #mlb = MultiLabelBinarizer(classes = classes)\n",
    "    \n",
    "  for file in [file[:-4] for file in os.listdir(sst_home) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home, file + '.txt')\n",
    "    if getTags:\n",
    "      file_ann = os.path.join(sst_home, file + '.ann')\n",
    "      _entities = getDictEntities(file_ann)\n",
    "    with open(file_text) as f:\n",
    "      text = f.read()\n",
    "      spacy_text = nlp(text)\n",
    "      #spacy_text = [token for token in spacy_text if not token.is_stop]\n",
    "    \n",
    "      _tweet = []\n",
    "      _tweet_tags = []\n",
    "      _tweet_pos = []\n",
    "      _tweet_gtags = []\n",
    "      _tweet_ent = []\n",
    "      _tweet_offset = []\n",
    "    \n",
    "      for token in spacy_text[0:max_len_seq]:\n",
    "          _tweet.append(token.text)\n",
    "          _entity = _entities.get((token.idx, token.idx + len(token.text)), 'O')\n",
    "          _tweet_tags.append(_entity)\n",
    "          _words[token.text] = _words.get(token.text, 0) + 1\n",
    "          _tweet_pos.append(token.pos_)\n",
    "          _tweet_gtags.append(token.tag_)\n",
    "          _tweet_ent.append(token.ent_iob_ + '_' + token.ent_type_)\n",
    "          _tweet_offset.append((token.text, token.idx, token.idx + len(token.text)))\n",
    "\n",
    "    _docs[file] = (_tweet, _tweet_pos, _tweet_gtags, _tweet_ent)\n",
    "    _doc_tags[file] = _tweet_tags\n",
    "    _docs_offset[file] = _tweet_offset\n",
    "\n",
    "  #_words = list(_words)\n",
    "\n",
    "  return _words, _docs, _doc_tags, _docs_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the 2idx\n",
    "\n",
    "Our model is based in a Bag of Words/Charachters/Syllables/PoS-Tags, so we need to create a dictionary to assign each of one see it during training to a cardinal id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get2idx(words, chars, pos, gramm_tags, sylls, ents):\n",
    "    \n",
    "  ET = ['B_SITUACION_LABORAL', 'B_PROFESION', \n",
    "        'I_SITUACION_LABORAL', 'I_PROFESION', \n",
    "        'E_SITUACION_LABORAL', 'E_PROFESION', \n",
    "        'S_SITUACION_LABORAL', 'S_PROFESION',\n",
    "        'O']\n",
    "\n",
    "  word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "  word2idx[\"#ENDPAD\"] = 0\n",
    "  word2idx[\"UNK\"] = 1\n",
    "\n",
    "  char2idx = {char:i + 2 for i,char in enumerate(chars)}\n",
    "  char2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  char2idx[\"UNK\"] = 1\n",
    "\n",
    "  pos2idx = {pos:i + 2 for i,pos in enumerate(pos)}\n",
    "  pos2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  pos2idx[\"UNK\"] = 1\n",
    "\n",
    "  grammtags2idx = {tag:i + 2 for i,tag in enumerate(gramm_tags)}\n",
    "  grammtags2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  grammtags2idx[\"UNK\"] = 1\n",
    "\n",
    "  sylls2idx = {syll:i + 2 for i,syll in enumerate(sylls)}\n",
    "  sylls2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  sylls2idx[\"UNK\"] = 1\n",
    "\n",
    "  tag2idx = {tag:i + 1 for i,tag in enumerate(ET)}\n",
    "  tag2idx[\"#ENDPAD\"] = 0 \n",
    "\n",
    "  ent2idx = {ent:i + 2 for i,ent in enumerate(ents)}\n",
    "  ent2idx[\"#ENDPAD\"] = 0  # to ignore this by mask_zero = True\n",
    "  ent2idx[\"UNK\"] = 1\n",
    "\n",
    "  return word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx, ent2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### CHARACTERS ###\n",
    "\n",
    "def getCharacterInput(sentences, max_seq_len, max_chars_len, char2idx):\n",
    "  X_char = []\n",
    "  for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    # max_len = 75\n",
    "    for i in range(max_seq_len):\n",
    "      word_seq = []\n",
    "      # char sequence for words\n",
    "      for j in range(max_chars_len):\n",
    "        try:\n",
    "          # chars of specific sentence of i\n",
    "          word_seq.append(char2idx.get(sentence[i][j], 1)) \n",
    "        except:  # if char-sequence is out of range , pad it with \"PAD\" tag\n",
    "          word_seq.append(char2idx.get(\"#ENDPAD\"))\n",
    "\n",
    "      sent_seq.append(word_seq)\n",
    "    # append sentence sequences as character-by-character to X_char for Model input\n",
    "    X_char.append(np.array(sent_seq))\n",
    "\n",
    "  return np.array(X_char)\n",
    "\n",
    "### SYLLABLES ###\n",
    "\n",
    "def getSyllsInput(sentences, max_seq_len, max_sylls_len, sylls2idx):\n",
    "  X_syll = []\n",
    "  for sentence in sentences:\n",
    "    sent_seq = []\n",
    "    # max_len = 75\n",
    "    for i in range(max_seq_len):\n",
    "      word_seq = []\n",
    "      syllables = []\n",
    "      try:\n",
    "        syllables = silabizer(sentence[i])\n",
    "      except:\n",
    "        pass\n",
    "      for j in range(max_sylls_len):\n",
    "        try:\n",
    "          # chars of specific sentence of i\n",
    "          word_seq.append(sylls2idx.get(str(syllables[j]).lower(), 1)) \n",
    "        except:  # if char-sequence is out of range , pad it with \"PAD\" tag\n",
    "          word_seq.append(sylls2idx.get(\"#ENDPAD\"))\n",
    "\n",
    "      sent_seq.append(word_seq)\n",
    "    # append sentence sequences as character-by-character to X_char for Model input\n",
    "    X_syll.append(np.array(sent_seq))\n",
    "\n",
    "  return np.array(X_syll)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "### WORDS ###\n",
    "\n",
    "def getWordInput(sentences, max_seq_len, word2idx):\n",
    "  X = [[word2idx.get(w,1) for w in s[:max_seq_len]] for s in sentences]\n",
    "  X = pad_sequences(maxlen = max_seq_len, sequences = X, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X)\n",
    "\n",
    "### PoS-Tags ###\n",
    "\n",
    "def getPosInput(sentences, max_seq_len, pos2idx):\n",
    "  X_pos = [[pos2idx.get(w,1) for w in s] for s in sentences]\n",
    "  X_pos = pad_sequences(maxlen = max_seq_len, sequences = X_pos, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X_pos)\n",
    "\n",
    "def getGTagInput(sentences, max_seq_len, grammtags2idx):\n",
    "  X_gramm_tags = [[grammtags2idx.get(w,1) for w in s] for s in sentences]\n",
    "  X_gramm_tags = pad_sequences(maxlen = max_seq_len, sequences = X_gramm_tags, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X_gramm_tags)\n",
    "\n",
    "def getEntInput(sentences, max_seq_len, ent2idx):\n",
    "  X_ents = [[ent2idx.get(w,1) for w in s] for s in sentences]\n",
    "  X_ents = pad_sequences(maxlen = max_seq_len, sequences = X_ents, truncating= 'post', padding ='post', value=0 )\n",
    "  return np.array(X_ents)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "### NER TAG ###\n",
    "def getY(tags, max_seq_len, tag2idx):\n",
    "  y = [[tag2idx[t] for t in sent_tags] for sent_tags in tags]\n",
    "  y = pad_sequences(maxlen = max_seq_len, sequences = y, padding = \"post\", truncating = \"post\", value = tag2idx[\"#ENDPAD\"])\n",
    "  \n",
    "  y = [to_categorical(i, num_classes = len(tag2idx)) for i in y]\n",
    "  return np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the total words\n",
    "\n",
    "Cause we are using pre-trained embeddings, we use as our vocabulary the words contained in both the training, development and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_total_words = set()\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/train'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_train, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            _total_words.add(token.text)\n",
    "            #if not token.is_stop:\n",
    "                #_total_words.add(token.text)\n",
    "             \n",
    "sst_home_test = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_test) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_test, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            _total_words.add(token.text)\n",
    "            #if not token.is_stop:\n",
    "                #_total_words.add(token.text)\n",
    "\n",
    "'''\n",
    "    //TODO: HASTA QUE LIBEREN EL TEST SET, ESTO DEBE PERMANECER COMENTADO\n",
    "    \n",
    "sst_home_test = sst_home + 'test-background-set-to-publish/'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_test) if file.endswith('.txt') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_test, file + '.txt')\n",
    "    \n",
    "    with open(file_text) as f:\n",
    "        text = f.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_stop:\n",
    "                _total_words.add(token.text)\n",
    "'''\n",
    "\n",
    "_words = list(_total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generating Training Data\n",
    "\n",
    "The next code generate the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/train'\n",
    "\n",
    "MAX_CHARS_LEN = 25\n",
    "MAX_SEQ_LEN = 75\n",
    "MAX_SYLLS_LEN = 10\n",
    "\n",
    "silabizer = silabizer()\n",
    "\n",
    "words_tr, docs, docs_tags, _ = getElements(sst_home_train, MAX_SEQ_LEN)\n",
    "words_tr = list(words_tr.keys())\n",
    "chars = list(set(''.join(words_tr)))\n",
    "sylls = list(set(str(syll).lower() for word in words_tr for syll in silabizer(word)))\n",
    "\n",
    "tweets_train = [tweet[0] for tweet in docs.values()]\n",
    "tweets_pos = [tweet[1] for tweet in docs.values()]\n",
    "tweets_gtags = [tweet[2] for tweet in docs.values()]\n",
    "tweets_ents = [tweet[3] for tweet in docs.values()]\n",
    "\n",
    "all_pos = set(pos for sent in tweets_pos for pos in sent)\n",
    "all_gtags = set(gtag for sent in tweets_gtags for gtag in sent)\n",
    "all_ents = set(ent for sent in tweets_ents for ent in sent)\n",
    "\n",
    "tags = [tags for tags in docs_tags.values()]\n",
    "\n",
    "word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx, ent2idx = get2idx(_words, chars, all_pos, all_gtags, sylls, all_ents)\n",
    "\n",
    "X_char_train = getCharacterInput(tweets_train, MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "X_words_train = getWordInput(tweets_train, MAX_SEQ_LEN, word2idx)\n",
    "X_pos_train = getPosInput(tweets_pos, MAX_SEQ_LEN, pos2idx)\n",
    "X_tag_train = getGTagInput(tweets_gtags, MAX_SEQ_LEN, grammtags2idx)\n",
    "X_syll_train = getSyllsInput(tweets_train, MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "X_ent_train = getEntInput(tweets_ents, MAX_SEQ_LEN, ent2idx)\n",
    "\n",
    "y_train = getY(tags, MAX_SEQ_LEN, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "\n",
    "train_bert = {}\n",
    "with open(sst_home + '/saved_data/bert_train.pickle', 'rb') as handle:\n",
    "    train_bert = pickle.load(handle)\n",
    "\n",
    "X_train_bert = []\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/train'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    bert_vector = train_bert[file]\n",
    "    #bert_vector = pad_sequences(maxlen = 100, sequences = bert_vector, truncating= 'post', padding ='post', value=np.zeros(768))\n",
    "    X_train_bert.append(bert_vector)\n",
    "X_train_bert = pad_sequences(maxlen = 75, sequences = X_train_bert, truncating= 'post', padding ='post', value=np.zeros(1536))\n",
    "del train_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Validation Data\n",
    "\n",
    "The next code generate the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sst_home_test = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "\n",
    "_, docs, docs_tags, docs_offset = getElements(sst_home_test, MAX_SEQ_LEN)\n",
    "\n",
    "tweets_test = [tweet[0] for tweet in docs.values()]\n",
    "tweets_pos = [tweet[1] for tweet in docs.values()]\n",
    "tweets_gtags = [tweet[2] for tweet in docs.values()]\n",
    "tweets_ents = [tweet[3] for tweet in docs.values()]\n",
    "\n",
    "\n",
    "tags_test = [tags for tags in docs_tags.values()]\n",
    "\n",
    "\n",
    "X_char_test = getCharacterInput(tweets_test, MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "X_words_test = getWordInput(tweets_test, MAX_SEQ_LEN, word2idx)\n",
    "X_pos_test = getPosInput(tweets_pos, MAX_SEQ_LEN, pos2idx)\n",
    "X_tag_test = getGTagInput(tweets_gtags, MAX_SEQ_LEN, grammtags2idx)\n",
    "X_syll_test = getSyllsInput(tweets_test, MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "X_ent_test = getEntInput(tweets_ents, MAX_SEQ_LEN, ent2idx)\n",
    "\n",
    "y_test = getY(tags_test, MAX_SEQ_LEN, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "test_bert = {}\n",
    "with open(sst_home + '/saved_data/bert_valid.pickle', 'rb') as handle:\n",
    "    test_bert = pickle.load(handle)\n",
    "    \n",
    "def getBertInput(file_name):\n",
    "    bert_vector = test_bert[file_name]\n",
    "    #bert_vector = pad_sequences(maxlen = 100, sequences = bert_vector, truncating= 'post', padding ='post', value=np.zeros(768))\n",
    "    X_test_bert = [bert_vector]\n",
    "    X_test_bert = pad_sequences(maxlen = 75, sequences = X_test_bert, truncating= 'post', padding ='post', value=np.zeros(1536))\n",
    "    return X_test_bert\n",
    "\n",
    "X_test_bert = []\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.txt') and not ' ' in file]:\n",
    "    bert_vector = test_bert[file]\n",
    "    #bert_vector = pad_sequences(maxlen = 100, sequences = bert_vector, truncating= 'post', padding ='post', value=np.zeros(768))\n",
    "    X_test_bert.append(bert_vector)\n",
    "X_test_bert = pad_sequences(maxlen = 75, sequences = X_test_bert, truncating= 'post', padding ='post', value=np.zeros(1536))\n",
    "del test_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings from Spanish Medical Corpora & Twitter\n",
    "\n",
    "In this model we'll use two different pre-trained word embeddings:\n",
    "\n",
    "1. The Word Embeddings from Spanish Medical Corpora can be found in https://www.aclweb.org/anthology/W19-1916/\n",
    "2. The Word Embedding from Spanish Twitter (Covid-19) can be found in https://zenodo.org/record/4449930#.YBbYOtaCE5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "def getEmbeddingMatrix(words2idx, emb_dim, model):\n",
    "  embedding_matrix = np.zeros((len(words2idx), emb_dim))\n",
    "  for word, i in words2idx.items():\n",
    "    embedding_matrix[i] = model[word]\n",
    "\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "sst_home_embeddings = sst_home + '/fast-text-model/'\n",
    "\n",
    "### SPANISH MEDICAL CORPORA ###\n",
    "ft = fasttext.load_model(sst_home_embeddings + 'cantemist-resource.bin')\n",
    "medical_embedding_matrix = getEmbeddingMatrix(word2idx, ft.get_dimension(), ft)\n",
    "del ft\n",
    "\n",
    "### SPANISH TWITTER COVID-19 ###\n",
    "ft = fasttext.load_model(sst_home_embeddings + 'covid_19_es_twitter_cbow_cased.bin')\n",
    "twitter_embedding_matrix = getEmbeddingMatrix(word2idx, ft.get_dimension(), ft)\n",
    "del ft\n",
    "\n",
    "spacy_embedding_matrix = np.array([nlp(word)[0].vector for word in word2idx.keys()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Entities to construct Cosine Similarity Vectors\n",
    "\n",
    "For each word in our vocabulary, we generate a vector with the cosine similarity of all entities found in training data. To help with the computational cost, we parallelize the proccess using the *multiprocessing* library\n",
    "\n",
    "1. Extracting the different entities\n",
    "2. Defining the cosine similarity function\n",
    "3. Parallelization of the vector obtention proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "entities = {}\n",
    "sst_home_train = sst_home + '/profner-data/subtask-2/brat/train'\n",
    "for file in [file[:-4] for file in os.listdir(sst_home_train) if file.endswith('.ann') and not ' ' in file]:\n",
    "    file_text = os.path.join(sst_home_train, file + '.ann')\n",
    "    with open(file_text) as f:\n",
    "        for line in f.readlines():\n",
    "            ent_type = line.split('\\t')[1].split(' ')[0]\n",
    "            ent_form = line.split('\\t')[-1][:-1]\n",
    "            \n",
    "            entities[ent_type] = entities.get(ent_type, []) + [ent_form]\n",
    "            \n",
    "profesion_entities = list(set(entities['PROFESION']))\n",
    "sit_lab_entities = list(set(entities['SITUACION_LABORAL']))\n",
    "\n",
    "entities = profesion_entities + sit_lab_entities\n",
    "dim = len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = lambda a,b: dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "sst_home_embeddings = sst_home + '/fast-text-model/'\n",
    "\n",
    "ft = fasttext.load_model(sst_home_embeddings + 'covid_19_es_twitter_cbow_cased.bin')\n",
    "entities_vectors_I = {}\n",
    "for i, entity in enumerate(entities):\n",
    "    entities_vectors_I[entity] = np.mean([ft[token.text] for token in nlp(entity)], axis = 0)    \n",
    "del ft\n",
    "\n",
    "ft = fasttext.load_model(sst_home_embeddings + 'cantemist-resource.bin')\n",
    "\n",
    "entities_vectors_II = {}\n",
    "for i, entity in enumerate(entities):\n",
    "    entities_vectors_II[entity] = np.mean([ft[token.text] for token in nlp(entity)], axis = 0)    \n",
    "del ft\n",
    "\n",
    "\n",
    "def getVector(t):\n",
    "    word = t[0]\n",
    "    key = t[1]\n",
    "    vector = np.zeros(dim * 2)\n",
    "    for i, entity in enumerate(entities):\n",
    "        vector[i] = cos_sim(twitter_embedding_matrix[key], entities_vectors_I[entity])\n",
    "        vector[i + dim] = cos_sim(medical_embedding_matrix[key], entities_vectors_II[entity])\n",
    "    \n",
    "    return vector\n",
    "with Pool(12) as p:\n",
    "     cosine_matrix = p.map(getVector, list(word2idx.items()))\n",
    "        \n",
    "cosine_matrix = np.array(cosine_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "\n",
    "We save the generated data so that it is not necessary to generate it again next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_data = [X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_ent_train, X_train_bert, y_train]\n",
    "with open(sst_home + '/saved_data/train.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "test_data = [X_char_test, X_words_test, X_pos_test, X_tag_test, X_syll_test, X_ent_test, X_test_bert, y_test]\n",
    "with open(sst_home + '/saved_data/test.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "embedding_matrix = [medical_embedding_matrix, twitter_embedding_matrix, cosine_matrix, spacy_embedding_matrix]   \n",
    "with open(sst_home + '/saved_data/we.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "two2idx = [word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx, ent2idx]\n",
    "with open(sst_home + '/saved_data/2idx.pickle', 'wb') as handle:\n",
    "    pickle.dump(two2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "In case the data is previously saved, we load these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_train = []\n",
    "with open(sst_home + '/saved_data/train.pickle', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "\n",
    "X_test = []\n",
    "with open(sst_home + '/saved_data/test.pickle', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "\n",
    "embedding_matrixes = []\n",
    "with open(sst_home + '/saved_data/we.pickle', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)\n",
    "\n",
    "two2idx = []\n",
    "with open(sst_home + '/saved_data/2idx.pickle', 'rb') as handle:\n",
    "    two2idx = pickle.load(handle)\n",
    "\n",
    "X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_ent_train, X_train_bert, y_train = X_train\n",
    "X_char_test, X_words_test, X_pos_test, X_tag_test, X_syll_test, X_ent_test, X_test_bert, y_test = X_test\n",
    "word2idx, char2idx, tag2idx, pos2idx, grammtags2idx, sylls2idx, ent2idx = two2idx\n",
    "medical_embedding_matrix, twitter_embedding_matrix, cosine_matrix, spacy_embedding_matrix = embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
      "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-k5oa8nwk\n",
      "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-k5oa8nwk\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages\n",
      "Requirement already satisfied: keras in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from keras-contrib==2.0.8) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.5.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (5.3.1)\n",
      "Requirement already satisfied: h5py in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101064 sha256=9f3a63ad58f3ea253d00ac68a08fa59468152251c8f2669c3a9788ae047e42e9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pcw3u071/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n",
      "Successfully built keras-contrib\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/home/sergio/anaconda3/envs/tfm/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, 75, 25)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sylls_input (InputLayer)        (None, 75, 10)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 75, 25, 30)   17760       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 75, 10, 75)   990900      sylls_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 75, 23, 50)   4550        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 75, 8, 50)    11300       time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 75, 23, 50)   0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 75, 8, 50)    0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "word_input (InputLayer)         (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tag_input (InputLayer)          (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ents_input (InputLayer)         (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling (TimeDistributed)   (None, 75, 50)       0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 75, 50)       0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "medical_word_embeddings (Embedd (None, 75, 300)      11853600    word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "twitter_word_embeddings (Embedd (None, 75, 300)      11853600    word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bert_input (InputLayer)         (None, 75, 1536)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_embeddings (Embedding)      (None, 75, 20)       380         pos_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gtags_embeddings (Embedding)    (None, 75, 20)       6640        tag_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ents_embeddings (Embedding)     (None, 75, 20)       220         ents_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "char_enc (TimeDistributed)      (None, 75, 50)       0           max_pooling[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 75, 50)       0           time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 2296)     0           medical_word_embeddings[0][0]    \n",
      "                                                                 twitter_word_embeddings[0][0]    \n",
      "                                                                 bert_input[0][0]                 \n",
      "                                                                 pos_embeddings[0][0]             \n",
      "                                                                 gtags_embeddings[0][0]           \n",
      "                                                                 ents_embeddings[0][0]            \n",
      "                                                                 char_enc[0][0]                   \n",
      "                                                                 time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 75, 300), (N 2936400     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 300)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 300)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 75, 300), (N 3116400     concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 75, 75)       0           lstm_2[0][0]                     \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 75, 75)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 75, 300)      0           activation_1[0][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "cosine_distance_embeddings (Emb (None, 75, 1632)     64483584    word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 75, 2232)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "                                                                 cosine_distance_embeddings[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 75, 2232)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 75, 200)      446600      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 75, 10)       2130        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 95,724,064\n",
      "Trainable params: 95,724,064\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Concatenate, Input, SpatialDropout1D\n",
    "from keras.layers import Conv1D, MaxPooling1D,Flatten,GlobalMaxPooling1D, Reshape, RepeatVector,  Dot, GRU, Activation\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "CHAR_EMBEDDINGS_SIZE = 30    # Characters Embeddings Size\n",
    "SYLL_EMBEDDINGS_SIZE = 75    # Syllable Embeddings Size\n",
    "WORD_EMBEDDINGS_SIZE = 300   # Word Embeddings Size\n",
    "COSINE_EMBEDDING_SIZE = cosine_matrix.shape[1]\n",
    "POS_EMBEDDING_SIZE = 20      # PoS Embedding Size\n",
    "GTAGS_EMBEDDING_SIZE = 20    # Tag Embedding Size\n",
    "\n",
    "MAX_CHARS_LEN = 25           # Max sequence char length\n",
    "MAX_SEQ_LEN = 75             # Max sequence word length Level II\n",
    "MAX_SYLLS_LEN = 10           # Max sequence sylls length\n",
    "\n",
    "CONV_FILTERS = 50            # Convolutional Filters in Character and Syllable Convolutional Layer\n",
    "LSTM_UNITS = 300             # LSTM Units in both LSTM layers\n",
    "DENSE_UNITS = 200            # Number of units in Dense layer  \n",
    "\n",
    "BERT_DIM = 1536\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), name = 'word_input')\n",
    "pos_input = Input(shape=(MAX_SEQ_LEN,), name = 'pos_input')\n",
    "gtag_input = Input(shape=(MAX_SEQ_LEN,), name = 'tag_input')\n",
    "char_input = Input(shape=(MAX_SEQ_LEN, MAX_CHARS_LEN), name = 'char_input')\n",
    "sylls_input = Input(shape=(MAX_SEQ_LEN, MAX_SYLLS_LEN), name = 'sylls_input')\n",
    "ents_input = Input(shape=(MAX_SEQ_LEN,), name = 'ents_input')\n",
    "bert_input = Input(shape=(MAX_SEQ_LEN, BERT_DIM,), name = 'bert_input')\n",
    "\n",
    "###################### CHARACTER SEQUENCE PROCCESSED BY CONVOLUTIONAL LAYER ################################\n",
    "char_embedding = TimeDistributed(Embedding(input_dim = len(char2idx), output_dim = CHAR_EMBEDDINGS_SIZE, input_length=MAX_CHARS_LEN, name = 'char_embeddings', trainable = True))(char_input)\n",
    "conv_1d = TimeDistributed(Conv1D(filters = CONV_FILTERS, kernel_size = 3,padding = \"valid\", activation = \"relu\", name=\"Conv1D_char\"))(char_embedding)\n",
    "conv_1d = TimeDistributed(Dropout(0.4))(conv_1d)\n",
    "maxpool1d = TimeDistributed(GlobalMaxPooling1D(), name = 'max_pooling')(conv_1d)\n",
    "char_enc = TimeDistributed(Flatten(), name = 'char_enc')(maxpool1d)\n",
    "\n",
    "###################### SYLLABLE SEQUENCE PROCCESSED BY CONVOLUTIONAL LAYER ################################\n",
    "syll_embedding = TimeDistributed(Embedding(input_dim = len(sylls2idx), output_dim = SYLL_EMBEDDINGS_SIZE, input_length=MAX_SYLLS_LEN, name = 'sylls_embeddings', trainable = True))(sylls_input)\n",
    "conv_1d_syll = TimeDistributed(Conv1D(filters = CONV_FILTERS, kernel_size = 3,padding=\"valid\", activation=\"relu\", name=\"Conv1D_syll\"))(syll_embedding)\n",
    "conv_1d_syll = TimeDistributed(Dropout(0.4))(conv_1d_syll)\n",
    "maxpool1d_syll = TimeDistributed(GlobalMaxPooling1D())(conv_1d_syll)\n",
    "syll_enc = TimeDistributed(Flatten())(maxpool1d_syll)\n",
    "\n",
    "##################### PoS + TAG EMBEDDINGS ################################################################\n",
    "pos_embedding = Embedding(input_dim = len(pos2idx), output_dim = POS_EMBEDDING_SIZE, input_length = MAX_SEQ_LEN, name = 'pos_embeddings', trainable = True)(pos_input)\n",
    "gtags_embedding = Embedding(input_dim = len(grammtags2idx), output_dim = GTAGS_EMBEDDING_SIZE, input_length = MAX_SEQ_LEN, name = 'gtags_embeddings', trainable = True)(gtag_input)\n",
    "ents_embeddings =  Embedding(input_dim = len(ent2idx), output_dim = 20, input_length = MAX_SEQ_LEN, name = 'ents_embeddings', trainable = True)(ents_input)\n",
    "##################### WORD EMBEDDING LAYER ################################################################\n",
    "medical_embedding_layer = Embedding(input_dim = len(word2idx), output_dim = WORD_EMBEDDINGS_SIZE, weights=[medical_embedding_matrix], trainable=True, name = 'medical_word_embeddings')\n",
    "medical_word_embedding = medical_embedding_layer(word_input)\n",
    "\n",
    "twitter_embedding_layer = Embedding(input_dim = len(word2idx), output_dim = WORD_EMBEDDINGS_SIZE, weights=[twitter_embedding_matrix], trainable=True, name = 'twitter_word_embeddings')\n",
    "twitter_word_embedding = twitter_embedding_layer(word_input)\n",
    "\n",
    "cosine_embedding_layer = Embedding(input_dim = len(word2idx), output_dim = COSINE_EMBEDDING_SIZE, weights=[cosine_matrix], trainable=True, name = 'cosine_distance_embeddings')\n",
    "cosine_word_embedding = cosine_embedding_layer(word_input)\n",
    "\n",
    "#spacy_embedding_layer = Embedding(input_dim = len(word2idx), output_dim = WORD_EMBEDDINGS_SIZE, weights=[spacy_embedding_matrix], trainable=True, name = 'spacy_embeddings')\n",
    "#spacy_word_embedding = spacy_embedding_layer(word_input)\n",
    "################### CONCATENATE INPUT FEATURES ###########################################\n",
    "x = Concatenate(axis = -1)([medical_word_embedding, twitter_word_embedding, bert_input, pos_embedding, gtags_embedding, ents_embeddings, char_enc, syll_enc])\n",
    "\n",
    "################## BiLSTM MAX SEQUENCE WORD LENGTH 50 ######################################################\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(units=LSTM_UNITS//2, return_sequences=True,recurrent_dropout=0, return_state = True, recurrent_activation = 'sigmoid'))(x)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#encoder_outputs = Dropout(0.4)(encoder_outputs)\n",
    "#encoder_states = Dropout(0.4)(encoder_states)\n",
    "\n",
    "decoder_outputs, _, _ = LSTM(units=LSTM_UNITS, recurrent_dropout=0, return_sequences = True, recurrent_activation = 'sigmoid', return_state = True)(x, initial_state = encoder_states)\n",
    "attention = Dot(axes = (2,2))([decoder_outputs, encoder_outputs])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = Dot(axes=(2,1))([attention, encoder_outputs])\n",
    "x = Concatenate()([context, decoder_outputs, cosine_word_embedding])\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "################## DENSE LAYER ############################################################################\n",
    "x = Dense(DENSE_UNITS, activation='tanh')(x)\n",
    "\n",
    "################## CRF LAYER ####################################################################################\n",
    "crf = CRF(len(tag2idx), sparse_target = False)\n",
    "loss = crf.loss_function\n",
    "y_output = crf(x)\n",
    "\n",
    "model = Model(inputs = [char_input, word_input, pos_input, gtag_input, sylls_input, ents_input, bert_input], outputs = y_output)\n",
    "model.compile(optimizer = \"adam\", loss = loss, metrics = [crf.accuracy])\n",
    "#model.load_weights(sst_home + '/model_weights/best_model.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 107s 18ms/step - loss: 0.0023 - crf_viterbi_accuracy: 0.9993 - val_loss: 0.0078 - val_crf_viterbi_accuracy: 0.9979\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.00231, saving model to /home/sergio/Escritorio/ProfNER/model_weights/best_model.hdf5\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "checkpoint = ModelCheckpoint(sst_home + '/model_weights/best_model.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
    "history = model.fit([X_char_train, X_words_train, X_pos_train, X_tag_train, X_syll_train, X_ent_train, X_train_bert], y_train,\n",
    "                    validation_data = ([X_char_test, X_words_test, X_pos_test, X_tag_test, X_syll_test, X_ent_test, X_test_bert], y_test),\n",
    "                    batch_size = 32,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the annotations (7b)\n",
    "\n",
    "The next step is generate the annotations in the right format to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(y_pred, offset, file_name, path_to_save):\n",
    "    SINGLE = 'S'\n",
    "    BEGIN = 'B'\n",
    "    END = 'E'\n",
    "    INSIDE = 'I'\n",
    "    #i = 1\n",
    "    has_begin = False\n",
    "    current_ent = None\n",
    "    text = ''\n",
    "  #file = path_to_save + file_name + '.ann'\n",
    "    for ann, info in zip(y_pred, offset):\n",
    "      #if not((ann[0] == 'O' and current_ent != None) or (current_ent != None and current_ent != ann[2:])):\n",
    "      if ann[0] == SINGLE:\n",
    "        if has_begin:\n",
    "          print('ERROR: Llego una SINGLE sin terminar la anterior BI -> S')\n",
    "          print('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, start, end, ann[2:], entity.replace('\\n', '#SALTO')))\n",
    "          print()\n",
    "          text = text + '{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, start, end, ann[2:], entity.replace('\\n', '#SALTO'))\n",
    "          current_ent = None\n",
    "          has_begin = False\n",
    "\n",
    "        text = text + '{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, info[1], info[2], ann[2:], info[0].replace('\\n', '#SALTO'))\n",
    "        current_ent = None\n",
    "\n",
    "      if ann[0] == BEGIN:\n",
    "        if has_begin:\n",
    "          print('ERROR: Llego un BEGIN sin terminar la anterior BI -> B')\n",
    "          print('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, start, end, ann[2:], entity.replace('\\n', '#SALTO')))\n",
    "          print()\n",
    "          text = text + '{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, start, end, ann[2:], entity.replace('\\n', '#SALTO'))\n",
    "          #i = i + 1\n",
    "\n",
    "        entity = info[0]\n",
    "        start = info[1]\n",
    "        end = info[2]\n",
    "        current_ent = ann[2:]\n",
    "        has_begin = True\n",
    "\n",
    "      if ann[0] == INSIDE:\n",
    "        current_ent = ann[2:]\n",
    "        if not has_begin:\n",
    "          print('ERROR: Llego INSIDE sin terminar la anterior BI -> I')\n",
    "          print()\n",
    "          entity = info[0]\n",
    "          start = info[1]\n",
    "          end = info[2]\n",
    "          has_begin = True\n",
    "\n",
    "        else:\n",
    "          entity = entity + ' ' + info[0]\n",
    "          end = info[2]\n",
    "\n",
    "      if ann[0] == END:\n",
    "        current_ent = None\n",
    "        if not has_begin:\n",
    "          print('ERROR: Llego un END sin tener BEGIN')\n",
    "          print('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, info[1], info[2], ann[2:], info[0].replace('\\n', '#SALTO')))\n",
    "          print()\n",
    "          #text = text + '{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, info[1], info[2], ann[2:], info[0].replace('\\n', '#SALTO'))\n",
    "          #i = i + 1\n",
    "        else:\n",
    "          entity_text = entity + ' ' + info[0]\n",
    "          text = text + '{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, start, info[2], ann[2:], entity_text.replace('\\n', '#SALTO'))\n",
    "          has_begin = False\n",
    "          #i = i + 1\n",
    "      '''else:\n",
    "          print('ERROR: Cambio de entidad sin final')\n",
    "          #print(f'actual: {current_ent}, siguiente {ann[2:]}')\n",
    "          print('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, start, end, current_ent, entity.replace('\\n', '#SALTO')))\n",
    "          print()\n",
    "          text = text + '{}\\t{}\\t{}\\t{}\\t{}\\n'.format(file_name, start, end, current_ent, entity.replace('\\n', '#SALTO'))\n",
    "          current_ent = None\n",
    "          has_begin = False '''\n",
    "          \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE = 'S'\n",
    "BEGIN = 'B'\n",
    "END = 'E'\n",
    "INSIDE = 'I'\n",
    "OUT = 'O'\n",
    "\n",
    "def annotateV2(y_pred, offset, file_name, extra = None):    \n",
    "    max_index = len(y_pred)\n",
    "    if '#ENDPAD' in y_pred:\n",
    "        max_index = y_pred.index('#ENDPAD')\n",
    "        \n",
    "    right_moves = {'B': [ 'I', 'E'],\n",
    "                   'I': ['I', 'E'],\n",
    "                   'E': ['B', 'S', 'O'],\n",
    "                   'S': ['B', 'S', 'O'],\n",
    "                   'O': ['B', 'S', 'O']}\n",
    "    text = ''\n",
    "    entity = ''\n",
    "    start = -1\n",
    "    end = -1\n",
    "    \n",
    "    has_error = False\n",
    "    for i in range(max_index - 1):\n",
    "        if not (has_error and y_pred[i][0] in ['I','E']):\n",
    "            has_error = False\n",
    "            ann = y_pred[i][0]\n",
    "            info = offset[i]\n",
    "            next_ann = y_pred[i + 1][0]\n",
    "            entity_class = y_pred[i][2:]\n",
    "            ### SI LA EL MOVIMIENTO ES PERMITIDO, SE EJECUTA SU POLITICA NORMAL\n",
    "            if next_ann in right_moves[ann]:\n",
    "                info = offset[i]\n",
    "                if ann == BEGIN:\n",
    "                    entity = info[0]\n",
    "                    start = info[1]\n",
    "                    end = info[2]\n",
    "                if ann == INSIDE:\n",
    "                    entity = entity + ' ' + info[0]\n",
    "                    end = info[2]\n",
    "                if ann == END:\n",
    "                    entity = entity + ' ' + info[0]\n",
    "                    end = info[2]\n",
    "                    text = text + f'{file_name}\\t{start}\\t{end}\\t{entity_class}\\t{entity}\\n'\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "                if ann == SINGLE:\n",
    "                    text = text + f'{file_name}\\t{info[1]}\\t{info[2]}\\t{entity_class}\\t{info[0]}\\n'\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "                if ann == OUT:\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "            else:\n",
    "                has_error = True\n",
    "                \n",
    "                file_ann = sst_home + f'/profner-data/subtask-2/brat/valid/{file_name}.ann' \n",
    "                with open(file_ann) as file:\n",
    "                  #print(file_ann)\n",
    "                  print(file.read())\n",
    "                \n",
    "                '''\n",
    "                if ann == 'I' and next_ann == 'O':\n",
    "                    entity = \" \".join([entity, info[0], offset[i + 1][0]])\n",
    "                    print(f'{file_name}\\t{start}\\t{offset[i + 1][2]}\\t{entity_class}\\t{entity}\\n')\n",
    "                    text = text + f'{file_name}\\t{start}\\t{offset[i + 1][2]}\\t{entity_class}\\t{entity}\\n'\n",
    "                \n",
    "                if ann == 'S' and next_ann == 'I' and y_pred[i + 2][0] == 'E':\n",
    "                    entity = \" \".join([info[0], offset[i + 1][0], offset[i + 2][0]])\n",
    "                    #print(f'{file_name}\\t{info[1]}\\t{offset[i + 2][2]}\\t{entity_class}\\t{entity}\\n')\n",
    "                    text = text + f'{file_name}\\t{info[1]}\\t{offset[i + 2][2]}\\t{entity_class}\\t{entity}\\n'\n",
    "                    \n",
    "                if ann == 'S' and next_ann == 'I' and y_pred[i + 2][0] == 'O':\n",
    "                    entity = \" \".join([info[0], offset[i + 1][0]])\n",
    "                    #print(f'{file_name}\\t{info[1]}\\t{offset[i + 1][2]}\\t{entity_class}\\t{entity}\\n')\n",
    "                    text = text + f'{file_name}\\t{info[1]}\\t{offset[i + 1][2]}\\t{entity_class}\\t{entity}\\n'\n",
    "   \n",
    "                if ann == 'E' and next_ann == 'I':\n",
    "                    entity = \" \".join([entity, info[0]])\n",
    "                    print(f'{file_name}\\t{start}\\t{offset[i][2]}\\t{entity_class}\\t{entity}\\n')\n",
    "                    text = text + f'{file_name}\\t{start}\\t{offset[i][2]}\\t{entity_class}\\t{entity}\\n'\n",
    "                    \n",
    "                if ann == 'E' and next_ann == 'E':\n",
    "                    entity = \" \".join([entity, info[0], offset[i + 1][0]])\n",
    "                    print(f'{file_name}\\t{start}\\t{offset[i + 1][2]}\\t{entity_class}\\t{entity}\\n')\n",
    "                    text = text + f'{file_name}\\t{start}\\t{offset[i + 1][2]}\\t{entity_class}\\t{entity}\\n'\n",
    "                '''\n",
    "                if ann == 'E' and next_ann == 'E':\n",
    "                    print('###### ERROR #######')\n",
    "                    print(f'Error: {y_pred[i - 1]} -> {ann} -> {next_ann} -> {y_pred[i + 2]}')\n",
    "                    info = offset[i]\n",
    "                    if entity == '':\n",
    "                        print(f'Entidad: \"{info[0]}\" Clase: \"{y_pred[i][2:]}\"')\n",
    "                    else:\n",
    "                        print(f'Entidad: {entity  + info[0]} Offset: ({start},{info[2]})')\n",
    "                    if next_ann != OUT:\n",
    "                        pass\n",
    "                    print(f'Anterior Entidad {offset[i - 1][0]}')\n",
    "                    print(f'Siguiente Entidad {offset[i + 1][0]}')\n",
    "                    print()\n",
    "                    \n",
    "                entity = ''\n",
    "                start = -1\n",
    "                end = -1\n",
    "            \n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1\tPROFESION 130 166\tDirectora de Personas y Comunicaci√≥n\n",
      "\n",
      "T1\tPROFESION 45 65\tpastores evang√©licos\n",
      "T2\tPROFESION 3 16\tGuardia Civil\n",
      "\n",
      "\n",
      "T1\tPROFESION 172 182\tlimpiadora\n",
      "T2\tPROFESION 24 49\tlimpiadora de un hospital\n",
      "T3\tPROFESION 118 130\taltos cargos\n",
      "\n",
      "T1\tPROFESION 172 182\tlimpiadora\n",
      "T2\tPROFESION 24 49\tlimpiadora de un hospital\n",
      "T3\tPROFESION 118 130\taltos cargos\n",
      "\n",
      "T1\tPROFESION 15 48\tvicepresidente d asuntos sociales\n",
      "T2\tPROFESION 176 190\tvicepresidente\n",
      "\n",
      "\n",
      "T1\tPROFESION 8 18\tpresidente\n",
      "T2\tSITUACION_LABORAL 95 107\ttrabajadores\n",
      "\n",
      "\n",
      "T1\tPROFESION 14 45\tDirector General de la @policia\n",
      "\n",
      "T1\tPROFESION 14 45\tDirector General de la @policia\n",
      "\n",
      "T1\tPROFESION 3 23\tpresidente de la Fed\n",
      "\n",
      "\n",
      "T1\tPROFESION 5 28\tministra de @Defensagob\n",
      "\n",
      "T1\tPROFESION 151 177\tprofesor de la Universidad\n",
      "T2\tPROFESION 126 138\tepidemi√≥logo\n",
      "\n",
      "T1\tPROFESION 101 109\tdirector\n",
      "\n",
      "T1\tSITUACION_LABORAL 34 46\ttrabajadores\n",
      "T2\tSITUACION_LABORAL 74 83\templeados\n",
      "T3\tSITUACION_LABORAL 179 188\templeados\n",
      "\n",
      "T1\tPROFESION 9 31\tsubsecretario de salud\n",
      "\n",
      "T1\tPROFESION 69 88\tmiembros de las FAS\n",
      "\n",
      "T1\tPROFESION 67 71\tjuez\n",
      "T2\tPROFESION 20 47\tcoronel de la guardia civil\n",
      "\n",
      "T1\tACTIVIDAD 125 136\tRey em√©rito\n",
      "\n",
      "\n",
      "T1\tPROFESION 16 51\tespecialista de medicina preventiva\n",
      "\n",
      "###### ERROR #######\n",
      "Error: I_PROFESION -> E -> E -> O\n",
      "Entidad: especialista demedicina Offset: (16,40)\n",
      "Anterior Entidad de\n",
      "Siguiente Entidad preventiva\n",
      "\n",
      "T1\tPROFESION 40 52\taltos cargos\n",
      "\n",
      "T1\tSITUACION_LABORAL 26 32\talumna\n",
      "\n",
      "T1\tPROFESION 205 212\tm√©dicos\n",
      "T2\tPROFESION 173 183\tresidentes\n",
      "T3\tPROFESION 35 45\tresidentes\n",
      "\n",
      "T1\tPROFESION 0 15\tpersonal m√©dico\n",
      "T2\tPROFESION 30 59\ttrabajadores de supermercados\n",
      "\n",
      "T1\tPROFESION 38 62\tjefe de Enf. Infecciosas\n",
      "T2\tPROFESION 23 26\tDr.\n",
      "\n",
      "###### ERROR #######\n",
      "Error: I_PROFESION -> E -> E -> E_PROFESION\n",
      "Entidad: jefe deEnf Offset: (38,49)\n",
      "Anterior Entidad de\n",
      "Siguiente Entidad .\n",
      "\n",
      "T1\tPROFESION 38 62\tjefe de Enf. Infecciosas\n",
      "T2\tPROFESION 23 26\tDr.\n",
      "\n",
      "T1\tPROFESION 235 243\tasesores\n",
      "\n",
      "T1\tPROFESION 171 178\tPolic√≠a\n",
      "T2\tPROFESION 184 197\tGuardia Civil\n",
      "T3\tPROFESION 131 166\tresponsables de las Fuerzas Armadas\n",
      "\n",
      "T1\tPROFESION 30 36\tm√©dico\n",
      "T2\tPROFESION 54 71\tm√©dico de familia\n",
      "\n",
      "T1\tPROFESION 3 10\talcalde\n",
      "T2\tPROFESION 18 53\tdirector del Centro de Coordinaci√≥n\n",
      "\n",
      "\n",
      "T1\tPROFESION 9 39\texperto espa√±ol en coronavirus\n",
      "\n",
      "T1\tPROFESION 209 219\tpresidente\n",
      "T2\tPROFESION 27 59\tespecialistas chinos de la salud\n",
      "\n",
      "T1\tPROFESION 209 219\tpresidente\n",
      "T2\tPROFESION 27 59\tespecialistas chinos de la salud\n",
      "\n",
      "T1\tSITUACION_LABORAL 169 178\templeadas\n",
      "T2\tPROFESION 181 199\templeados p√∫blicos\n",
      "\n",
      "T1\tPROFESION 131 154\tPresidente del Gobierno\n",
      "T2\tPROFESION 46 70\tprofesionales sanitarios\n",
      "T3\tPROFESION 208 218\tsanitarios\n",
      "\n",
      "T1\tPROFESION 123 160\tprofesionales de la seguridad p√∫blica\n",
      "T2\tPROFESION 12 41\tprofesionales de la seguridad\n",
      "\n",
      "###### ERROR #######\n",
      "Error: I_PROFESION -> E -> E -> O\n",
      "Entidad: profesionales de laseguridad Offset: (123,152)\n",
      "Anterior Entidad la\n",
      "Siguiente Entidad p√∫blica\n",
      "\n",
      "T1\tPROFESION 67 100\texpendedor de alimentos en plazas\n",
      "\n",
      "T1\tPROFESION 10 16\trector\n",
      "T2\tFIGURATIVA 142 147\tdue√±o\n",
      "T3\tPROFESION 31 66\tmiembro del Comit√© Ol√≠mpico Espa√±ol\n",
      "\n",
      "T1\tPROFESION 2 9\tM√©dicos\n",
      "T2\tPROFESION 130 161\tdeportistas de alto rendimiento\n",
      "T3\tPROFESION 10 32\tprofesores de Medicina\n",
      "\n",
      "T1\tPROFESION 38 59\tpersonal de seguridad\n",
      "\n",
      "T1\tPROFESION 72 79\tm√©dicos\n",
      "T2\tSITUACION_LABORAL 80 89\tjubilados\n",
      "T3\tSITUACION_LABORAL 103 114\testudiantes\n",
      "T4\tSITUACION_LABORAL 91 100\tgraduados\n",
      "\n",
      "T1\tSITUACION_LABORAL 15 20\tpreso\n",
      "\n",
      "T1\tPROFESION 107 136\tComisionada de la Generalitat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sst_home_test = sst_home + '/profner-data/subtask-2/brat/valid'\n",
    "idx2tag = {idx:tag for (tag, idx) in tag2idx.items()}\n",
    "\n",
    "_, docs, _, docs_offset = getElements(sst_home_test, MAX_SEQ_LEN, getTags = False)\n",
    "\n",
    "result_path = sst_home + '/final-result/'\n",
    "text = ''\n",
    "\n",
    "for (file_name, doc_sents) in docs.items():\n",
    "  X_char = getCharacterInput([docs[file_name][0]], MAX_SEQ_LEN, MAX_CHARS_LEN, char2idx)\n",
    "  X_words = getWordInput([docs[file_name][0]], MAX_SEQ_LEN, word2idx)\n",
    "  X_pos = getPosInput([docs[file_name][1]], MAX_SEQ_LEN, pos2idx)\n",
    "  X_tag = getGTagInput([docs[file_name][2]], MAX_SEQ_LEN, grammtags2idx)\n",
    "  X_syll = getSyllsInput([docs[file_name][0]], MAX_SEQ_LEN, MAX_SYLLS_LEN, sylls2idx)\n",
    "  X_ent = getEntInput([docs[file_name][2]], MAX_SEQ_LEN, ent2idx)\n",
    "  X_bert = getBertInput(file_name)\n",
    "\n",
    "  y_pred = model.predict([X_char, X_words, X_pos, X_tag, X_syll, X_ent, X_bert])\n",
    "  y_pred = [list(map(lambda x: idx2tag[np.argmax(x)], sent)) for sent in y_pred][0]\n",
    "  offset_test = docs_offset[file_name]\n",
    "  text = text + annotateV2(y_pred, offset_test, file_name, result_path)\n",
    "\n",
    "text = 'tweet_id\\tbegin\\tend\\ttype\\textraction\\n' + text\n",
    "with open(result_path + 'results.tsv', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the annotations (7a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google_trans_new\n",
    "!pip install PyDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp('El capell√°n'):\n",
    "    print(token.pos_.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text = translator.translate('√âl',lang_src='es',lang_tgt='en')\n",
    "print(translate_text)\n",
    "try:\n",
    "    a = dictionary.meaning(translate_text)\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_trans_new import google_translator  \n",
    "from PyDictionary import PyDictionary\n",
    "\n",
    "translator = google_translator() \n",
    "dictionary = PyDictionary()\n",
    "def getSpanishMeaning(word):\n",
    "    translate_text = translator.translate(word,lang_src='es',lang_tgt='en')\n",
    "    english_meaning = dictionary.meaning(translate_text)\n",
    "    spanish_meanings = {}\n",
    "    if english_meaning:\n",
    "        for tag in english_meaning.keys():\n",
    "            for mean in english_meaning[tag]:\n",
    "                translate_text = translator.translate(mean,lang_src='en',lang_tgt='es')\n",
    "                if not type(translate_text) == str:\n",
    "                    translate_text = translate_text[0]\n",
    "                translate_text = translate_text.strip().replace('(', '')\n",
    "                spanish_meanings[tag.upper()] = spanish_meanings.get(tag.upper(), []) + [translate_text]\n",
    "    return spanish_meanings\n",
    "\n",
    "getSpanishMeaning('pap√°')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = {user for user in word2idx.keys() if user.startswith('@')}\n",
    "hashtags = {user for user in word2idx.keys() if user.startswith('#')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(map(str.lower, users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r\"[A-Z][a-z]+|\\d+|[A-Z]+(?![a-z])\")\n",
    "def split_hashtag(tag):\n",
    "    return pattern.findall(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(split_hashtag, hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler('XDZYetUApxdI3hnrYWcFZbX2w', 'r7nWPIRwhXaxsoCNVjyMtFGPJBEIlpoZoePKRH60mL3MJ2rtsf')\n",
    "auth.set_access_token('314861452-PFLPbE8Mgkt4XF1VRpA0tJGvh4sXyM08uPm3oTX9', 'k5NbfAh83UuKothMnc97mN8MIlGdY7AlzI0lvzhhEPNs3')\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "user = api.get_user('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user._json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import TweepError\n",
    "#users_info = {}\n",
    "rejected = []\n",
    "for user in users:\n",
    "    if user in rejected:\n",
    "        try:    \n",
    "            users_info[user] = api.get_user(user)._json\n",
    "        except TweepError as e:\n",
    "            if (e.api_code!= 50) and (e.api_code != 63):\n",
    "                users_info[user] = {}\n",
    "            else:\n",
    "                rejected.append(user)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = api.get_user('nocdknjcfdknjdfhbkvfdhfdv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_in = [user for user in users_info.keys() if users_info[user].get('id', False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sst_home + '/saved_data/twitter_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(users_info, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = {}\n",
    "with open(sst_home + '/profner-data/subtask-2/tsv/valid.tsv', 'r') as file:\n",
    "    for line in file.readlines()[1:]:\n",
    "        t = line.split('\\t')\n",
    "        validation_data[t[0]] = validation_data.get(t[0], []) + [(t[1], t[2], t[3], t[4][:-1])]\n",
    "\n",
    "prediction_data = {}\n",
    "with open(sst_home + '/final-result/results.tsv', 'r') as file:\n",
    "    for line in file.readlines()[1:]:\n",
    "        t = line.split('\\t')\n",
    "        prediction_data[t[0]] = prediction_data.get(t[0], []) + [(t[1], t[2], t[3], t[4][:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in validation_data.keys():\n",
    "    print(f'######## Tweet: {tweet} ########')\n",
    "    print('######## Validation ########')\n",
    "    print(validation_data[tweet])\n",
    "    print('######## Prediction ########')\n",
    "    print(prediction_data.get(tweet, {}))\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    input('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rae.DLE.search_word('capell√°n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyrae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_text = translator.translate('polcia',lang_src='es',lang_tgt='en')\n",
    "translate_text = translator.translate(translate_text,lang_src='en',lang_tgt='es')\n",
    "print(translate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDictEntities(sst_home + '/profner-data/subtask-2/brat/valid/1254776314545836032.ann' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "The purpose of this section is to generate new training data based on existing data by using the similarity vectors to replace entities with synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPANISH MEDICAL CORPORA ###\n",
    "sst_home_embeddings = sst_home + '/fast-text-model/'\n",
    "ft = fasttext.load_model(sst_home_embeddings + 'covid_19_es_twitter_cbow_cased.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in ft.get_nearest_neighbors('gobierno', k=10):\n",
    "    print(nlp(result[1])[0].ipy result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE = 'S'\n",
    "BEGIN = 'B'\n",
    "END = 'E'\n",
    "INSIDE = 'I'\n",
    "OUT = 'O'\n",
    "\n",
    "def annotateV2(y_pred, offset, file_name, extra = None):    \n",
    "    max_index = len(y_pred)\n",
    "    if '#ENDPAD' in y_pred:\n",
    "        max_index = y_pred.index('#ENDPAD')\n",
    "        \n",
    "    right_moves = {'B': [ 'I', 'E'],\n",
    "                   'I': ['I', 'E'],\n",
    "                   'E': ['B', 'S', 'O'],\n",
    "                   'S': ['B', 'S', 'O'],\n",
    "                   'O': ['B', 'S', 'O']}\n",
    "    text = ''\n",
    "    entity = ''\n",
    "    start = -1\n",
    "    end = -1\n",
    "    \n",
    "    has_error = False\n",
    "    for i in range(max_index - 1):\n",
    "        if not (has_error and y_pred[i][0] in ['I','E']):\n",
    "            has_error = False\n",
    "            ann = y_pred[i][0]\n",
    "            info = offset[i]\n",
    "            next_ann = y_pred[i + 1][0]\n",
    "            entity_class = y_pred[i][2:]\n",
    "            ### SI LA EL MOVIMIENTO ES PERMITIDO, SE EJECUTA SU POLITICA NORMAL\n",
    "            if next_ann in right_moves[ann]:\n",
    "                info = offset[i]\n",
    "                if ann == BEGIN:\n",
    "                    entity = info[0]\n",
    "                    start = info[1]\n",
    "                    end = info[2]\n",
    "                if ann == INSIDE:\n",
    "                    entity = entity + ' ' + info[0]\n",
    "                    end = info[2]\n",
    "                if ann == END:\n",
    "                    entity = entity + ' ' + info[0]\n",
    "                    end = info[2]\n",
    "                    text = text + f'{file_name}\\t{start}\\t{end}\\t{entity_class}\\t{entity}\\n'\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "                if ann == SINGLE:\n",
    "                    text = text + f'{file_name}\\t{info[1]}\\t{info[2]}\\t{entity_class}\\t{info[0]}\\n'\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "                if ann == OUT:\n",
    "                    entity = ''\n",
    "                    start = -1\n",
    "                    end = -1\n",
    "            else:\n",
    "                has_error = True\n",
    "                \n",
    "                file_ann = sst_home + f'/profner-data/subtask-2/brat/valid/{file_name}.ann' \n",
    "                with open(file_ann) as file:\n",
    "                  print(file.read())\n",
    "                \n",
    "                '''\n",
    "                if ann == 'I' and next_ann == 'O':\n",
    "                    entity = \" \".join([entity, info[0], offset[i + 1][0]])\n",
    "                    print(f'{file_name}\\t{start}\\t{offset[i + 1][2]}\\t{entity_class}\\t{entity}\\n')\n",
    "                    text = text + f'{file_name}\\t{start}\\t{offset[i + 1][2]}\\t{entity_class}\\t{entity}\\n'\n",
    "                '''\n",
    "                \n",
    "                if ann == 'B' and next_ann == 'O':\n",
    "                    print('###### ERROR #######')\n",
    "                    print(f'Error: {ann} -> {next_ann} -> {y_pred[i + 2]}')\n",
    "                    if ann != OUT:\n",
    "                        info = offset[i]\n",
    "                        if entity == '':\n",
    "                            print(f'Entidad: \"{info[0]}\" Clase: \"{y_pred[i][2:]}\"')\n",
    "                        else:\n",
    "                            print(f'Entidad: {entity  + info[0] + offset[i + 1][0]} Clase: {y_pred[i][2:]} Offset: ({start},{info[2]})')\n",
    "                    if next_ann != OUT:\n",
    "                        pass\n",
    "                    print(f'Siguiente Entidad {offset[i + 1][0]}')\n",
    "                    print()\n",
    "                    \n",
    "                entity = ''\n",
    "                start = -1\n",
    "                end = -1\n",
    "            \n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('#Cami√≥n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
